<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Kai Kunze</title>
 <link href="http://kaikunze.de/atom.xml" rel="self"/>
 <link href="http://kaikunze.de/"/>
 <updated>2015-02-09T00:47:08+09:00</updated>
 <id>http://kaikunze.de</id>
 <author>
   <name>Kai Kunze</name>
 </author>

 
 <entry>
   <title>31C3 Talk Slides: Eye Wear Computing</title>
   <link href="http://kaikunze.de/posts/31c3-talk-slides-eye-wear-computing"/>
   <updated>2015-02-09T00:00:00+09:00</updated>
   <id>http://kaikunze.de/posts/31c3-talk-slides-eye-wear-computing</id>
   <content type="html">&lt;p class=&quot;lead&quot;&gt;Here are the video and slides 
from my talk. Hope you like it. Please
if you have some critique write me a mail.&lt;/p&gt;


&lt;p&gt;I got tremendous, positive feedback. Thanks a lot!
Even Heise had a news post about it.
&lt;a href=&quot;http://www.heise.de/newsticker/meldung/31C3-Mit-smarten-Brillen-das-Gehirn-ausforschen-2507482.html&quot;&gt;http://www.heise.de/newsticker/meldung/31C3-Mit-smarten-Brillen-das-Gehirn-ausforschen-2507482.html&lt;/a&gt;
(Although I cannot and don&amp;rsquo;t want to read your thoughts, as the article implies ;&amp;ndash;) ).&lt;/p&gt;

&lt;p&gt;Video on Youtube:&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/aWO8aejiRnA&quot; frameborder=&quot;0&quot; allowfullscreen&gt; &lt;/iframe&gt;


&lt;p&gt;Slides on Speakerdeck:&lt;/p&gt;

&lt;script async class=&quot;speakerdeck-embed&quot; data-id=&quot;f75c69a07f4c01328d155ab31af9093f&quot; data-ratio=&quot;1.77777777777778&quot; src=&quot;//speakerdeck.com/assets/embed.js&quot;&gt; &lt;/script&gt;


&lt;p&gt;I got mixed some feedback on twitter. Some people
mentioned that I&amp;rsquo;m working on spy wear helping
the surveillance state&amp;hellip; I believe that
the research I do is necessary to be out in the
open for the society to discuss the merits
and problems. I want to make sure that we can maximise the benefit for the individual for the tech I develop and minimize abuse from military, companies and governments. Please contact me
if you want to discuss privacy issues or found concrete problems with my work.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>31C3 Aftermath</title>
   <link href="http://kaikunze.de/posts/31c3-aftermath"/>
   <updated>2015-01-17T00:00:00+09:00</updated>
   <id>http://kaikunze.de/posts/31c3-aftermath</id>
   <content type="html">&lt;p class=&quot;lead&quot;&gt;Finally, I have a little time to write about the congress. As the last
couple of years, I attended the 31st Chaos Communication Congress between Christmas and New Year.
&lt;/p&gt;


&lt;p&gt;Here&amp;rsquo;s my talk selection in random order (I definitely forgot some as I haven&amp;rsquo;t watched all):&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://media.ccc.de/browse/congress/2014/31c3_-_6385_-_en_-_saal_g_-_201412292245_-_the_machine_to_be_another_-_beanotherlab.html#video&quot;&gt; The Machine to be Another&lt;/a&gt;
great research and talk. Mesmerizing, I&amp;rsquo;m thinking about how to use this effect for my work ;)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://media.ccc.de/browse/congress/2014/31c3_-_6573_-_en_-_saal_2_-_201412281130_-_from_computation_to_consciousness_-_joscha.html#video&quot;&gt;From Computation to Consciousness&lt;/a&gt;.
Nice &amp;ldquo;Philosopy&amp;rdquo; talk by Joscha.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://media.ccc.de/browse/congress/2014/31c3_-_6180_-_en_-_saal_2_-_201412271830_-_rocket_science_how_hard_can_it_be_-_david_madlener.html#video&amp;t=672&quot;&gt;Rocket Science&lt;/a&gt;
David Madlener gives a nice, entertaining
intro in why it&amp;rsquo;s important to go to space and
how to build rockets.&lt;/p&gt;

&lt;p&gt;&lt;a href=http://media.ccc.de/browse/congress/2014/31c3_-_6574_-_en_-_saal_1_-_201412301245_-_why_are_computers_so_and_what_can_we_do_about_it_-_peter_sewell.html#video&gt;Why are computers so @#!*, and what can we do about it?&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://media.ccc.de/browse/congress/2014/31c3_-_6558_-_de_-_saal_g_-_201412282300_-_traue_keinem_scan_den_du_nicht_selbst_gefalscht_hast_-_david_kriesel.html#video&quot;&gt; Traue keinem Scan, den du nicht selbst gef√§lscht hast
&lt;/a&gt; In German, yet I think there&amp;rsquo;s a translation.
Extremely funny, hope the translation captures it.&lt;/p&gt;

&lt;p&gt;Don&amp;rsquo;t watch the keynote. I wonder who picked
the speaker &amp;hellip; terrible.&lt;/p&gt;

&lt;p&gt;This year, I spent a substantial amount
with at the &lt;a href=&quot;https://foodhackingbase.org&quot;&gt; Food Hacking Base&lt;/a&gt;. Interesting talks.
I&amp;rsquo;m thinking more and more to do some research
in this direction. Especially after I listened
to the new Resonator Podcast about
the &lt;a href=&quot;http://resonator-podcast.de/2015/res049-kurz-zu-verdauungssystem-und-gehirn/&quot;&gt;connection between our gut and our brain&lt;/a&gt;
(recording is unfortunately in German).&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Eye-Wear Computing</title>
   <link href="http://kaikunze.de/posts/eyewear-computing"/>
   <updated>2014-11-30T00:00:00+09:00</updated>
   <id>http://kaikunze.de/posts/eyewear-computing</id>
   <content type="html">&lt;p class=&quot;lead&quot;&gt; Smart glasses and, in general, eyewear are a fairly novel device class with a lot of possibilities for unobtrusive activity tracking. That&#39;s why I&#39;m very excited to be working in the Team of Masahiko Inami Sensei at Keio Media Design to do research on J!NS MEME.&lt;/p&gt;


&lt;p&gt;You might have seen the &lt;a href=&quot;http://academic.jins.com/en/&quot;&gt; J!NS academic videos &lt;/a&gt; by now,
I added embedded versions to the end of the post.&lt;/p&gt;

&lt;p&gt;Bellow is the full video of the sneak peek of our work in the J!NS promotion. Special thanks to &lt;a href=&quot;http://shoya.io/aboutme/&quot;&gt; Shoya Ishimaru &lt;/a&gt; and &lt;a href=&quot;http://questbe.at&quot;&gt;Katsuma Tanaka &lt;/a&gt;, two talented students
Koichi Kise Sensei (Osaka Prefecture University)
and I are co-supervising. Check out their other (private) work if you are into programming for smart phones, Mac, iOS and Google Glass ;).
The video is a summary  of research work mostly done by Shoya.&lt;/p&gt;

&lt;iframe src=&quot;//player.vimeo.com/video/113188940&quot; width=&quot;500&quot; height=&quot;281&quot; frameborder=&quot;0&quot; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;
&lt;/iframe&gt;


&lt;p&gt;In the video we show applications using an early prototype of J!NS MEME, smart glasses with integrated electrodes to detect eye movements (Electrooculography, EOG) and motion sensors (accelerometer and gyroscope) to monitor head motions. We show several demonstrations: a simple eye movement visualization, detecting left/right eye motion and blink. Additionally, users can play a game, &amp;ldquo;Blinky Bird&amp;rdquo;. They need to help a bird avoid obstacles using eye movements. Using a combination of blink, eye movement and head motion
we can also detect reading and talking behavior. We can give people a long term view of their reading, talking, and also walking activity over the day.&lt;/p&gt;

&lt;p&gt;Publications still pending, so I cannot
talk about the features, algorithms used etc.
In the mean time, here is a demo we gave at UbiComp this year.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/papers/ishimaru2014smarter.pdf&quot;&gt;&lt;em&gt;Smarter Eyewear- Using Commercial EOG Glasses for Activity Recognition&lt;/em&gt;&lt;/a&gt;. Ishimaru, Shoya and Kunze, Kai and Tanaka, Katsuma and Uema, Uji and Kise, Koichi and Inami, Masahiko. Proceedings of UbiComp&#39;14 Adjunct. 2014. &lt;a href=&quot;papers/bib/ishimaru2014smarter.bib&quot;&gt;Bibtex&lt;/a&gt;. &lt;/p&gt;


&lt;p&gt;J!NS Academic Video:&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;//www.youtube.com/embed/W6lWbnoxmqM&quot; frameborder=&quot;0&quot; allowfullscreen&gt; 
&lt;/iframe&gt;


&lt;p&gt;Oh and if you haven&amp;rsquo;t had enough:
Here&amp;rsquo;s an extended Interview with Inami Sensei and me.
Me wearing the optical camouflage for the first time at 0:04 :D (very short).&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;//www.youtube.com/embed/9vVtUe6gQ8E&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;

</content>
 </entry>
 
 <entry>
   <title>Google Glass for Older Adults</title>
   <link href="http://kaikunze.de/posts/google-glass-for-eldery"/>
   <updated>2014-08-24T00:00:00+09:00</updated>
   <id>http://kaikunze.de/posts/google-glass-for-eldery</id>
   <content type="html">&lt;p class=&quot;lead&quot;&gt; As the first article about
my grandparents using Google Glass received a lot of interest,
I decided to delve a little bit more into the topic.&lt;/p&gt;


&lt;p&gt;My grandparents conducted a longer Google Glass usability study for me.
I&amp;rsquo;m happy they agreed that I can use their images and
insights to share here.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/imgs/work.jpg&quot; alt=&quot;oma&quot; /&gt;
&lt;img src=&quot;/imgs/shopping_g.jpeg&quot; alt=&quot;opa&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;Evaluation of the Current Google Glass&lt;/h2&gt;

&lt;p&gt;My grandparents mentioned that the current functionality of the device is quite limited. This might be due to the English only menu and bad to no Internet connectivity during usage. The experimental setup seems unobtrusive, as both of them are used to wear glasses, they got easily accustomed to carry Glass. All confirmed that the head-mounted display was not hindering them performing everyday tasks. Only
my grandmother mentioned discomfort as the device got unusual hot after a longer usage session of recording video and displaying directions.
&lt;D-r&gt;
During the simple reading test, they could read text of font sizes of 40px and higher on the 640 x 360 screen if a white font on a black background was used (best contrast). However, other font colors were problematic and needed larger sizes to be legible. Especially the light grey font color
sed by some standard Glass applications  was hard to read. For example, they could not read the night temperature for the weather card.
Participants were able to use most speech commands and the &amp;ldquo;head wake&amp;rdquo; feature (tilting the head back to activate Glass) without problems. Only the &amp;ldquo;google search&amp;rdquo; speech command was not recognized, probably due to German accent ;)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/imgs/wrong_g.png&quot; alt=&quot;oma&quot; /&gt;
&lt;img src=&quot;/imgs/dessert_g.png&quot; alt=&quot;opa&quot; /&gt;&lt;/p&gt;

&lt;p&gt;While both were able to activate the photo functions easily, they had difficulties to take the picture they intended. Glass seems to be optimized for taking photos in a slight distance (e.g. taking tourist pictures). However, my grandparents mostly wanted to take pictures to remember what they were doing (e.g. what things they had in the hand). It took a while for them to realize that the camera won‚Äôt make a photo of what they see (depicted in the both pictures above).&lt;/p&gt;

&lt;p&gt;It was difficult for the two to use the touch panel for navigating Glass. The activation tab and scrolling usually works, yet the cancel gesture (‚Äùswiping down‚Äù) is more problematic and often not recognized. We assume this is due to fingers getting dry when getting older.&lt;/p&gt;

&lt;p&gt;The active card display of Glass ‚Äì swiping right to see cards relevant right now (e.g. weather, appointments), swiping left to see older events (e.g. taken pictures) ‚Äì was intuitive. Yet, they had difficulties to use some of the hierarchical menu structures (e.g. for settings and doing a video call). They mentioned that it is hard to realize in which context they are currently operating, as Glass gives no indication of the hierarchy level.&lt;/p&gt;

&lt;h3&gt;Usage Patterns&lt;/h3&gt;

&lt;p&gt;Already during the two days of system use and the shopping tour a number of usage patterns emerged. They used the camera feature the most. A common use case was memory augmentation. Making pictures of things they don&amp;rsquo;t want to forget. For example, taking a picture of medication, so they can remember that it was already taken or taking pictures of interesting items while shopping.&lt;/p&gt;

&lt;p&gt;Both preferred the &amp;ldquo;hands free&amp;rdquo; operations using the speech interface (although it was in English) compared to the touch interface during house work. Yet, being in town, they switched to the touch interface.
During cooking and house work, my grandmother appreciated the timer provided by Glass. However, it was difficult for them to set the timer using the current interface, as this involves a hierarchical menu. It was not clear if they currently change the hours, minutes or seconds while in the respective sub menu. Wearing the timer on the body is highly appreciated, as it&amp;rsquo;s not possible to miss the timer going off.&lt;/p&gt;

&lt;p&gt;For gardening and cooking, they would like to do video chats, for example to ask friends about tips and get their advice. Unfortunately, the limited internet reception at participants‚Äô place did not allow video chats during the test phase.&lt;/p&gt;

&lt;h3&gt;Requirements&lt;/h3&gt;

&lt;p&gt;Through the initial assessment of the system&amp;rsquo;s functionalities and through the observation we found a number of requirements.
The focus on readability is even more crucial for older adults. Although Glass was already designed with this in mind, it seems font size is not the only thing that matters. Contrast seems equally important, as participants found it very difficult to read the light grey fonts used in some of the screens provided by Glass.&lt;/p&gt;

&lt;p&gt;My grandparents request intuitive, hands-free interactions. They appreciated the speech interface if they are not in public or the &amp;ldquo;blink to take a picture&amp;rdquo; feature as they don&amp;rsquo;t need to interact with the touch panel. They also did not want to make the device dirty, especially during cooking and gardening.
The touch interface was sometimes difficult to use for them. A potential reason is that they are not used to capacitive touch devices such as current smartphones. Scrolling worked, yet the cancel gesture (swiping down) was difficult to perform. It needed 2 or 3 tries every time they wanted to use it.&lt;/p&gt;

&lt;h2&gt;Application Ideas&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/imgs/garden_g.jpeg&quot; alt=&quot;work&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Short Term Memory Augmentation&lt;/strong&gt; ‚Äì As we described above, participants frequently took pictures to use them as reminder (e.g. taking medication). Using the time card interface of Glass, it was already easy to check if they performed the ac- tion or task in question by browsing through the taken pic- tures. Each picture has also a timestamp with it (see Figure 2). However, this only works for the last couple of days, as other- wise the user needs to scroll too far back. The most requested feature was Zoom for images. While shopping, users took pictures to remember interesting items. Prices from items can be recorded, yet as the device does not support zoom for pic- tures, it‚Äôs impossible to read the price on the head mounted display (see Figure 5).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Long Term Capture and Access&lt;/strong&gt;‚Äì The participants saw po- tential in having a long term capture and access interface. Checking how and what they worked on/did a couple months or even years back. Search on specific activities (e.g. bak- ing a apple cake) should be possible for access. Participants thought other types of indexing (e.g. location or time) would be not so useful.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Timer and Reminders&lt;/strong&gt; ‚Äì Although the interface was not op- timal for them the users already found the timer application useful. The raised the need for several simultaneous timers and reminders. Right now the installed timer application just supports a single task.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Instructions&lt;/strong&gt; ‚Äì For the gardening, cooking and workshop scenario, my grandparents would like to get instructions (e.g. ingredient lists, work steps) for more complex tasks they do not perform often. They prefer the Glass display to paper or instruction manuals, as they don‚Äôt need a context switch, clean their hands, stop what they do. Yet, they emphasized that the instructions need to be easily browsable. They would prefer the ‚Äùright‚Äù information at the ‚Äùright‚Äù time. Participant 2: ‚ÄùCan‚Äôt Glass infer that I‚Äôm backing a cake right now and show me the ingredients I need for it?‚Äù&lt;/p&gt;

&lt;h2&gt;Summary&lt;/h2&gt;

&lt;p&gt;These application scenarios are very similar to applications discussed for maintenance and, in general, industrial domains. Yet, as seen from the requirements usability and interface need to be adjusted significantly before wearable computing can be used by older adults without help.&lt;/p&gt;

&lt;h2&gt;Finally, see you @ UbiComp&lt;/h2&gt;

&lt;p&gt;This is a serious of articles about our UbiComp Submissions
if you want to read more, check out the Poster paper:
&lt;a href=&quot;/papers/kunze2014wearable.pdf&quot;&gt;&lt;em&gt;Wearable Computing for Older Adults -Initial Insights into Head-Mounted Display Usage&lt;/em&gt;&lt;/a&gt;. Kunze, Kai and Henze, Niels and Kise, Koichi. Proceedings of UbiComp&#39;14 Adjunct. 2014. &lt;a href=&quot;papers/bib/kunze2014wearable.bib&quot;&gt;Bibtex&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you are attending UbiComp/ISWC this year please drop by at
our poster. Oh and if you found the write-up useful, please cite us
or share the article :)&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Looking forward to ISWC/Ubicomp 2014</title>
   <link href="http://kaikunze.de/posts/looking-forward-to-iswcubicomp-2014"/>
   <updated>2014-08-10T00:00:00+09:00</updated>
   <id>http://kaikunze.de/posts/looking-forward-to-iswcubicomp-2014</id>
   <content type="html">&lt;p class=&quot;lead&quot;&gt;With roughly around 1 month to go, 
we are busy with demo preparations etc.
&lt;/p&gt;


&lt;p&gt;I hope to see you in Seattle. This year we have
again a couple papers outline work from our students.
Attached is a list with draft versions of the papers.
I will write a bit about each topic in the next coming
weeks.&lt;/p&gt;

&lt;p&gt;Oh and if you attend please think about stopping
by our &lt;a href=&quot;http://recall-fet.eu/wahm2014/&quot;&gt;Workshop on Ubiquitous Technologies for Augmenting the Human Mind&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/papers/okoso2014implicit.pdf&quot;&gt;&lt;em&gt;Implicit Gaze based Annotations to Support Second Language Learning&lt;/em&gt;&lt;/a&gt;. Okoso, Ayano and Kunze, Kai and Kise, Koichi. Proceedings of UbiComp&#39;14 Adjunct. 2014. &lt;a href=&quot;papers/bib/okoso2014implicit.bib&quot;&gt;Bibtex&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/papers/kunze2014wearable.pdf&quot;&gt;&lt;em&gt;Wearable Computing for Older Adults -Initial Insights into Head-Mounted Display Usage&lt;/em&gt;&lt;/a&gt;. Kunze, Kai and Henze, Niels and Kise, Koichi. Proceedings of UbiComp&#39;14 Adjunct. 2014. &lt;a href=&quot;papers/bib/kunze2014wearable.bib&quot;&gt;Bibtex&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/papers/tanaka2014memory.pdf&quot;&gt;&lt;em&gt;Memory Specs-An Annotation System on Google Glass using Document Image Retrieval&lt;/em&gt;&lt;/a&gt;. Tanaka, Katsuma and Kunze, Kai and Iwata, Motoi and Kise, Koichi. Proceedings of UbiComp&#39;14 Adjunct. 2014. &lt;a href=&quot;papers/bib/tanaka2014memory.bib&quot;&gt;Bibtex&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/papers/ishimaru2014smarter.pdf&quot;&gt;&lt;em&gt;Smarter Eyewear- Using Commercial EOG Glasses for Activity Recognition&lt;/em&gt;&lt;/a&gt;. Ishimaru, Shoya and Kunze, Kai and Tanaka, Katsuma and Uema, Uji and Kise, Koichi and Inami, Masahiko. Proceedings of UbiComp&#39;14 Adjunct. 2014. &lt;a href=&quot;papers/bib/ishimaru2014smarter.bib&quot;&gt;Bibtex&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;/papers/ishimaru2014brain.pdf&quot;&gt;&lt;em&gt;Position Paper: Brain Teasers &amp;ndash; Toward Wearable Computing that Engages Our Mind&lt;/em&gt;&lt;/a&gt;. Ishimaru, Shoya and Kunze, Kai and Kise, Koichi and Inami, Masahiko. Proceedings of UbiComp&#39;14 Adjunct. 2014. &lt;a href=&quot;papers/bib/ishimaru2014brain.bib&quot;&gt;Bibtex&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Augmented Human 2014</title>
   <link href="http://kaikunze.de/posts/augmented-human-2014"/>
   <updated>2014-03-19T00:00:00+09:00</updated>
   <id>http://kaikunze.de/posts/augmented-human-2014</id>
   <content type="html">&lt;p class=&quot;lead&quot;&gt;Innovative research, that makes you first laugh and then think. The conference develops into one of my favorite venues.&lt;/p&gt;


&lt;p&gt;Ok, I&amp;rsquo;m &amp;ldquo;a bit&amp;rdquo; biased as I&amp;rsquo;m one of the conference co-chairs. Still I enjoyed this years Augmented Human.
Below is the tag cloud from all abstracts, to give you a brief overview about the topics.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/imgs/ah-cloud.png&quot; alt=&quot;Tag Cloud&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Considering the small size of the conference, the quality of the work is exceptional. It&amp;rsquo;s not one of the conferences that gets the rejected papers from CHI, Ubicomp, PerComp etc. The steering committee really set up a venue for far-out, novel ideas. Also it&amp;rsquo;s a good opportunity to meet great researchers up close; last year for example Thad Starner, Albrecht Schmidt etc. this year, Jun Rekimoto, Masahiko Inami, Paul Lukowicz and especially &lt;a href=&quot;http://www.cyberdyne.jp/english/&quot;&gt;Yoshiyuki Sankai&lt;/a&gt; &amp;hellip;
pretty impressive if you ask me. They might be around at other bigger events, yet try to catch them and talk to them, impossible. At AH, it&amp;rsquo;s very easy. I recommend any young researcher interested in the research topics to attend next year&amp;rsquo;s AH. Surely, I will try to get some papers accepted ;)&lt;/p&gt;

&lt;p&gt;I believe we will see a lot of the work presented at AH2014 at CHI or Ubicomp next year. Yet, decide for yourself.&lt;/p&gt;

&lt;p&gt;In the following, I&amp;rsquo;ll show you just a couple of highlights. I&amp;rsquo;m sorry, I cannot mention all of the cool work (I realized by writing that the blog post got bigger and bigger and decided to stop at some point so I can finally publish it &amp;hellip;).&lt;/p&gt;

&lt;h1&gt;Sports&lt;/h1&gt;

&lt;p&gt;As already the tag cloud suggested, augmenting sports was a hot topic at the conference.&lt;/p&gt;

&lt;p&gt;So just in case you want to play a round of Quidditch or Shaolin Soccer in the real world, we might have the tech for it. Rekimoto research about &amp;ldquo;Hover Ball&amp;rdquo;.
This topic has also been picked up by the &lt;a href=&quot;http://www.newscientist.com/article/mg22129614.900-dronepowered-hoverball-could-spice-up-games.html&quot;&gt;New Scientist&lt;/a&gt;.
I also recommend to check out some work by Takuya Nojima Sensei (TAMA and PhotoelasticBall).&lt;/p&gt;

&lt;p&gt;The best paper award also went to a sports themed paper: &amp;ldquo;Around Me: A System for Providing Sports Player&amp;rsquo;s Self-images with an Escort Robot&amp;rdquo;. Nice!&lt;/p&gt;

&lt;iframe src=&quot;//player.vimeo.com/video/88497066&quot; width=&quot;500&quot; height=&quot;375&quot; frameborder=&quot;0&quot; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;


&lt;h1&gt;Around the Eye&lt;/h1&gt;

&lt;p&gt;As you might know, I have a personal interest in &lt;a href=&quot;http://kaikunze.de/posts/30c3-toward-a-cognitive-quantified-self/&quot;&gt;Eyetracking and related research&lt;/a&gt;, as I think it&amp;rsquo;s a very promising direction (especially inferring types of information that you otherwise cannot easily get hold off). So I was very curious about related work presented at AH about the topic and was not disappointed.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m wondering if I feel comfortable sharing my sad emotions, as suggested by Tearsense (Marina Mitani, Yasuaki Kakehi). Maybe in a dark cinema this is alright. As a part of life logging it might be also interesting. We had a couple of interesting discussions also during the social about the technology.&lt;/p&gt;

&lt;iframe src=&quot;//player.vimeo.com/video/88341946&quot; width=&quot;500&quot; height=&quot;281&quot; frameborder=&quot;0&quot; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;


&lt;p&gt;&lt;/p&gt;

&lt;p&gt;Asako Hosobori and Yasuaki Kakehi want to support face to face interaction with Eyefeel &amp;amp; EyeChime.
Although the setup still seems a bit unnatural, I love the direction of the research using technology to enrich our social life and make us focus more on things that are important (away from looking at smartphone screens). Yet, judge yourself.&lt;/p&gt;

&lt;iframe src=&quot;//player.vimeo.com/video/88344477&quot; width=&quot;500&quot; height=&quot;281&quot; frameborder=&quot;0&quot; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;


&lt;h1&gt;Haptics&lt;/h1&gt;

&lt;p&gt;The most far out work regarding output devices was
definitely &amp;ldquo;A Haptic Foot Interface for Language Communication&amp;rdquo; by Erik Hill et. al. They use vibration
motors to convey text messages on your foot. Made me wonder why we don&amp;rsquo;t use our feet more for HCI, regarding how sensitive our feet are and that a large part of our brain is also dedicated to sensing on the foot.&lt;/p&gt;

&lt;p&gt;Max Peiffer et. al. showed how to make free-hand interactions (e.g. with a kinect or similar body tracking system) more realistic using haptic feedback. Nice work!&lt;/p&gt;

&lt;iframe width=&quot;420&quot; height=&quot;315&quot; src=&quot;//www.youtube.com/embed/MMe6-ZG4-ww&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;


&lt;p&gt;The half implant device on a fingernail by Emi Tamaki and Ken Iwasaki was also nice; especially considering that it&amp;rsquo;s already (or soon) a commercial product.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;//www.youtube.com/embed/M2IwAeVe1lg&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;


&lt;p&gt;As always, I particularly liked Inami-Sensei&amp;rsquo;s work.
Suzanne Low presented &amp;ldquo;Pressure Detection on Mobile Phone By Camera and Flash&amp;rdquo;. Very innovative use of the camera and nice demonstrations.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;//www.youtube.com/embed/2MQkFmQr_TI&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;


&lt;p&gt;Also the multi-touch car steering wheel presented by Shunsuke Koyama (you can do gestures anywhere on the wheel) was really well thought out and cool research.&lt;/p&gt;

&lt;h1&gt;Our work&lt;/h1&gt;

&lt;p&gt;We had 3 papers and 1 poster at the conference.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;/papers/cheng2014tip.pdf&quot;&gt;&lt;em&gt;On the Tip of my Tongue &amp;ndash; A Non-Invasive Pressure-Based Tongue Interface&lt;/em&gt;&lt;/a&gt;. Cheng, Jingyuan and Okoso, Ayano and Kunze, Kai and Henze, Niels and Schmidt, Albrecht and Lukowicz, Paul and Kise. Proceedings of the 5th Augmented Human International Conference. 2014. &lt;a href=&quot;papers/bib/cheng2014tip.bib&quot;&gt;Bibtex&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;/papers/ishimaru2014blink.pdf&quot;&gt;&lt;em&gt;In the Blink of an Eye &amp;ndash; Combining Head Motion and Eye Blink Frequency for Activity Recognition with Google Glass&lt;/em&gt;&lt;/a&gt;. Ishimaru, Shoya and Kunze, Kai and Kise, Koichi and Weppner, Jens and Dengel, Andreas and Lukowicz, Paul and Bulling, Andreas. Proceedings of the 5th Augmented Human International Conference. 2014. &lt;a href=&quot;papers/bib/ishimaru2014blink.bib&quot;&gt;Bibtex&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;/papers/shirazi2014what.pdf&quot;&gt;&lt;em&gt;What&amp;rsquo;s on your mind? Mental Task Awareness Using Single Electrode Brain Computer Interfaces&lt;/em&gt;&lt;/a&gt;. Shirazi, Alireza Sahami and Hassib, Mariam and Henze, Niels and Schmidt, Albrecht and Kunze, Kai. Proceedings of the 5th Augmented Human International Conference. 2014. &lt;a href=&quot;papers/bib/shirazi2014what.bib&quot;&gt;Bibtex&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;/papers/iwamura2014havent.pdf&quot;&gt;&lt;em&gt;Haven&amp;rsquo;t we met before? &amp;ndash; A Realistic Memory Assistance System to Remind You of The Person in Front of You&lt;/em&gt;&lt;/a&gt;. Iwamura, Masakazu and Kunze, Kai and Kato, Yuya and Utsumi, Yuzuko and Kise, Koichi. Proceedings of the 5th Augmented Human International Conference. 2014. &lt;a href=&quot;papers/bib/iwamura2014havent.bib&quot;&gt;Bibtex&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m particularly proud of Okoso&amp;rsquo;s and Shoya&amp;rsquo;s work.
They are both still Bachelor students and their
research is already published in an international conference.&lt;/p&gt;

&lt;p&gt;As Shoya was still visiting DFKI in Germany, he sadly
could not attend.
Okoso gave the Tongue Interface presentation and I was impressed by her. It&amp;rsquo;s her first talk at a conference and she&amp;rsquo;s a 3rd year bachelor. The English was perfect, the talk easy to understand and entertaining. Well done!&lt;/p&gt;

&lt;h1&gt;Concluding&lt;/h1&gt;

&lt;p&gt;The full program can be found at the &lt;a href=&quot;http://cse.eedept.kobe-u.ac.jp/ah2014/program/&quot;&gt;AH website&lt;/a&gt; in
case you&amp;rsquo;re looking for the references.
See you next year at &lt;a href=&quot;http://www.augmented-human.com/augmented-human-international-conference-2015&quot;&gt;AH in Singapore&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Beyond FuturICT</title>
   <link href="http://kaikunze.de/posts/Attending-the-symposium-on-service-systems-science"/>
   <updated>2014-03-05T00:00:00+09:00</updated>
   <id>http://kaikunze.de/posts/Attending-the-symposium-on-service-systems-science</id>
   <content type="html">&lt;p class=&quot;lead&quot;&gt;  
Attending the International Symposium on Service Systems Science on the 26.02.2014 in Tokyo, I got a glimpse on how the next steps of FuturICT and how similar projects and efforts are on their way in Japan.
&lt;/p&gt;


&lt;p&gt;Although the &lt;a href=&quot;http://www.futurict.eu&quot;&gt;FuturICT&lt;/a&gt; project did not get funding from
the EU so far (I still believe this was a grave mistake),
I can see that the spirit and our ideas live on. The Japanese COI-T Program focuses on the same issues and problems as FuturICT.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/imgs/pres.jpg&quot; alt=&quot;FuturICT&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.soms.ethz.ch/people/dhelbing&quot;&gt;Dirk Helbing&lt;/a&gt;&amp;rsquo;s presentation gave an overview of the FuturICT effort and the refined research agenda. I really enjoyed seeing how the material has matured.
Also the talks from Shunri Oda and Maso Fukuma addressed similar problems and presented similar conclusions.&lt;/p&gt;

&lt;p&gt;In the afternoon, Cornelius Herstatt gave some interesting observations about the future potential of the Japanese Market. Especially, I liked his conclusions about the use of robots in society: Seeing robots not as replacements to workers but as complementary, allowing more independence and privacy for older adults.&lt;/p&gt;

&lt;h1&gt;Take Home Message&lt;/h1&gt;

&lt;p&gt;Not sure, if I got it right as social and system sciences are quite new to me, but what I took home from the
symposium:
System behavior is determined by connectivity patterns, coupling strength, interactions etc. and small changes in any of them can lead to dramatic changes in the overall system.
Technology today has fundamentally changed connectivity and coupling in human interactions. Yet, so far we develop technology as a standalone in an uncontrolled way. So far, we have little idea how it influences society.&lt;/p&gt;

&lt;p&gt;The Internet made any piece of digital, archival knowledge instantly, globally available.
We are now at the verge of any real life event becoming, instantly globally connected to the digital domain.
With this &amp;ldquo;Internet of Things&amp;rdquo; we might have a perfect substrate and basis to explore these effects. A key word seems &amp;ldquo;participatory social sensing&amp;rdquo;.
&lt;img src=&quot;/imgs/Tokyo.jpg&quot; alt=&quot;FuturICT&quot; /&gt;&lt;/p&gt;

&lt;h1&gt;Discussions and Plenary Summary&lt;/h1&gt;

&lt;p&gt;The discussions centered around the big
picture of society and how to induce beneficial change (as well as to prevent negative effects).
Yet, most strikingly, the panel members also mentioned
some concrete ideas about change and addressed especially
 the research community. There was a long discussion about how to accelerate research, in which Dirk Helbing again emphasized his concept of an &amp;ldquo;Idea Github for researchers&amp;rdquo;: a platform to share ideas /implementations and research results more freely with easy reproducibility and attribution to the corresponding inventors. The main premise is it should not take us 2-3 years to finally publish our findings.&lt;/p&gt;

&lt;p&gt;I ran into this problem, also in the wearable computing field. It is hard for &amp;ldquo;outsiders&amp;rdquo; (researchers not in the community) to enter the field. If they just read papers and work on the published research, they can never work on bleeding edge research. You have to meet with people of the different labs and know what they are working on to find interesting topics (and more important they have to trust you &amp;hellip;).&lt;/p&gt;

&lt;h1&gt;Concluding&lt;/h1&gt;

&lt;p&gt;I&amp;rsquo;m very happy that the FuturICT lives on.
In general, I find the Japanese research community
is very open towards social computing, especially the idea of participatory social sensing. Maybe it&amp;rsquo;s more fruitful to continue the project ideas first here in Japan. It&amp;rsquo;s a bit sad that Europe has given away the chance of being a innovation leader in this field.
Yet meeting Dirk again and seeing how his ideas and research towards social computing matured and developed was also great. Europe might have missed a chance, yet there&amp;rsquo;s still hope ;)&lt;/p&gt;

&lt;p&gt;If you got interested in research about society and social science, I can recommed &lt;a href=&quot;http://www.springer.com/physics/complexity/book/978-3-642-28999-6&quot;&gt;&amp;ldquo;Society is a Complex Matter&amp;rdquo; by Philip Ball&lt;/a&gt;. It&amp;rsquo;s an engaging read for novices to the topic. I could get a copy of at the event.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Glass Talk at Hacker News Kansai</title>
   <link href="http://kaikunze.de/posts/glass-talk-at-hacker-news-kansai"/>
   <updated>2014-02-13T00:00:00+09:00</updated>
   <id>http://kaikunze.de/posts/glass-talk-at-hacker-news-kansai</id>
   <content type="html">&lt;p class=&quot;lead&quot;&gt; Introducing Google Glass to Hacker News Kansai Community was a lot of fun. &lt;/p&gt;


&lt;p&gt;&lt;img src=&quot;/imgs/glass3.jpg&quot; alt=&quot;Glass 1&quot; /&gt;
&lt;img src=&quot;/imgs/glass4.jpg&quot; alt=&quot;Glass 2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m sorry it took so long to put online.
Too much to do, especially due to Augmented Human 2014 (really looking forward to the beginning of march) :)&lt;/p&gt;

&lt;p&gt;As part of our monthly &lt;a href=&quot;http://hnkansai.org&quot;&gt;Hacker News meetup&lt;/a&gt;in Kansai,
I gave a small introduction on how to use (and hack for) Google Glass.&lt;/p&gt;

&lt;script class=&quot;speakerdeck-embed&quot; data-id=&quot;a241cc30769d0131bdae2a0fad9cf655&quot; data-ratio=&quot;1.33333333333333&quot; src=&quot;//speakerdeck.com/assets/embed.js&quot;&gt; &lt;/script&gt;


&lt;p&gt;The slides are also available as &lt;a href=&quot;../../slides/kansai_15.pdf&quot;&gt;pdf download&lt;/a&gt; (7 MB).&lt;/p&gt;

&lt;p&gt;The &amp;ldquo;Hacks&amp;rdquo; are fairly simple. I more or less show how to develop for Glass using the Native Development Kit and some demonstrations how to read out sensors or other demo code I scraped from github and adjusted (see the slides for links to the sources). The examples include CameraZoom, Face Detection and Blink Recognition.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/imgs/glass0.jpg&quot; alt=&quot;Glass 3&quot; /&gt;
&lt;img src=&quot;/imgs/glass2.jpg&quot; alt=&quot;Glass 4&quot; /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Looking back at 30C3</title>
   <link href="http://kaikunze.de/posts/looking-back-at-30c3"/>
   <updated>2014-01-28T00:00:00+09:00</updated>
   <id>http://kaikunze.de/posts/looking-back-at-30c3</id>
   <content type="html">&lt;p class=&quot;lead&quot;&gt;Impressions and Talk Recommendations. Finally, a couple of days with normal people ...&lt;/p&gt;


&lt;p&gt;Honestly I was impressed by the professionalism of the 30th Chaos Communication Congress. It changed a lot from the last time I visited ( 25c3), grew bigger without loosing its atmosphere. With the assemblies and workshops the event starts to get more and more interactive.&lt;/p&gt;

&lt;h2&gt;Talk Recommendations&lt;/h2&gt;

&lt;p&gt;I link to the youtube streams of the recordings
yet you can get them also over at &lt;a href=&quot;http://media.ccc.de/browse/congress/2013/&quot;&gt;media.ccc.de&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Due to recent events, there were a lot of Snowden/NSA themed talks. I skipped most of them (as some were just plain to catch attention, others were simply to depressing).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.youtube.com/watch?v=HTVgPw7TR_k&quot;&gt;Seeing The Secret State: Six Landscapes&lt;/a&gt; is a must watch on the topic. I don&amp;rsquo;t want to spoil to much, just watch it.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.youtube.com/watch?v=52NbMghN4ws&quot;&gt;Machines that make&lt;/a&gt; DIY is always fun. Although talking with Nadya Peek after the presentation, I think she could have given a better talk (the chat was more interesting).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.youtube.com/watch?v=2o2xBOQeB7Q&quot;&gt;How to Build a Mind&lt;/a&gt; Relatively high level talk about artificial
intelligence and related topics, quite entertaining.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.youtube.com/watch?v=Er9luiBa32k&quot;&gt;FPGA 101&lt;/a&gt;
Karsten Becker gives a good overview about why and when you want/don&amp;rsquo;t want to use FPGAs. I have not much experience, yet usually programming FPGAs sucked the few days I tried it.
The toolchain he introduces sounds cool.
As a friend of mine said the right level
of nerdiness enough to be interesting and
not too much to lose the audience.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.youtube.com/watch?v=CPEzLNh5YIo&quot;&gt;SD-Card Exploits&lt;/a&gt;
So SD-cards have micro processors and guess what you can program them yourself. Quite scary :)&lt;/p&gt;

&lt;h2&gt;My Talk Feedback&lt;/h2&gt;

&lt;p&gt;I was a bit surprised how positive it was.
As the talk is kind of scary. If you imagine
we are really able to infer how much somebody
understands about a material using eye gaze,
who should be allowed to access this information?
Is it alright, to trust Google, Apple, Samsung etc.
with this type of information? It might no longer be
&amp;ldquo;just&amp;rdquo; our personal communication that can be
recorded and analyzed, but also our reading habits and
comprehension level.&lt;/p&gt;

&lt;p&gt;If you haven&amp;rsquo;t seen it yet, feel free &lt;a href=&quot;http://www.youtube.com/embed/LrARH4KJRro&quot;&gt;to watch it&lt;/a&gt;.
You can also &lt;a href=&quot;https://frab.cccv.de/en/30C3/public/events/5387/feedback/new&quot;&gt;give feedback&lt;/a&gt; on the talk if you want.
Highly appreciated.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>30C3 Toward a Cognitive Quantified Self</title>
   <link href="http://kaikunze.de/posts/30c3-toward-a-cognitive-quantified-self"/>
   <updated>2013-12-28T00:00:00+09:00</updated>
   <id>http://kaikunze.de/posts/30c3-toward-a-cognitive-quantified-self</id>
   <content type="html">&lt;p class=&quot;lead&quot;&gt;Discussing activity recognition for the Mind and its potential applications.
&lt;/p&gt;


&lt;p&gt;Here are my &lt;a href=&quot;/slides/30c3.pdf&quot;&gt;talk slides&lt;/a&gt; for my 30c3 talk &amp;hellip; Sorry it took so long.
Thanks for the great feedback, will write a bit
more if I have sometime ;)&lt;/p&gt;

&lt;p&gt;The video is also online. This happened very quick
after the event. I&amp;rsquo;m impressed by the 30c3 content team.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;//www.youtube.com/embed/LrARH4KJRro&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;




&lt;script class=&quot;speakerdeck-embed&quot; data-id=&quot;6535ee80517b0131e1f61635afb1c999&quot; data-ratio=&quot;1.77777777777778&quot; src=&quot;//speakerdeck.com/assets/embed.js&quot;&gt; &lt;/script&gt;


&lt;p&gt;Thanks a lot for the great reviews and suggestions.
They are highly appreciated. You can still &lt;a href=&quot;https://frab.cccv.de/en/30C3/public/events/5387/feedback/new&quot;&gt;review
the talk&lt;/a&gt;.&lt;/p&gt;

&lt;h2&gt;Talk Abstract&lt;/h2&gt;

&lt;p&gt;The talk gives an overview about our work of quantifying knowledge acquisition tasks in real-life environments, focusing on reading. We combine several pervasive sensing approaches (computer vision, motion-based activity recognition etc.) to tackle the problem of recognizing and classifying knowledge acquisition tasks with a special focus on reading.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Hacking Glass</title>
   <link href="http://kaikunze.de/posts/hacking-glass"/>
   <updated>2013-12-22T00:00:00+09:00</updated>
   <id>http://kaikunze.de/posts/hacking-glass</id>
   <content type="html">&lt;p class=&quot;lead&quot;&gt;
So I got my hands on another Glass device and can now
play with it a bit longer.
&lt;/p&gt;


&lt;p&gt;Disclaimer: Rooting and flashing your device voids your warranty and can brick your Glass. Also you won&amp;rsquo;t receive OTA updates afterwards.
This is not an instruction manual. I just use it as
a scratch pad to give a record what I did and
what worked for me. The commands below will erase all data on your device. Proceed at your own risk.&lt;/p&gt;

&lt;h2&gt;Rooting and Flashing Images&lt;/h2&gt;

&lt;p&gt;To get root follow the instructions from Google.
Unfortunately, the fastboot under Mac OS does not work.
I could use a virtual machine on my Mac with ubuntu
to get root and flash images.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;adb devices
adb reboot-bootloader
fastboot devices

fastboot oem unlock
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I needed to execute the last command twice. The first time
it just asked me if I was sure if I want to void my
warranty etc.&lt;/p&gt;

&lt;p&gt;Next I flashed the boot image from the &lt;a href=&quot;&quot;&gt;Glass developer page&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fastboot flash boot boot.img
fastboot reboot
adb root
adb shell
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you want to update to a new OTA (in my case XE12) and
rooted your device,
you can download the zip with all necessary images from
Google. It&amp;rsquo;s cool that they support rooting and
flashing (even if it voids your warranty).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fastboot flash boot boot.img
fastboot flash system system.img
fastboot flash recovery recovery.img
fastboot flash userdata userdata.img
fastboot erase cache
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Reading out the Proximity Sensor&lt;/h2&gt;

&lt;p&gt;I&amp;rsquo;m most interested in accessing the proximity
sensor facing the eye. So thanks to Philip Scholl&amp;rsquo;s and
Shoya&amp;rsquo;s help, I was able to do it.
The device is under&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;adb root
adb shell
&amp;gt; cat /sys/bus/i2c/devices/4-0035/proxraw
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Gives back one raw proximity value from
the sensor. Unfortunately without timestamp.
If you want to read out the proximity
data from Android Apps etc. you need to change
the access rights.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;chmod 664 /sys/bus/i2c/devices/4-0035/proxraw
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Privacy Enhancement&lt;/h2&gt;

&lt;p&gt;As I will be visiting the &lt;a href=&quot;&quot;&gt;Chaos Communication Congress&lt;/a&gt;
next weekend, I wanted to &amp;ldquo;privacy enhance&amp;rdquo; GLASS
for the event. I want to wear Glass but don&amp;rsquo;t
really need the camera functionality.&lt;/p&gt;

&lt;p&gt;So I used my &lt;a href=&quot;http://www.the3doodler.com&quot;&gt;3Doodler&lt;/a&gt; to make a simple attachment to block the camera of.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/imgs/doodler.jpg&quot; alt=&quot;Doodler&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The tricky part is that the light sensor for adjusting
screen brightness is directly under the camera.
If it&amp;rsquo;s blocked the screen will be very dark.
Here&amp;rsquo;s the &amp;ldquo;privacy enhanced&amp;rdquo; Google Glass version.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/imgs/glass_p.jpg&quot; alt=&quot;Glass Enhanced&quot; /&gt;&lt;/p&gt;

&lt;p&gt;and a picture taken by it. It&amp;rsquo;s not completely black
due to the issue with the light sensor, yet I
think it&amp;rsquo;s a start ;)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/imgs/glass_p2.jpg&quot; alt=&quot;Glass Enhanced&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here is the very &lt;a href=&quot;/imgs/glass.stencil.pdf&quot;&gt;basic stencil&lt;/a&gt; I used to build the attachment.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Bits and Bytes instead of a Bookshelf</title>
   <link href="http://kaikunze.de/posts/bits-and-bytes-instead-of-a-book-shelf"/>
   <updated>2013-11-29T00:00:00+09:00</updated>
   <id>http://kaikunze.de/posts/bits-and-bytes-instead-of-a-book-shelf</id>
   <content type="html">&lt;p class=&quot;lead&quot;&gt; An interview made me wonder about how reading habits are changing and how we will narrate stories in the future. &lt;/p&gt;


&lt;p&gt;Recently, I gave an interview for the German online issue
of the Scientific American (&lt;a href=&quot;http://spectrum.de/&quot;&gt;Spectrum der Wissenschaft&lt;/a&gt;) for a &lt;a href=&quot;http://www.spektrum.de/alias/lesen/bits-und-bytes-statt-buchregal/1211519&quot;&gt;special about reading habits (in German, paywall)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As I&amp;rsquo;m interested in the topic, I often hear
that &amp;ldquo;endless scrolling&amp;rdquo; is bad as it destroys
the mental map we make of books and pages or
that reading from backlit screens is eye-straining
inducing headaches. Personally, I cannot really
understand these complaints. I&amp;rsquo;m doing most
of my reading on tablet devices or computer screens
though I never experienced these problems directly.&lt;/p&gt;

&lt;p&gt;However, active reading -the process of working with the text through highlight, notes, marks- is still better
on paper. In Human Computer interaction terms, people talk about &lt;a href=&quot;http://en.wikipedia.org/wiki/Affordance&quot;&gt;affordances&lt;/a&gt;. The affordance of paper is very high for active reading. So I find myself still printing out drafts
or review papers, especially if it&amp;rsquo;s a close call and
I need to concentrate on the contents. In later case, I even like to change the reading environment, moving away
from my laptop/desktop to a meeting table or a bank outside. I believe this helps me concentrate, deliberately shutting out any distractions.&lt;/p&gt;

&lt;p&gt;We just started to modify the reading experience using electronic devices.
So far most of the applications and reading devices
directly mimic the book. We have &amp;ldquo;ebooks&amp;rdquo;, &amp;ldquo;e-reading&amp;rdquo; software
use pages and page turns etc. I believe there is a lot of room for improvement related to reading on screens.&lt;/p&gt;

&lt;p&gt;Given the possibility to assess the user&amp;rsquo;s mental state using
&lt;a href=&quot;/papers/kunze2013activity.pdf&quot;&gt;Cognitive Activity Recognition&lt;/a&gt;, we
can change content, structure and style of reading materials dynamically.
Most straight forward, if an application detects that a reader looses
interest, it could prompt her with an interactive challenge/video or
similar. Changing fonts, colors and lettering according to mood and
context could be also interesting. There is a fairly new
playground opening up for anybody curious and interested in
defining new forms of reading.&lt;/p&gt;

&lt;p&gt;Interesting further reading:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.fxpal.com/publications/FXPAL-PR-98-053.pdf&quot;&gt;Shilit et. al. Beyond Paper: Supporting Active Reading with Free Form Digital Ink Annotations&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://courses.cs.vt.edu/~cs5714/fall2003/Affordances,%20as%20appeared.pdf&quot;&gt;Hartson. Cognitive, physical, sensory, and functional affordances in interaction design&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://hci.ucsd.edu/hollan/Pubs/piperCHI2009.pdf&quot;&gt;Piper et. al. Tabletop Displays for Small Group Study: Affordances of Paper and Digital Materials&lt;/a&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Amazing Okinawa - Attending the ASVAI Workshop</title>
   <link href="http://kaikunze.de/posts/amazing-okinawa-attending-the-asvai-workshop"/>
   <updated>2013-11-05T00:00:00+09:00</updated>
   <id>http://kaikunze.de/posts/amazing-okinawa-attending-the-asvai-workshop</id>
   <content type="html">&lt;p class=&quot;lead&quot;&gt;Cool research discussions at a nice location. The workshop was perfect fit to my research interests.&lt;/p&gt;


&lt;p&gt;The &lt;a href=&quot;http://www.am.sanken.osaka-u.ac.jp/ASVAI2013/&quot;&gt;ASVAI workshop&lt;/a&gt; gave a good overview about several research
efforts part of and related to the &lt;a href=&quot;http://www.jst.go.jp/kisoken/crest/en/research_area/ongoing/areah21-1.html&quot;&gt;JST CREST&lt;/a&gt; and the JSPS Core-to-Core Sanken Program.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.am.sanken.osaka-u.ac.jp/~yagi/&quot;&gt;Prof. Yasushi Yagi&lt;/a&gt;
showed how to infer intention from gait analysis.
Interestingly, he showed research about the relationship
of gaze and gait.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://ai.stanford.edu/~alireza/&quot;&gt;Dr. Alireza Fathi&lt;/a&gt;
presented cool work about ego centric cameras. He showed
how to estimate gaze using ego centric cameras during
cooking tasks and psychological studies.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.uh.edu/class/psychology/about/people/hanako-yoshida/index.php&quot;&gt;Prof. Hanako Yoshida&lt;/a&gt; explores
social learning in infants (equipping children with mobile
eye trackers &amp;hellip; awesome!), inferring developmental stages
giving more insights in the learning process.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.irc.atr.jp/~m-shiomi/&quot;&gt;Prof. Masahiro Shiomi&lt;/a&gt;
spoke about his research trying to adapt robot behavior
to fit into social public spaces ( videos about
people running away from a robot included ;) ).
Currently, they focus on service robots and model their
behavior according to successful human service personnel.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.hci.iis.u-tokyo.ac.jp/~ysato/&quot;&gt;Prof. Yoichi Sato&lt;/a&gt; presented work related to
detecting visual attention. They use visual saliency
on video to train an appearance-based eye tracking.
Really interesting work, I had a chance to talk a bit
more with &lt;a href=&quot;http://www.hci.iis.u-tokyo.ac.jp/~sugano/&quot;&gt;Yusuke Sugano&lt;/a&gt;, cool research :)&lt;/p&gt;

&lt;p&gt;Of course, Koichi also gave an overview about our work.
If you want to read more, checkout the &lt;a href=&quot;/papers/kunze2013activity.pdf&quot;&gt;IEEE Computer article&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m looking forward to the main conference.
Here&amp;rsquo;s a tag cloud using the abstracts of ACPR and ASVAI papers:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/imgs/acpr_wordcloud.png&quot; alt=&quot;Tag cloud&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We present demonstrations and new results
of the eye tracking on commodity
tablets/smart phones and a sharing infrastructure for our document annotation for smart phones.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>A Week with Glass</title>
   <link href="http://kaikunze.de/posts/a-week-with-glass"/>
   <updated>2013-09-20T00:00:00+09:00</updated>
   <id>http://kaikunze.de/posts/a-week-with-glass</id>
   <content type="html">&lt;p class=&quot;lead&quot;&gt; How my grandparents interact with GLASS, showed me that Google seems to be onto something. &lt;/p&gt;


&lt;p&gt;As there were a lot of researchers visiting for the Ubicomp /ISWC conferences, I could grab a Google Glass from one of the participants for a week. Thanks to the anonymous
donor (to avoid speculations it was none of the people mentioned below).
In the following some unsorted thoughts &amp;hellip; Sorry for typos etc. It&amp;rsquo;s more a scratch pad so I don&amp;rsquo;t forget the impressions I had.&lt;/p&gt;

&lt;h2&gt;First Impressions&lt;/h2&gt;

&lt;p&gt;The Glass device feels expensive and a little bit futuristic.
I&amp;rsquo;m impressed by the build quality and design. It has also a &amp;ldquo;google&amp;rdquo; feel to it, e.g. &amp;ldquo;funny&amp;rdquo; jokes in the manual (&amp;ldquo;don&amp;rsquo;t use glass for scuba diving &amp;hellip;&amp;rdquo;).
The display works extremely well and although glass is made for micro-interactions (quickly checking an email/sms, google now updates, making pictures), I could watch videos and read longer emails/documents on it without trouble and any sight problems (I experienced no headaches as happend with other setups, see below). It would be perfect for boring meetings if other people could not see what you are doing &amp;hellip;
I assume the Glass design team made the conscious decision, to let other people know if you interact with glass. People can see if the screen is on and even recognize what&amp;rsquo;s on the screen if they get close enough.&lt;/p&gt;

&lt;h2&gt;Grandparents and Mother with Google Glass&lt;/h2&gt;

&lt;p&gt;I have a basic test for technology or research topics in general.
I try to explain it to my grandparents and mother to see if they understand it and find it interesting.
Various head-mounted displays, tablets and activity recognition algorithms were tested this way &amp;hellip;
E.g. they were not so big fans of tablets/slates or smart phones until they played with an iPhone and iPad.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/imgs/oma.jpg&quot; alt=&quot;oma&quot; /&gt;
&lt;img src=&quot;/imgs/opa_sh.jpg&quot; alt=&quot;opa&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Surprisingly, my grandparents did not have the reservation they have towards other computing devices.
Usually, they have the feeling that they could destroy something and are extra careful/hesitant.
Yet, Google Glass looks like glasses, so it was easy for them to setup and use.
The system worked quite well (although so far only English is supported), speech recognition
and touch interface were simple to learn after a quick 5 min. introduction. I was surprised myself &amp;hellip;&lt;/p&gt;

&lt;p&gt;Sadly, the speech interface does a poor job with German names, e.g. googleing for &amp;ldquo;Apfelkuchen Rezept&amp;rdquo; (Apple cake recipe) did not work as intended.&lt;/p&gt;

&lt;p&gt;Yet, both of them saw potential in Glass and could imagine wearing it during the day.
I was most astound by the application cases they came up with.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/imgs/opa_pills.jpg&quot; alt=&quot;opa pills&quot; /&gt;&lt;/p&gt;

&lt;p&gt;My grandfather took a picture of his pills he needs to take after each meal.
He told me, he always wonders if he has taken them or not and sometimes checks 2-3 times after
a meal to be certain. Taking a picture and using the touch panel to browse recent pictures (with timestamp), he can easily figure out when he took them the last time.&lt;/p&gt;

&lt;p&gt;My grandmother would love to use Glass for gardening. It happens sometimes, that she gets a
phone call during garden work and then she has to change shoes, take of gloves etc. and hurry
to the portable phone. Additionally, she likes to get the advice of my mum or friends about
where to put which flower seeds etc. so she asked me if it&amp;rsquo;s possible to show the video stream from
Glass to other people over the Internet :)&lt;/p&gt;

&lt;p&gt;We also did a practise test, My grandmother and mother wore Glass during shopping in Karlsruhe.
Both of them wear glasses, so not too many people noticed or looked at them. I think they assumed it&amp;rsquo;s some kind of medical device or sight improvement etc.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/imgs/oma2.jpg&quot; alt=&quot;oma ka&quot; /&gt;
&lt;img src=&quot;/imgs/mum.jpg&quot; alt=&quot;mum ka&quot; /&gt;&lt;/p&gt;

&lt;p&gt;My mother used the time line in glass to track when she made the pictures and traced back when she saw something nice to figure out at which store the item was. She tried taking pictures of price tags. Unfortunately, the resolution on the screen is not high enough
to read the price, yet this could be easily fixed with a zoom function for pictures.
Interestingly, she also carries a smart phone, yet she never got the idea to use it for shopping like Glass.&lt;/p&gt;

&lt;h2&gt;Public Reactions&lt;/h2&gt;

&lt;p&gt;As mentioned my mum and grandmother wore Glass nearly unnoticed.
This is quite different to my experience &amp;hellip; If I wear it in public, most people in Karlsruhe
and Mannheim (the two cities I tried) eyed at me with wary faces (you can see the questions in their eyes : &amp;ldquo;What is he wearing ?? Some medical device ?? NERD!! &amp;rdquo;). This was particularly bad
when I spoke with a clerk or a person directly, as they kept staring at Glass instead of looking into my eyes ;)
Social reception was better when I was with my family. Strangely, people asked mostly my grandmother
what I was wearing. Very few approached me directly.
Reactions fell into 3 broad categories:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&amp;ldquo;WOW Cool &amp;hellip; Glass! How is it? Can I try??&amp;rdquo; &amp;mdash; &lt;em&gt;Note&lt;/em&gt; : Before it&amp;rsquo;s released in public, I strongly recommend not wearing it on any campus with a larger IT faculty. I did not account for that and it was quite difficult to get over Karlsruhe University Campus :)&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Stop violating my privacy!&amp;rdquo; &amp;mdash; During the week I had only one person directly approach me about privacy concerns. The person was quite angry at first. I believe it&amp;rsquo;s mostly due to misinformation (something Google needs to take serious), as he believed Glass would stream automatically everything to Google and listen to all the conversations etc.. After I showed him the functionality of the device, how to use it and how to see if somebody is using it, he was calmer and actually liked it (could see the potential of a wearable display).&lt;/li&gt;
&lt;li&gt;&amp;ldquo;What&amp;rsquo;s wrong with this guy?&amp;rdquo; &amp;mdash; Especially if I was traveling alone people stared at me. I asked 1 or 2 of the most obnoxious persons starring at me about it and they answered they thought I was wearing a medical device and they wondered &amp;ldquo;what&amp;rsquo;s wrong with me&amp;rdquo; as I looked otherwise &amp;ldquo;normal&amp;rdquo;.&lt;/li&gt;
&lt;/ol&gt;


&lt;h2&gt;Some Issues&lt;/h2&gt;

&lt;p&gt;The 3 biggest issues I had with it:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Weight and placement &amp;ndash; You need to get used to its weight. As I&amp;rsquo;m not wearing prescription glasses, it feels strange to me wearing something on my nose. It&amp;rsquo;s definitely heavier than glasses. After a couple of hours it is ok. Also it&amp;rsquo;s always in your peripheral view, you need to get used to it.&lt;/li&gt;
&lt;li&gt;Battery life &amp;ndash; Ok, I played a lot with it, given I could use Glass only for a week. At the end (when me playing with it got fewer) I could get barely a day of usage. I expect that&amp;rsquo;s something they can easily fix. Pst&amp;hellip; you can also plug-in a portable USB battery to charge during usage :)&lt;/li&gt;
&lt;li&gt;Social acceptance &amp;ndash; This is the hardest one to crack. Having used Glass, I don&amp;rsquo;t understand most of the privacy fears people raise. It&amp;rsquo;s very obvious if a person is using the device/taking a picture etc. If I want to take covert pictures/videos of people, I believe it&amp;rsquo;s easier to do with today&amp;rsquo;s smart phones or spy cameras (available on Amazon for example) &amp;hellip;&lt;/li&gt;
&lt;/ol&gt;


&lt;h2&gt;Some more Context&lt;/h2&gt;

&lt;p&gt;When I unboxed Glass, I remembered how Paul, my phD. advisor, and Thad (Glass project manager)
chatted about how in future everybody would wear some kind of head-mounted
display and a computing device always connected to the
Internet, helping us with everyday tasks &amp;ndash; augmentations
to our brain.&lt;/p&gt;

&lt;p&gt;In the past, Paul was not a huge enthusiast about wearable displays and I agreed
with him. I attempted to use the &lt;a href=&quot;https://en.wikipedia.org/wiki/Optical_head-mounted_display#MicroOptical_.2F_MyVu&quot;&gt;MicroOptical&lt;/a&gt; (the display used by Thad) several times and had always terrible headaches afterwards &amp;hellip; Just not for me.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/imgs/me.jpg&quot; alt=&quot;Me&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Around 2004 &amp;ndash; 2010, I played with various wearable setups to use during everyday life during my phD. each only for a week or couple of days. If you work on wearable computing you have to try at least. As seen in the picture above, the only setup working for me was a Prototype HMD from Zeiss with the
&lt;a href=&quot;http://www.qbic.ethz.ch&quot;&gt;Qbic&lt;/a&gt;, an awesome belt-integrated linux pc by ETH (black belt buckle in the picture), and &lt;a href=&quot;http://www.handykey.com&quot;&gt;Twiddler 2&lt;/a&gt;. Yet, I stopped using it as the glasses were quite heavy, maintaining/adjusting the software was a hassle (compared to the advantages) and -I have to admit- due to social pressure, imagine living as a cyborg in a small Bavarian town, mostly occupied by law and business students &amp;hellip; I found my small, black, analog notebook more handy and less intimidating to other people. Today, I&amp;rsquo;m an avid iPhone user (Things, Clear, Habit List, Textastic, Prompt and Lendromat  &amp;hellip;).&lt;/p&gt;

&lt;h2&gt;To sum up&lt;/h2&gt;

&lt;p&gt;In total I was quite sceptical at first, the design reminded me too much on the Microoptical and the headaches I got using it. Completely unfounded! Even given the social acceptance issue, I cannot wait to get Glass for a longer test. However, I really need a good note taking app, running vim on glass would already be a selling point for me, replacing my black notebook (and maybe smart phone?). I undusted my Twiddler2 (took a long time to find it in the cellar) with hacked bluetooth connection, started practicing again and hope I can try it soon with Vim for Glass :D This is definitely not an application case for the mass market &amp;hellip;  My grandparents told me that they believe there is a broader demand for such a device also by &amp;ldquo;normal&amp;rdquo; people (they actually want to use it!). So let&amp;rsquo;s see.&lt;/p&gt;

&lt;p&gt;Plus the researcher in my cannot wait to get easy accessible motion sensors onto the heads of a lot of people. Combined with the sensors in your pocket it&amp;rsquo;s activity recognition heaven!&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s &lt;a href=&quot;https://news.ycombinator.com/item?id=6456220&quot;&gt;discuss on Hacker News&lt;/a&gt; if you want.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Ubicomp ISWC Impressions</title>
   <link href="http://kaikunze.de/posts/ubicom-iswc-impressions"/>
   <updated>2013-09-15T00:00:00+09:00</updated>
   <id>http://kaikunze.de/posts/ubicom-iswc-impressions</id>
   <content type="html">&lt;p&gt;Usually, I&amp;rsquo;m not such a big fan of conference openings,
yet Friedemann Mattern provided a great intro giving
an overview about the origins of Pervasive and Ubicom
mentioning all important people and showing nice vintage
pictures from Hans Gellersen, Alois Ferscha,  Marc Langheinrich, Albrecht Schmidt, Kristof Van Laerhoven etc.&lt;/p&gt;

&lt;p&gt;Deeply impressed by the organization, social and general
talk quality, I was a bit sceptical before the merger
of Pervasive / Ubicom and collocating ISWC, yet it was
completely unfounded.&lt;/p&gt;

&lt;p&gt;We got some great feedback for Kazuma&amp;rsquo;s and Shoya&amp;rsquo;s demos.
They both did a great job introducing their work about:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;/papers/kunze2013my.pdf&quot;&gt;My Reading Life ‚Äì Towards Utilizing Eyetracking on Unmodified Tablets and Phones&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/papers/kunze2013annotate.pdf&quot;&gt;Annotate Me ‚Äì Supporting Active Reading using Real-Time Document Image Retrieval On Mobile Devices&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;We got also a lof of interest and feedback
to Andreas Bulling&amp;rsquo;s and my work about recognizing
document types using only eye gaze.
By the way, below are the talk slides and the abstract of
the paper.&lt;/p&gt;

&lt;h2&gt;ISWC Talk Slides&lt;/h2&gt;

&lt;script class=&quot;speakerdeck-embed&quot; data-id=&quot;a0f70c60fdc20130468e062acf92b5fe&quot; data-ratio=&quot;1.33333333333333&quot; src=&quot;//speakerdeck.com/assets/embed.js&quot;&gt;  &lt;/script&gt;


&lt;h2&gt;Abstract&lt;/h2&gt;

&lt;p&gt;Reading is a ubiquitous activity that many people even per- form in transit, such as while on the bus or while walking. Tracking reading enables us to gain more insights about ex- pertise level and potential knowledge of users ‚Äì towards a reading log tracking and improve knowledge acquisition. As a first step towards this vision, in this work we investigate whether different document types can be automatically de- tected from visual behaviour recorded using a mobile eye tracker. We present an initial recognition approach that com- bines special purpose eye movement features as well as ma- chine learning for document type detection. We evaluate our approach in a user study with eight participants and five Japanese document types and achieve a recognition perfor- mance of 74% using user-independent training.&lt;/p&gt;

&lt;p&gt;Full paper link:
&lt;a href=&quot;/papers/2013Kunze-5.pdf&quot;&gt;I know what you are reading ‚Äì Recognition of Document Types Using Mobile Eye Tracking&lt;/a&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Excited about Ubicomp and ISWC</title>
   <link href="http://kaikunze.de/posts/excited-about-ubicomp-and-iswc"/>
   <updated>2013-09-06T00:00:00+09:00</updated>
   <id>http://kaikunze.de/posts/excited-about-ubicomp-and-iswc</id>
   <content type="html">&lt;p&gt;This year I&amp;rsquo;m really looking forward to &lt;a href=&quot;http://ubicomp.org&quot;&gt;Ubicomp&lt;/a&gt; and &lt;a href=&quot;http://iswc.net&quot;&gt;ISWC&lt;/a&gt;,
it&amp;rsquo;s the first time that Ubicomp and Pervasive merged into one conference and it&amp;rsquo;s the first time the venue sold out with 700 participants.&lt;/p&gt;

&lt;p&gt;I cannot wait to chat with old friends and experts (most are both :)).&lt;/p&gt;

&lt;p&gt;The field slowly matures. Especially, the wearable research is really pushing towards prime-time. Most prominently, Google Glass is getting a lot of focus also discussing its impacts on privacy. Yet, there is more and more talk about fitness bracelets/trackers and smart watches. I expect that we see more intelligent clothes and activity recognition work in commercial products in the coming years.&lt;/p&gt;

&lt;p&gt;By the way, we have 3 poster papers and 2 demos at Ubicomp
and a short paper at ISWC.&lt;/p&gt;

&lt;h3&gt;Ubicomp Demos and Posters&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;/papers/kunze2013my.pdf&quot;&gt;My Reading Life ‚Äì Towards Utilizing Eyetracking on Unmodified Tablets and Phones&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/papers/kunze2013annotate.pdf&quot;&gt;Annotate Me ‚Äì Supporting Active Reading using Real-Time Document Image Retrieval On Mobile Devices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/papers/cheng2013activity.pdf&quot;&gt;Activity Recognition and Nutrition Monitoring in Every Day Situations with a Textile Capacitive Neckband&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;ISWC paper&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;/papers/kunze2013know.pdf&quot;&gt;I know what you are reading ‚Äì Recognition of Document Types Using Mobile Eye Tracking&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Drop by at the demo,poster sessions and/or see me my talk on Thursday.&lt;/p&gt;

&lt;p&gt;On a side note, Ubicomp really picks great locations. This year it&amp;rsquo;s Zurich, next year Seattle and the year after it will be in Osaka. Seems I might be staying longer in Japan, than I originally planned ;)&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>ICDAR 2013 Talk Slides Online</title>
   <link href="http://kaikunze.de/posts/icdar-2013-talk-slides-online"/>
   <updated>2013-08-26T00:00:00+09:00</updated>
   <id>http://kaikunze.de/posts/icdar-2013-talk-slides-online</id>
   <content type="html">&lt;p&gt;The slides for my two talks today are online now.&lt;/p&gt;

&lt;h2&gt;The Wordometer&lt;/h2&gt;

&lt;script class=&quot;speakerdeck-embed&quot; data-id=&quot;05bef8c0f0aa01300a6d065f623bb37c&quot; data-ratio=&quot;1.33333333333333&quot; src=&quot;//speakerdeck.com/assets/embed.js&quot;&gt;  &lt;/script&gt;


&lt;h2&gt;Reading activity recognition using an off-the-shelf EEG&lt;/h2&gt;

&lt;script class=&quot;speakerdeck-embed&quot; data-id=&quot;8a535520f0a901304b9d56bebfa6c61b&quot; data-ratio=&quot;1.33333333333333&quot; src=&quot;//speakerdeck.com/assets/embed.js&quot;&gt; &lt;/script&gt;


&lt;p&gt;For more details, check out the papers:
* &lt;a href=&quot;/papers/kunze2013wordometer.pdf&quot;&gt;The Wordometer ‚Äì Estimating the Number of Words Read Using Document Image Retrieval and Mobile Eye Tracking&lt;/a&gt;
* &lt;a href=&quot;/papers/kunze2013reading.pdf&quot;&gt;Reading Activity Recognition using an off-the-shelf EEG ‚Äî Detecting Reading Activities and Distinguishing Genres of Documents&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;They are both published at &lt;a href=&quot;http://icdar2013.org&quot;&gt;ICDAR 2013&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Recognizing Reading Activities</title>
   <link href="http://kaikunze.de/posts/recognizing-reading-activities"/>
   <updated>2013-08-23T00:00:00+09:00</updated>
   <id>http://kaikunze.de/posts/recognizing-reading-activities</id>
   <content type="html">&lt;script class=&quot;speakerdeck-embed&quot; data-id=&quot;e9181bd0ee3501308d2a1ad4cad5346c&quot; data-ratio=&quot;1.2994923857868&quot; src=&quot;//speakerdeck.com/assets/embed.js&quot;&gt; &lt;/script&gt;


&lt;p&gt;Just finished my keynote talk at CBDAR (Workshop of &lt;a href=&quot;http://icdar2013.org/&quot;&gt;ICDAR&lt;/a&gt;),
got a lot of questions and have a lot of new research ideas :)&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m pretty ignorant about Document Analysis (and Computer Vision in general), so it&amp;rsquo;s great to talk to some
experts in the field. Pervasive Computing and Document Analysis are very complementary
and as such interesting to combine.&lt;/p&gt;

&lt;p&gt;Here are my talk slides, followed by the talk abstract.&lt;/p&gt;

&lt;h2&gt;Real-life Activity Recognition &amp;ndash; Talk Abstract&lt;/h2&gt;

&lt;p&gt;Most applications in intelligent environments so far strongly rely on specific sensor combinations at predefined positions, orientations etc. While this might be acceptable for some application domains (e.g. industry), it hinders the wide adoption of pervasive computing. How can we extract high level information about human actions and complex real world situations from heterogeneous ensembles of simple, often unreliable sensors embedded in commodity devices?&lt;/p&gt;

&lt;p&gt;This talk mostly focuses on how to use body-worn devices for activity recognition in general, and how to combine them with infrastructure sensing and computer vision approaches for a specific high level human activity, namely better understanding knowledge acquisition (e.g. recognizing reading activities).&lt;/p&gt;

&lt;p&gt;We discuss how placement variations of electronic appliances carried by the user influence the possibility of using sensors integrated in those appliances for human activity recognition. I categorize possible variations into four classes: environmental placements, placement on different body parts (e.g. jacket pocket on the chest, vs. a hip holster vs. the trousers pocket), small displacement within a given coarse location (e.g. device shifting in a pocket), and different orientations.For each of these variations, I give an overview of our efforts to deal with them.&lt;/p&gt;

&lt;p&gt;In the second part of the talk, we combine several pervasive sensing approaches (computer vision, motion-based activity recognition etc.) to tackle the problem of recognizing and classifying knowledge acquisition tasks with a special focus on reading. We discuss which sensing modalities can be used for digital and offline reading recognition, as well as how to combine them dynamically.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Wordometer and Document Analysis using Pervasive Sensing</title>
   <link href="http://kaikunze.de/posts/wordometer-and-document-analysis-using-pervasive-sensing"/>
   <updated>2013-08-20T00:00:00+09:00</updated>
   <id>http://kaikunze.de/posts/wordometer-and-document-analysis-using-pervasive-sensing</id>
   <content type="html">&lt;p&gt;&lt;img src=&quot;/imgs/wordometer.png&quot; alt=&quot;wordometer&quot; /&gt;
In the last couple of months, I got more and more interested in learning, especially reading.
Loving tech and sports, I got easily hooked on the Quantified Self movement (I own a Zeo Sleeping Coach and several step counters).
Seeing how measuring myself transformed me. I lost around 4 kg and feel healthier/fitter, since
I started tracking. I wonder why we don&amp;rsquo;t have similar tools for our learning behavior.&lt;/p&gt;

&lt;p&gt;So we created a simple Wordometer in our Lab, using the SMI mobile eyetracker and document image retrieval
(LLAH). We simply detect reading (very distinct horizontal or vertical movements) and afterwards count line breaks.
Assuming a fixed number of words per line, voil√† here is your Wordometer. The document image retreival is used
to keep the accuracy at around 5-7 % (comparable to the pedometers measuring your steps each day).&lt;/p&gt;

&lt;p&gt;Of course, there are a couple of limitations:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Mobile Eyetrackers are DAMN expensive. Yet, the main reason being that there is no demand and they are manufactured in relatively low numbers. A glass frame, 2 cameras and 2 infra-red sources that&amp;rsquo;s it (together with a bit of image processing magic).&lt;/li&gt;
&lt;li&gt;Document Image Retrieval means you need to register all documents with a server before reading them. I won&amp;rsquo;t go into details as this limitation is the easiest to get rid of. We are currently working on a method without it. At the beginning it was easier to include (and improve the accuracy rate).&lt;/li&gt;
&lt;li&gt;Not everybody likes to wear glasses. With the recent mixed reception of Google Glass, it seems that wearing glasses is way more a fashion statement than wearing a big &amp;ldquo;smart&amp;rdquo; phone or similar. So this tech might not be for everybody.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Overall, I&amp;rsquo;m still very exited on what a cheap, public avaiable Wordometer will do to the reading habits of people and their &amp;ldquo;knowledge life&amp;rdquo;.
We&amp;rsquo;ll continue working on it ;)&lt;/p&gt;

&lt;p&gt;We are also using eyetracking, EEG and other sensors to get more information about what/how a user is reading.
Interestingly, it seems using the Emotiv EEG we can detect reading versus not reading and even some document types
(manga versus textbook).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Disclaimer:
This work would not be possible without two very talented students: Hitoshi Kawaichi and Kazuyo Yoshimura.
Thanks for the hard work :D&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;For more details, check out the papers:
* &lt;a href=&quot;/papers/kunze2013reading.pdf&quot;&gt;The Wordometer ‚Äì Estimating the Number of Words Read Using Document Image Retrieval and Mobile Eye Tracking&lt;/a&gt;
* &lt;a href=&quot;/papers/kunze2013wordometer.pdf&quot;&gt;Reading Activity Recognition using an off-the-shelf EEG ‚Äî Detecting Reading Activities and Distinguishing Genres of Documents&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;They are both published at &lt;a href=&quot;http://icdar2013.org&quot;&gt;ICDAR 2013&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Kai @ CHI</title>
   <link href="http://kaikunze.de/posts/kai-%40-chi"/>
   <updated>2013-04-29T00:00:00+09:00</updated>
   <id>http://kaikunze.de/posts/kai-@-chi</id>
   <content type="html">&lt;p&gt;So it&amp;rsquo;s my first time at &lt;a href=&quot;http://chi2013.acm.org&quot;&gt;CHI&lt;/a&gt;. Pretty amazing so far &amp;hellip;
Will blog about more later.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://kaikunze.de/posters/chiposter2013.pdf&quot;&gt;&lt;img src=&quot;/posters/tb-chiposter2013.jpg&quot; alt=&quot;Chi Poster&quot; /&gt;&lt;/a&gt;&lt;img src=&quot;/imgs/kai@chi.jpg&quot; alt=&quot;Kai@CHI&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m in the first poster rotation, starting this afternoon:
&amp;ldquo;Towards inferring language expertise using eye tracking&amp;rdquo;
Drop by my poster if you&amp;rsquo;re around (or try to spot me, I&amp;rsquo;m wearing the white &amp;ldquo;Kai@CHI&amp;rdquo; Shirt today :)).&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s the abstract of our work, as well as the &lt;a href=&quot;http://kaikunze.de/papers/kunze2013towards.pdf&quot;&gt;link to the paper&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&amp;ldquo;We present initial work towards recognizing reading activities. This paper describes our efforts detect the English skill level of a user and infer which words are difficult for them to understand. We present an initial study of 5 students and show our findings regarding the skill level assessment. We explain a method to spot difficult words. Eye tracking is a promising technology to examine and assess a user‚Äôs skill level.&amp;rdquo;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Activity Recognition Dagstuhl Report Online</title>
   <link href="http://kaikunze.de/posts/ar-dagstuhl-report-online"/>
   <updated>2013-04-24T00:00:00+09:00</updated>
   <id>http://kaikunze.de/posts/ar-dagstuhl-report-online</id>
   <content type="html">&lt;p&gt;If you wonder how we spent German tax money, the summary of the
Activity Recognition Dagstuhl seminar is now online.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://drops.dagstuhl.de/opus/volltexte/2013/3987/&quot;&gt;Human Activity Recognition in Smart Environments (Dagstuhl Seminar 12492)&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Here&amp;rsquo;s the abstract:&lt;/p&gt;

&lt;p&gt;This report documents the program and the outcomes of Dagstuhl Seminar 12492 &amp;ldquo;Human Activity Recognition in Smart Environments&amp;rdquo;. We established the basis for a scientific community surrounding &amp;ldquo;activity recognition&amp;rdquo; by involving researchers from a broad range of related research fields. 30 academic and industry researchers from US, Europe and Asia participated from diverse fields including pervasive computing, over network analysis and computer vision to human computer interaction. The major results of this Seminar are the creation of a activity recognition repository to share information, code, publications and the start of an activity recognition book aimed to serve as a scientific introduction to the field. In the following, we go into more detail about the structure of the seminar, discuss the major outcomes and give an overview about discussions and talks given during the seminar.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Some of my favorites from the 29c3 recordings</title>
   <link href="http://kaikunze.de/posts/some-of-my-favorites-from-the-29c3-recordings"/>
   <updated>2013-02-06T00:00:00+09:00</updated>
   <id>http://kaikunze.de/posts/some-of-my-favorites-from-the-29c3-recordings</id>
   <content type="html">&lt;p&gt;Over the last weeks, I finally got around to watch some of the &lt;a href=&quot;https://events.ccc.de/congress/2012/wiki/Main_Page&quot;&gt;29c3&lt;/a&gt; recordings. Here are some of my favorites. I will update the list accordingly.&lt;/p&gt;

&lt;p&gt;I link to the official recording available from the CCC domain. The talks however are also on youtube. Just search for the talk title.&lt;/p&gt;

&lt;p&gt;In General, I found most talks focused on security, sadly not really my main interest. I missed some research and culture talks that were present the last years. Examples from the last years:&lt;a href=&quot;http://www.youtube.com/watch?v=mS4k0hFhPeQ&quot;&gt;Data Mining for Hackers&lt;/a&gt; awesome talk!! or &lt;a href=&quot;http://www.youtube.com/watch?v=Lxf60pRz1tM&quot;&gt;one&lt;/a&gt; of Bicyclemark episodes. Bicylcemark we miss you :)&lt;/p&gt;

&lt;h2&gt;English&lt;/h2&gt;

&lt;p&gt;Out of the hacking talks, for me by far the most entertaining was &lt;a href=&quot;http://media.ccc.de/browse/congress/2012/29c3-5400-en-hacking_cisco_phones_h264.html&quot;&gt;Hacking Cisco Phones&lt;/a&gt;. Scary and so cool. Ang Cui and Michael Costello are also quite good presenters. The hand-drawn slides give the visuals also a nice touch. I won&amp;rsquo;t spoil the contents. just watch it.&lt;/p&gt;

&lt;p&gt;So far my most favorite talk is &lt;a href=&quot;http://media.ccc.de/browse/congress/2012/29c3-5138-en-romantichackers_h264.html&quot;&gt;Romantic Hackers&lt;/a&gt; by Anne Marggraf-Turley and Prof. Richard Marggraf-Turley. About surveillance andy hackers in the Romantic period. I was not aware that the privacy problems and the ideas about pervasive surveillance had been discussed and encountered so early in human history. Very Insightful and fun.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;http://media.ccc.de/browse/congress/2012/29c3-5088-en-many_tamagotchis_were_harmed_in_the_making_of_this_presentation_h264.html&quot;&gt;Tamagochi Talk&lt;/a&gt; was fun. Although the speaker seemed to be a bit nervous (listening to her voice), she gave some great insides how Tamagochis work and how to hack them.&lt;/p&gt;

&lt;p&gt;The keynote from Jacob Applebaum, &lt;a href=&quot;http://media.ccc.de/browse/congress/2012/29c3-5385-en-not_my_department_h264.html&quot;&gt;Not my department&lt;/a&gt; is a call to action for the tech community discussing about the responsibilities we have regarding our research and how it might be used. Although Applebaum is a great public speaker and the topic is of utmost importance, for some people new to the discussion it might seem a bit out of context and difficult to understand.&lt;/p&gt;

&lt;h2&gt;German&lt;/h2&gt;

&lt;p&gt;If you can speak German or want to practice it, check them out &amp;hellip;
Of course, the usual subjects &lt;a href=&quot;http://media.ccc.de/browse/congress/2012/29c3-5198-en-de-fnord_jahresrueckblick2012_h264.html&quot;&gt;Fnord News Show&lt;/a&gt; and &lt;a href=&quot;http://media.ccc.de/browse/congress/2012/29c3-5244-de-en-security_nightmares2012_h264.html&quot;&gt;Security Nightmares&lt;/a&gt; are always great candidates to watch.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m also always looking forward to the yearly Martin Haase Talk. Unfortunately, the official release is not online yet. Interesting especially for language geeks.&lt;/p&gt;

&lt;p&gt;The talk &lt;a href=&quot;http://media.ccc.de/browse/congress/2012/29c3-5121-de-en-sind_faire_computer_moeglich_h264.html&quot;&gt;Are fair computers possible?&lt;/a&gt; explores what needs to change in manufacturing standards etc. to produce computers without child labor and fair employment conditions for all workers involved.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>ACM Multimedia 2012 Main Conference Notes</title>
   <link href="http://kaikunze.de/posts/acm-multimedia-2012-day-2-notes"/>
   <updated>2012-10-31T00:00:00+09:00</updated>
   <id>http://kaikunze.de/posts/acm-multimedia-2012-day-2-notes</id>
   <content type="html">&lt;p&gt;This is a scratchpad &amp;hellip; will fill the rest when I have time.&lt;/p&gt;

&lt;h2&gt;Papers&lt;/h2&gt;

&lt;p&gt;I really enjoyed the work from Heng Liu, Tao Mei et. al.
&amp;ldquo;Finding Perfect Rendezvous On the Go: Accurate Mobile Visual Localization and Its Applications to Routing&amp;rdquo;.
They combine existing research in a very interesting mixture. They use a visual localization method
based on &lt;a href=&quot;http://phototour.cs.washington.edu/bundler/&quot;&gt;bundler&lt;/a&gt; to detect where in the city
a mobile phone user is. The application scenario I liked best was their collaborative localization
for rendezvous :)&lt;/p&gt;

&lt;p&gt;The best paper award went to Zhi Wang, Lifeng Sun, Xiangwen Chen, Wenwu Zhu, Jiangchuan Liu, Minghua Chen and Shiqiang Yang for &amp;ldquo;Propagation-Based Social-Aware Replication for Social Video Contents&amp;rdquo;. They use the contacts mined over
social networking to replicate content for better streaming and content distribution.
The presentation was great, the research solid, still it&amp;rsquo;s not a topic I&amp;rsquo;m very interested in. However, for content providers it seems very useful.&lt;/p&gt;

&lt;p&gt;Shih-Yao Lin et. al. presented a system to recognize the users motion using the kinect and imitate them via a
marionette in &amp;ldquo;Action Recognition for Human-Marionette Interaction&amp;rdquo;. I hoped to get more information
about the interactions between users and marionettes, still very stylish presentation and artsy topic.&lt;/p&gt;

&lt;p&gt;Hamdi Dibeklioglu et. al. showed how to infer the age of a person when they are simling in &amp;ldquo;A Smile Can Reveal Your Age: Enabling Facial Dynamics in Age Estimation&amp;rdquo;. I find fascinating to hear about small cues that can
tell a lot about a person or a situation.&lt;/p&gt;

&lt;p&gt;Fascinating work by Victoria Yanulevskaya et. al. (&amp;ldquo;In the Eye of the Beholder: Employing Statistical Analysis and Eye Tracking for Analyzing Abstract Paintings&amp;rdquo;). They link the emotional impact of a painting to the eye movements
of the observer. Very interesting and in line with my current focus. I wonder if also expertise etc. can be
recognized using sensors.&lt;/p&gt;

&lt;p&gt;Another very art focused paper I enjoyed was &amp;ldquo;Dinner of LucieÃÅrnaga-An interactive Play with iPhone App in Theater&amp;rdquo;
by Yu-Chuan Tseng. Theater visitors can interact with the play using their smart phone (getting also feedback on the device &amp;hellip;.).&lt;/p&gt;

&lt;h2&gt;Posters, Demos, Competitions&lt;/h2&gt;

&lt;p&gt;The winner of the Multimedia Grand Challenge was very well deserved. &amp;ldquo;Analysis of Dance Movements using Gaussian Processes&amp;rdquo; by Antoine Liutkus et. al. decomposed dance moves using Gaussian processes in movements with
slow periodicity, high periodicity and moves that happened just once. Fascinating and applicable to so many fields &amp;hellip; :)&lt;/p&gt;

&lt;p&gt;A very neat demo was presented by Wei Zhang et. al.: &amp;ldquo;FashionAsk: Pushing Community Answers to Your Fingertips&amp;rdquo;.&lt;/p&gt;

&lt;h2&gt;Other Notes&lt;/h2&gt;

&lt;p&gt;As expected from any conference in Japan :), the organisation was flawless.
In case any if the organisers is reading this. Thanks again. Nara is a perfect place for a venue like this
(deer, world heritage sites, good food &amp;hellip;).&lt;/p&gt;

&lt;p&gt;More curiously, although there was a lot of talk about social media and some lively discussions
on twitter, I seemed to be the only participant on &lt;a href=&quot;https://alpha.app.net/&quot;&gt;ADN&lt;/a&gt; at least posting with hashtag.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>ACM Multimedia 2012 Tutorials and Workshops</title>
   <link href="http://kaikunze.de/posts/acm-multimedia-2012-day-1-notes"/>
   <updated>2012-10-29T00:00:00+09:00</updated>
   <id>http://kaikunze.de/posts/acm-multimedia-2012-day-1-notes</id>
   <content type="html">&lt;p&gt;I attended the Tutorials &amp;ldquo;Interacting with Image Collections ‚Äì Visualisation and Browsing of Image Repositories&amp;rdquo;
and &amp;ldquo;Continuous Analysis of Emotions for Multimedia Applications&amp;rdquo; on the first day.&lt;/p&gt;

&lt;p&gt;The last day I went to &amp;ldquo;Workshop on Audio and Multimedia Methods for Large Scale Video Analysis&amp;rdquo; and
to the &amp;ldquo;Workshop on Interactive Multimedia on Mobile and Portable Devices&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;This is meant as a scratchpad &amp;hellip; I&amp;rsquo;ll add more later if I have time.&lt;/p&gt;

&lt;h3&gt;Interacting with Image Collections ‚Äì Visualisation and Browsing of Image Repositories&lt;/h3&gt;

&lt;p&gt;Schaefer gave a overview about how to browse large scale image repositories.
Interesting, yet of not really related to my research interests.
He showed 3 approaches for retrieval: mapping-based, clustering-based and graph-based.
I would have loved if he could have gone a bit more in detail in the mobile section at the end.&lt;/p&gt;

&lt;h3&gt;Continuous Analysis of Emotions for Multimedia Applications&lt;/h3&gt;

&lt;p&gt;Hatice Gunes and Bjoern Schuller introduced a state of the art in emotion analysis.
Their problems seem very similar to what we have to cope with in activity recognition,
especially in terms of segmentation and continuous recognition problems.
Their inference pipeline is comparable to ours in context recognition.&lt;/p&gt;

&lt;p&gt;Where Affective Computing seems to have an edge is in the standardized data sets.
There are already quite a lot (mainly focusing on video and audio).
I guess it&amp;rsquo;s also easier compared to the very multi-modal datasets we deal with in activity recogntion.&lt;/p&gt;

&lt;p&gt;Hatice Gunes showed two videos of two girls, one is faking a laugh the other one is authentic.
Interestingly enough, the whole audience was wrong in picking the authentic laugh.
The fake laughing girl was overdoing it and laughed constantly.
However, authentic laughter has a time component (coming in waves: increasing, decreasing, increasing again etc.).&lt;/p&gt;

&lt;p&gt;The tools section contained the obvious candidates (opencv, kinect, weka &amp;hellip;).
Sadly they did not mention the new set of tools I love to use. Check out &lt;a href=&quot;http://pandas.pydata.org&quot;&gt;Pandas&lt;/a&gt;
and &lt;a href=&quot;http://ipython.org&quot;&gt;iPython&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Good overview about the state of the art.
I would have loved to get more information about the subjective nature of emotion. For me it&amp;rsquo;s not
as obvious as activity (already there is a lot of room of ambiguity).
Also, depending on personal experience and cultural background, the emotional response to specific stimuli can be diverse.&lt;/p&gt;

&lt;h4&gt;interesting links&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;http://semaine-db.eu&quot;&gt;Semaine Corpus&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.multimediaeval.org/&quot;&gt;Media Eval&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.informatik.uni-augsburg.de/de/lehrstuehle/hcm/projects/tools/emovoice/&quot;&gt;EmoVoice Audio Emotion classifier&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.affectiva.com/q-sensor/&quot;&gt;qsensor&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://cinimodstudio.com/project/london-eye-mood-conductor/&quot;&gt;London eye mood&lt;/a&gt;&lt;/p&gt;

&lt;h3&gt;Workshop on Audio and Multimedia Methods for Large Scale Video Analysis&lt;/h3&gt;

&lt;h3&gt;Workshop on Interactive Multimedia on Mobile and Portable Devices&lt;/h3&gt;
</content>
 </entry>
 
 <entry>
   <title>Laughing Faces App in the AppStore</title>
   <link href="http://kaikunze.de/posts/laughing-faces-app-in-the-appstore"/>
   <updated>2012-08-30T00:00:00+09:00</updated>
   <id>http://kaikunze.de/posts/laughing-faces-app-in-the-appstore</id>
   <content type="html">&lt;p&gt;Over the last couple of weeks, I was getting settled in my new job.
As I&amp;rsquo;m working with &lt;a href=&quot;http://imlab.jp&quot;&gt;computer vision researchers&lt;/a&gt; now,
I started playing with the camera api for the iPhone.&lt;/p&gt;

&lt;p&gt;Again, I&amp;rsquo;m very surprised by the accessibility and quality of Apples
apis and their sample code.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/imgs/laughing.png&quot; alt=&quot;Laughing Face&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As a start, this little app is a &amp;ldquo;privacy enhanced&amp;rdquo; camera
app for entertainment purposes.
It uses face detection and draws a little
laughing face on top of each recognized head in real time.
I hesitated putting it in the store, yet was asked by some friends to
do so (had to exchange the laughing face due to copyright constraints).&lt;/p&gt;

&lt;p&gt;Grab it while it&amp;rsquo;s hot &amp;hellip; it&amp;rsquo;s quite popular in Japan (understandable
given the background, see below), China and Saudi Arabia (of all places, &amp;hellip; if
somebody can tell me why, please send me a mail):
&lt;a href=&quot;http://itunes.apple.com/us/app/laughing-faces/id551656355?mt=8&quot;&gt;Laughing Faces AppStore Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;By the way, I had over 250 downloads the first day :)
Oh if you wonder, the inspiration came from
&lt;a href=&quot;http://en.wikipedia.org/wiki/Laughing_Man_(Ghost_in_the_Shell&quot;&gt;Ghost in the Shell Standalone Complex&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;If people bug me enough, I will make the png exchangable.
Cannot tell you too much, yet expect an update when iOS6 hits.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>AAAI activity context workshop notes</title>
   <link href="http://kaikunze.de/posts/aaai-activity-context-workshop-notes"/>
   <updated>2012-07-26T00:00:00+09:00</updated>
   <id>http://kaikunze.de/posts/aaai-activity-context-workshop-notes</id>
   <content type="html">&lt;p&gt;I enjoyed the AAAI context activity workshop a lot.&lt;/p&gt;

&lt;p&gt;The keynote &lt;a href=&quot;http://www.aaai.org/ocs/index.php/WS/AAAIW12/paper/view/5292/5556&quot;&gt;How to make Face
Recognition work (pdf)&lt;/a&gt;
by &lt;a href=&quot;http://research.microsoft.com/en-us/um/people/akapoor/&quot;&gt;Ashis Kapoor&lt;/a&gt;
showed how to increase face recognition introducing very simple &amp;ldquo;context&amp;rdquo; constrains
(two people in the same image cannot be the same person etc.).
Very interesting work, I wonder how much better you can get introducing
some more dynamic context recognition to the face recognition task.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.cs.ubc.ca/~murphy/&quot;&gt;Gail Murphy&lt;/a&gt; gave the other keynote
&lt;a href=&quot;http://www.aaai.org/ocs/index.php/WS/AAAIW12/paper/view/5302/5557&quot;&gt;Task Context for Knowledge Workers (pdf)&lt;/a&gt;.
She introduces context modelling for tasks in GTD scenarios.
Also quite interesting, as completely complimentary to my work (no mobile
clients, sensors etc.).&lt;/p&gt;

&lt;p&gt;A lot of people were aware of our efforts during the
&lt;a href=&quot;http://www.opportunity-project.eu/&quot;&gt;Opportunity Project&lt;/a&gt; and the standard datasets we want to put out.&lt;/p&gt;

&lt;p&gt;Rim Helaoui presented work about using &lt;a href=&quot;http://www.aaai.org/ocs/index.php/WS/AAAIW12/paper/view/5269/5552&quot;&gt;Probabilistic Description Logics (pdf)&lt;/a&gt;
for activity recognition, an interesting approach trying to combine
  data driven and rule-based activity inference. They used
  the opportunity dataset ;)&lt;/p&gt;

&lt;p&gt;Bostjan Kaluza shared the call for more standardized datasets in context recognition
in his talk about &lt;a href=&quot;http://www.aaai.org/ocs/index.php/WS/AAAIW12/paper/view/5305/5555&quot;&gt;The Activity Recognition Repository (pdf)&lt;/a&gt;.
A very important endeavor, I already also discussed several times.
I think a broad effort in the field is necessary.&lt;/p&gt;

&lt;p&gt;All the final papers are up on the &lt;a href=&quot;http://activitycontext.org/final-papers/&quot;&gt;workshop website&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Towards Dynamically Configurable Context Recognition Systems</title>
   <link href="http://kaikunze.de/posts/draft-version-of-aaai-workshop-paper-online"/>
   <updated>2012-07-09T00:00:00+09:00</updated>
   <id>http://kaikunze.de/posts/draft-version-of-aaai-workshop-paper-online</id>
   <content type="html">&lt;p&gt;Here&amp;rsquo;s a &lt;a href=&quot;http://kaikunze.de/papers/2012Kunze.pdf&quot;&gt;draft version of my publication&lt;/a&gt; for the &lt;a href=&quot;http://activitycontext.org/&quot;&gt;Activity Context Workshop&lt;/a&gt;
in Toronto. Bellow the abstract.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s the link to the &lt;a href=&quot;https://github.com/kkai/snsrlog&quot;&gt;source code for snsrlog for iPhone&lt;/a&gt; (which I mentioned during my talk).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Abstract&lt;/p&gt;

&lt;p&gt;General representation, abstraction and exchange definitions are crucial for dynamically configurable context recognition.
However, to evaluate potential definitions, suitable standard datasets are needed.
This paper presents our effort to create and maintain large scale, multimodal standard datasets
for context recognition research. We ourselves used these datasets in previous research to deal with placement effects
and presented low-level sensor abstractions in motion based on-body sensing.
Researchers, conducting novel data collections, can rely on the toolchain and the the low-level sensor
abstractions summarized in this paper. Additionally, they can draw from our experiences developing and
conducting context recognition experiments.
Our toolchain is already a valuable rapid prototyping tool. Still, we plan to extend it to crowd-based
sensing, enabling the general public to gather context data, learn more about their lives and contribute
to context recognition research.
Applying higher level context reasoning on the gathered context data is a obvious extension to our work.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Some of my publications are online</title>
   <link href="http://kaikunze.de/posts/some-of-my-publications-are-online"/>
   <updated>2012-07-08T00:00:00+09:00</updated>
   <id>http://kaikunze.de/posts/some-of-my-publications-are-online</id>
   <content type="html">&lt;p&gt;I&amp;rsquo;m slowly uploading a couple of references and the pdf draft versions of them.
Please find some of my &lt;a href=&quot;http://kaikunze.de/publications.html&quot;&gt;publications&lt;/a&gt; in the corresponding
section of this website.&lt;/p&gt;

&lt;p&gt;Stay tuned for the bibtex description and some more papers.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Compensating for On-body Placement Effects in Activity Recognition</title>
   <link href="http://kaikunze.de/posts/phd-thesis-sources-on-github"/>
   <updated>2012-07-07T00:00:00+09:00</updated>
   <id>http://kaikunze.de/posts/phd-thesis-sources-on-github</id>
   <content type="html">&lt;p&gt;Finished my phD. last year in Passau. The
thesis is already published over &lt;a href=&quot;http://www.opus-bayern.de/uni-passau/volltexte/2012/2611/&quot;&gt;Opus Bayern&lt;/a&gt;.
The pdf is open access, so feel free to read it (careful 19 MB pdf):
&lt;a href=&quot;http://www.opus-bayern.de/uni-passau/volltexte/2012/2611/pdf/kunze_kai.pdf&quot;&gt;Compensating for On-Body Placement Effects in Activity Recognition as pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;However, the sources were not available.
Finally, I got around to push the &lt;a href=&quot;http://github.com/kkai/phdthesis&quot;&gt;latex sources
of my dissertation&lt;/a&gt; up to github.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Please feel free to use it as a thesis template,
attribution would be apprecitated ;)&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Please share if you make improvements, there
are a lot of hacks and quick fixes in the sources.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;I&amp;rsquo;ll try to share most of the algorithms discussed in my dissertation.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Here&amp;rsquo;s a quick summary about the content together with the slides of my defense:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;This thesis investigates, how placement variations of electronic
devices influence the possibility of using sensors integrated in
those devices for context recognition. The vast majority of context
recognition research assumes well defined, fixed sen- sor locations.
Although this might be acceptable for some application domains
(e.g. in an industrial setting), users, in general, will have a
hard time coping with these limitations. If one needs to remember
to carry dedicated sensors and to adjust their orientation from
time to time, the activity recognition system is more distracting
than helpful. How can we deal with device location and orientation
changes to make context
sensing mainstream? This thesis presents a systematic
evaluation of device placement effects in context recognition.&lt;/p&gt;&lt;/blockquote&gt;

&lt;script class=&quot;speakerdeck-embed&quot; data-id=&quot;eece7090eb64013086517aec58278375&quot; data-ratio=&quot;1.33333333333333&quot; src=&quot;//speakerdeck.com/assets/embed.js&quot;&gt; &lt;/script&gt;



</content>
 </entry>
 
 <entry>
   <title>Using device motion in html/javascript</title>
   <link href="http://kaikunze.de/posts/using-device-motion-from-a-mobile-device-in-htmljavascript"/>
   <updated>2012-06-18T00:00:00+09:00</updated>
   <id>http://kaikunze.de/posts/using-device-motion-from-a-mobile-device-in-htmljavascript</id>
   <content type="html">&lt;p&gt;A while ago, I built a simple demonstration on how to stream accelerometer
data from a mobile device over websockets to a server just using
html and javascript. It consists of a nodejs web server and a processing.org
visualization. As soon as a mobile browser connects to the server a new red
cube is shown on the screen (placed between randomly generated cubes).
The transparent area around the cube changes depending on how strong
one shakes the phone.&lt;/p&gt;

&lt;iframe src=&quot;http://player.vimeo.com/video/45626605&quot; 
width=&quot;500&quot; height=&quot;281&quot; frameborder=&quot;0&quot;
&gt;
&lt;/iframe&gt;


&lt;p&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://vimeo.com/45626605&quot;&gt;Visualization based on mobile phone data&lt;/a&gt; from &lt;a href=&quot;http://vimeo.com/user8093378&quot;&gt;Kai Kunze&lt;/a&gt; on &lt;a href=&quot;http://vimeo.com&quot;&gt;Vimeo&lt;/a&gt;.&lt;/p&gt;


&lt;p&gt;You can get the code from my &lt;a href=&quot;https://github.com/kkai/devicemotion-demo&quot;&gt;github page&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s based on these tutorials and sample code:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://martinsikora.com/nodejs-and-websocket-simple-chat-tutorial&quot;&gt;a simple chat server node.js tutorial&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.paulrhayes.com/2009-07/animated-css3-cube-interface-using-3d-transforms/&quot;&gt;3d css cube&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://openprocessing.org/sketch/19216&quot;&gt;3d cube world&lt;/a&gt;&lt;/p&gt;
</content>
 </entry>
 

</feed>
