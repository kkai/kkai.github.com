<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
 <title>Kai Kunze</title>
 <link href="http://kkai.github.com/site/atom.xml" rel="self"/>
 <link href="http://kkai.github.com/site"/>
 <updated>2012-08-07T10:43:19+09:00</updated>
 <id>http://kkai.github.com/site</id>
 <author>
   <name>Kai Kunze</name>
   <email>kai.kunze@gmail.com</email>
 </author>

 
 <entry>
   <title>AAAI activity context workshop notes</title>
   <link href="http://kkai.github.com/site/2012/07/26/aaai-activity-context-workshop-notes"/>
   <updated>2012-07-26T00:00:00+09:00</updated>
   <id>http://kkai.github.com/site/2012/07/26/aaai-activity-context-workshop-notes</id>
   <content type="html"></content>
 </entry>
 
 <entry>
   <title>Towards Dynamically Configurable Context Recognition Systems</title>
   <link href="http://kkai.github.com/site/2012/07/09/draft-version-of-aaai-workshop-paper-online"/>
   <updated>2012-07-09T00:00:00+09:00</updated>
   <id>http://kkai.github.com/site/2012/07/09/draft-version-of-aaai-workshop-paper-online</id>
   <content type="html">&lt;p&gt;Here&amp;#8217;s a &lt;a href='http://kaikunze.de/papers/2012Kunze.pdf'&gt;draft version of my publication&lt;/a&gt; for the &lt;a href='http://activitycontext.org/'&gt;Activity Context Workshop&lt;/a&gt; in Toronto. Bellow the abstract.&lt;/p&gt;

&lt;p&gt;Also download &lt;a href='http://kaikunze.de/slides/2012aaai-slides.pdf'&gt;the slides&lt;/a&gt; of my talk.&lt;/p&gt;

&lt;p&gt;Here&amp;#8217;s the link to the &lt;a href='https://github.com/kkai/snsrlog'&gt;source code for snsrlog for iPhone&lt;/a&gt; (which I mentioned during my talk).&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Abstract&lt;/p&gt;

&lt;p&gt;General representation, abstraction and exchange definitions are crucial for dynamically configurable context recognition. However, to evaluate potential definitions, suitable standard datasets are needed. This paper presents our effort to create and maintain large scale, multimodal standard datasets for context recognition research. We ourselves used these datasets in previous research to deal with placement effects and presented low-level sensor abstractions in motion based on-body sensing. Researchers, conducting novel data collections, can rely on the toolchain and the the low-level sensor abstractions summarized in this paper. Additionally, they can draw from our experiences developing and conducting context recognition experiments. Our toolchain is already a valuable rapid prototyping tool. Still, we plan to extend it to crowd-based sensing, enabling the general public to gather context data, learn more about their lives and contribute to context recognition research. Applying higher level context reasoning on the gathered context data is a obvious extension to our work.&lt;/p&gt;</content>
 </entry>
 
 <entry>
   <title>Some of my publications are online</title>
   <link href="http://kkai.github.com/site/2012/07/08/some-of-my-publications-are-online"/>
   <updated>2012-07-08T00:00:00+09:00</updated>
   <id>http://kkai.github.com/site/2012/07/08/some-of-my-publications-are-online</id>
   <content type="html">&lt;p&gt;I&amp;#8217;m slowly uploading a couple of references and the pdf draft versions of them. Please find some of my &lt;a href='http://kaikunze.de/publications.html'&gt;publications&lt;/a&gt; in the corresponding section of this website.&lt;/p&gt;

&lt;p&gt;Stay tuned for the bibtex description and some more papers.&lt;/p&gt;</content>
 </entry>
 
 <entry>
   <title>Compensating for On-body Placement Effects in Activity Recognition</title>
   <link href="http://kkai.github.com/site/2012/07/07/phd-thesis-sources-on-github"/>
   <updated>2012-07-07T00:00:00+09:00</updated>
   <id>http://kkai.github.com/site/2012/07/07/phd-thesis-sources-on-github</id>
   <content type="html">&lt;p&gt;Finished my phD. last year in Passau. The thesis is already published over &lt;a href='http://www.opus-bayern.de/uni-passau/volltexte/2012/2611/'&gt;Opus Bayern&lt;/a&gt;. The pdf is open access, so feel free to read it (careful 19 MB pdf): &lt;a href='http://www.opus-bayern.de/uni-passau/volltexte/2012/2611/pdf/kunze_kai.pdf'&gt;Compensating for On-Body Placement Effects in Activity Recognition as pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;However, the sources were not available. Finally, I got around to push the &lt;a href='http://github.com/kkai/phdthesis'&gt;latex sources of my dissertation&lt;/a&gt; up to github.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Please feel free to use it as a thesis template, attribution would be apprecitated ;)&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Please share if you make improvements, there are a lot of hacks and quick fixes in the sources.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;I&amp;#8217;ll try to share most of the algorithms discussed in my dissertation.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Here&amp;#8217;s a quick summary about the content:&lt;/p&gt;

&lt;p&gt;This thesis investigates, how placement variations of electronic devices influence the possibility of using sensors integrated in those devices for context recognition. The vast majority of context recognition research assumes well defined, fixed sen- sor locations. Although this might be acceptable for some application domains (e.g. in an industrial setting), users, in general, will have a hard time coping with these limitations. If one needs to remember to carry dedicated sensors and to adjust their orientation from time to time, the activity recognition system is more distracting than helpful. How can we deal with device location and orientation changes to make context sensing mainstream? This thesis presents a systematic evaluation of device placement effects in context recognition.&lt;/p&gt;</content>
 </entry>
 
 <entry>
   <title>using device motion in html/javascript</title>
   <link href="http://kkai.github.com/site/2012/06/18/using-device-motion-from-a-mobile-device-in-htmljavascript"/>
   <updated>2012-06-18T00:00:00+09:00</updated>
   <id>http://kkai.github.com/site/2012/06/18/using-device-motion-from-a-mobile-device-in-htmljavascript</id>
   <content type="html">&lt;p&gt;A while ago, I built a simple demonstration on how to stream accelerometer data from a mobile device over websockets to a server just using html and javascript. It consists of a nodejs web server and a processing.org visualization. As soon as a mobile browser connects to the server a new red cube is shown on the screen (placed between randomly generated cubes). The transparent area around the cube changes depending on how strong one shakes the phone.&lt;/p&gt;
&lt;iframe frameborder='0' height='281' src='http://player.vimeo.com/video/45626605' width='500'&gt;
&lt;/iframe&gt;&lt;p&gt;&lt;a href='http://vimeo.com/45626605'&gt;Visualization based on mobile phone data&lt;/a&gt; from &lt;a href='http://vimeo.com/user8093378'&gt;Kai Kunze&lt;/a&gt; on &lt;a href='http://vimeo.com'&gt;Vimeo&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can get the code from my &lt;a href='https://github.com/kkai/devicemotion-demo'&gt;github page&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It&amp;#8217;s based on these tutorials and sample code:&lt;/p&gt;

&lt;p&gt;&lt;a href='http://martinsikora.com/nodejs-and-websocket-simple-chat-tutorial'&gt;a simple chat server node.js tutorial&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href='http://www.paulrhayes.com/2009-07/animated-css3-cube-interface-using-3d-transforms/'&gt;3d css cube&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href='http://openprocessing.org/sketch/19216'&gt;3d cube world&lt;/a&gt;&lt;/p&gt;</content>
 </entry>
 
 
</feed>