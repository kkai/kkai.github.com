<!DOCTYPE html>
<html>
<head>
  <title>Posts</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">
    
    <link href="/css/bootstrap.min.css" rel="stylesheet">
    
    <link href="/css/blog.css" rel="stylesheet">
    <link rel="stylesheet" href="/css/font-awesome.min.css">

  </head>

      <div class="blog-masthead">
      <div class="container">
        <nav class="nav">
          
          <a class="nav-link" href=/> Kai </a>
          
          <a class="nav-link active" href=/post/> blog </a>
          
          <a class="nav-link" href=/research/> research </a>
          
          <a class="nav-link" href=/contact/> contact </a>
          
        </nav>
      </div>
    </div>
    <body>

	<main>
		<div class="container">
		<div class="col-sm-9">
		
		
		<div class="blog-post">
			<article>
  <h3 class="post-title"><a href="/2013/04/24/ar-dagstuhl-report-online/">Activity Recognition Dagstuhl Report Online</a></h3>
   <p class="blog-post-meta">Wed, Apr 24, 2013</p>
    If you wonder how we spent German tax money, the summary of the Activity Recognition Dagstuhl seminar is now online.
Human Activity Recognition in Smart Environments (Dagstuhl Seminar 12492)
Here&rsquo;s the abstract:
This report documents the program and the outcomes of Dagstuhl Seminar 12492 &ldquo;Human Activity Recognition in Smart Environments&rdquo;. We established the basis for a scientific community surrounding &ldquo;activity recognition&rdquo; by involving researchers from a broad range of related research fields. <a href="http://kaikunze.de/2013/04/24/ar-dagstuhl-report-online/"> Read more... </a>
  </p>
</article>

		</div>
		
		<div class="blog-post">
			<article>
  <h3 class="post-title"><a href="/2013/02/06/some-of-my-favorites-from-the-29c3-recordings/">Some of my favorites from the 29c3 recordings</a></h3>
   <p class="blog-post-meta">Wed, Feb 6, 2013</p>
    Over the last weeks, I finally got around to watch some of the 29c3 recordings. Here are some of my favorites. I will update the list accordingly.
I link to the official recording available from the CCC domain. The talks however are also on youtube. Just search for the talk title.
In General, I found most talks focused on security, sadly not really my main interest. I missed some research and culture talks that were present the last years. <a href="http://kaikunze.de/2013/02/06/some-of-my-favorites-from-the-29c3-recordings/"> Read more... </a>
  </p>
</article>

		</div>
		
		<div class="blog-post">
			<article>
  <h3 class="post-title"><a href="/2012/10/31/acm-multimedia-2012-day-2-notes/">ACM Multimedia 2012 Main Conference Notes</a></h3>
   <p class="blog-post-meta">Wed, Oct 31, 2012</p>
    This is a scratchpad &hellip; will fill the rest when I have time.
##Papers I really enjoyed the work from Heng Liu, Tao Mei et. al. &ldquo;Finding Perfect Rendezvous On the Go: Accurate Mobile Visual Localization and Its Applications to Routing&rdquo;. They combine existing research in a very interesting mixture. They use a visual localization method based on bundler to detect where in the city a mobile phone user is. The application scenario I liked best was their collaborative localization for rendezvous :) <a href="http://kaikunze.de/2012/10/31/acm-multimedia-2012-day-2-notes/"> Read more... </a>
  </p>
</article>

		</div>
		
		<div class="blog-post">
			<article>
  <h3 class="post-title"><a href="/2012/10/29/acm-multimedia-2012-day-1-notes/">ACM Multimedia 2012 Tutorials and Workshops</a></h3>
   <p class="blog-post-meta">Mon, Oct 29, 2012</p>
    I attended the Tutorials &ldquo;Interacting with Image Collections â€“ Visualisation and Browsing of Image Repositories&rdquo; and &ldquo;Continuous Analysis of Emotions for Multimedia Applications&rdquo; on the first day.
The last day I went to &ldquo;Workshop on Audio and Multimedia Methods for Large Scale Video Analysis&rdquo; and to the &ldquo;Workshop on Interactive Multimedia on Mobile and Portable Devices&rdquo;.
This is meant as a scratchpad &hellip; I&rsquo;ll add more later if I have time. <a href="http://kaikunze.de/2012/10/29/acm-multimedia-2012-day-1-notes/"> Read more... </a>
  </p>
</article>

		</div>
		
		<div class="blog-post">
			<article>
  <h3 class="post-title"><a href="/2012/08/30/laughing-faces-app-in-the-appstore/">Laughing Faces App in the AppStore</a></h3>
   <p class="blog-post-meta">Thu, Aug 30, 2012</p>
    Over the last couple of weeks, I was getting settled in my new job. As I&rsquo;m working with computer vision researchers now, I started playing with the camera api for the iPhone.
Again, I&rsquo;m very surprised by the accessibility and quality of Apples apis and their sample code.
As a start, this little app is a &ldquo;privacy enhanced&rdquo; camera app for entertainment purposes. It uses face detection and draws a little laughing face on top of each recognized head in real time. <a href="http://kaikunze.de/2012/08/30/laughing-faces-app-in-the-appstore/"> Read more... </a>
  </p>
</article>

		</div>
		
		<div class="blog-post">
			<article>
  <h3 class="post-title"><a href="/2012/07/26/aaai-activity-context-workshop-notes/">AAAI activity context workshop notes</a></h3>
   <p class="blog-post-meta">Thu, Jul 26, 2012</p>
    I enjoyed the AAAI context activity workshop a lot.
The keynote How to make Face Recognition work (pdf) by Ashis Kapoor showed how to increase face recognition introducing very simple &ldquo;context&rdquo; constrains (two people in the same image cannot be the same person etc.). Very interesting work, I wonder how much better you can get introducing some more dynamic context recognition to the face recognition task.
Gail Murphy gave the other keynote Task Context for Knowledge Workers (pdf). <a href="http://kaikunze.de/2012/07/26/aaai-activity-context-workshop-notes/"> Read more... </a>
  </p>
</article>

		</div>
		
		<div class="blog-post">
			<article>
  <h3 class="post-title"><a href="/2012/07/09/draft-version-of-aaai-workshop-paper-online/">Towards Dynamically Configurable Context Recognition Systems</a></h3>
   <p class="blog-post-meta">Mon, Jul 9, 2012</p>
    Here&rsquo;s a draft version of my publication for the Activity Context Workshop in Toronto. Bellow the abstract.
Here&rsquo;s the link to the source code for snsrlog for iPhone (which I mentioned during my talk).
Abstract
General representation, abstraction and exchange definitions are crucial for dynamically configurable context recognition. However, to evaluate potential definitions, suitable standard datasets are needed. This paper presents our effort to create and maintain large scale, multimodal standard datasets for context recognition research. <a href="http://kaikunze.de/2012/07/09/draft-version-of-aaai-workshop-paper-online/"> Read more... </a>
  </p>
</article>

		</div>
		
		<div class="blog-post">
			<article>
  <h3 class="post-title"><a href="/2012/07/08/some-of-my-publications-are-online/">Some of my publications are online</a></h3>
   <p class="blog-post-meta">Sun, Jul 8, 2012</p>
    I&rsquo;m slowly uploading a couple of references and the pdf draft versions of them. Please find some of my publications in the corresponding section of this website.
Stay tuned for the bibtex description and some more papers. <a href="http://kaikunze.de/2012/07/08/some-of-my-publications-are-online/"> Read more... </a>
  </p>
</article>

		</div>
		
		<div class="blog-post">
			<article>
  <h3 class="post-title"><a href="/2012/07/07/phd-thesis-sources-on-github/">Compensating for On-body Placement Effects in Activity Recognition</a></h3>
   <p class="blog-post-meta">Sat, Jul 7, 2012</p>
    Finished my phD. last year in Passau. The thesis is already published over Opus Bayern. The pdf is open access, so feel free to read it (careful 19 MB pdf): Compensating for On-Body Placement Effects in Activity Recognition as pdf
However, the sources were not available. Finally, I got around to push the latex sources of my dissertation up to github.
Please feel free to use it as a thesis template, attribution would be apprecitated ;) <a href="http://kaikunze.de/2012/07/07/phd-thesis-sources-on-github/"> Read more... </a>
  </p>
</article>

		</div>
		
		<div class="blog-post">
			<article>
  <h3 class="post-title"><a href="/2012/06/18/using-device-motion-from-a-mobile-device-in-htmljavascript/">Using device motion in html/javascript</a></h3>
   <p class="blog-post-meta">Mon, Jun 18, 2012</p>
    A while ago, I built a simple demonstration on how to stream accelerometer data from a mobile device over websockets to a server just using html and javascript. It consists of a nodejs web server and a processing.org visualization. As soon as a mobile browser connects to the server a new red cube is shown on the screen (placed between randomly generated cubes). The transparent area around the cube changes depending on how strong one shakes the phone. <a href="http://kaikunze.de/2012/06/18/using-device-motion-from-a-mobile-device-in-htmljavascript/"> Read more... </a>
  </p>
</article>

		</div>
		
		<nav class="pagination" role="navigation">
	
	    <a class="newer-posts" href="/post/page/3/">&larr; Newer ...  </a>
	
	<span class="page-number">Page 4 of 4</span>
	
</nav>

	</div>
	</div>
	</main>
</br>
<footer class="blog-footer">
  <p> built by Kai, 2017. </p>
  <p>
    <a href="#">Back to top</a>
  </p>
</footer>
</body>
<script type="text/javascript">
    var GoSquared = {};
    GoSquared.acct = "GSN-181843-L";
     (function(w){
    function gs(){
      w._gstc_lt = +new Date;
      var d = document, g = d.createElement("script");
      g.type = "text/javascript";
      g.src = "//d1l6p2sc9645hc.cloudfront.net/tracker.js";
      var s = d.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(g, s);
    }
    w.addEventListener ?
      w.addEventListener("load", gs, false) :
      w.attachEvent("onload", gs);
    })(window);
</script>
</html>

