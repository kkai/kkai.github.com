<!DOCTYPE html>
<html lang="en" itemscope itemtype="http://schema.org/WebPage">
  <head>
    

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">

  <title>Posts - Kai Kunze</title><script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "WebSite",
    "name": "Kai Kunze",
    
    "url": "https:\/\/kaikunze.de\/"
}
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Organization",
  "name": "",
  "url": "https:\/\/kaikunze.de\/"
  
  
  
  
}
</script>

<meta property="og:title" content="Posts" />
<meta property="og:image" content="https://kaikunze.de/img/avatar-icon-1.png" />
<meta property="og:url" content="https://kaikunze.de/post/" />
<meta property="og:type" content="website" />
<meta property="og:site_name" content="Kai Kunze" />

  <meta name="twitter:title" content="Posts" />
  <meta name="twitter:image" content="https://kaikunze.de/img/avatar-icon-1.png" />
  <meta name="twitter:card" content="summary" />
  <link href='https://kaikunze.de/img/favicon.ico' rel='icon' type='image/x-icon'/>
  <meta name="generator" content="Hugo 0.154.3">
  <link rel="alternate" href="https://kaikunze.de/index.xml" type="application/rss+xml" title="Kai Kunze"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css" integrity="sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.5.0/css/all.css" integrity="sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU" crossorigin="anonymous">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous"><link rel="stylesheet" href="https://kaikunze.de/css/main.css" /><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" />
  <link rel="stylesheet" href="https://kaikunze.de/css/highlight.min.css" /><link rel="stylesheet" href="https://kaikunze.de/css/codeblock.css" /><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.css" integrity="sha384-h/L2W9KefUClHWaty3SLE5F/qvc4djlyR4qY3NUV5HGQBBW7stbcfff1+I/vmsHh" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/default-skin/default-skin.min.css" integrity="sha384-iD0dNku6PYSIQLyfTOpB06F2KCZJAKLOThS5HRe8b3ibhdEQ6eKsFf/EeFxdOt5R" crossorigin="anonymous">


  </head>
  <body>
    <nav class="navbar navbar-default navbar-fixed-top navbar-custom">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#main-navbar">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="https://kaikunze.de/">Kai Kunze</a>
    </div>

    <div class="collapse navbar-collapse" id="main-navbar">
      <ul class="nav navbar-nav navbar-right">
        
          
            <li>
              <a title="writing" href="/">writing</a>
            </li>
          
        
          
            <li>
              <a title="projects" href="/projects/">projects</a>
            </li>
          
        
          
            <li>
              <a title="papers" href="/publications/">papers</a>
            </li>
          
        
          
            <li>
              <a title="about" href="/about/">about</a>
            </li>
          
        

        

        
      </ul>
    </div>

    
      <div class="avatar-container">
        <div class="avatar-img-border">
          <a title="Kai Kunze" href="https://kaikunze.de/">
            <img class="avatar-img" src="https://kaikunze.de/img/avatar-icon-1.png" alt="Kai Kunze" />
          </a>
        </div>
      </div>
    

  </div>
</nav>




    


<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>


  
  
  







 
        <div id = "pagefloat"> </div>        
        <div id="nsplash" >
        <iframe id="nsplash_iframe"> </iframe>
        <div id="splash_title">
                   <h1>Posts</h1>
         </div>
              
              
        </div>

        <div class="intro-header no-img">
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
            <div class="post-heading">
                           
                <span class="post-meta">
  
  
  <i class="fas fa-calendar"></i>&nbsp;
  
  
    &nbsp;|&nbsp;<i class="fas fa-clock"></i>&nbsp;0&nbsp;minutes
  
  
    &nbsp;|&nbsp;<i class="fas fa-book"></i>&nbsp;0&nbsp;words
  
  
  
</span>


              
            </div>
          </div>
        </div>
      </div>
    </div>



    
  <div class="container" role="main">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        
        <div class="posts-list">
          
            <article class="post-preview">
    <a href="https://kaikunze.de/2013/11/05/amazing-okinawa-attending-the-asvai-workshop/">
        <h2 class="post-title">Amazing Okinawa - Attending the ASVAI Workshop</h2>
        
        
        
    </a>

    <p class="post-meta">
        <span class="post-meta">
  
  
  <i class="fas fa-calendar"></i>&nbsp;
  
  
    &nbsp;|&nbsp;<i class="fas fa-clock"></i>&nbsp;2&nbsp;minutes
  
  
    &nbsp;|&nbsp;<i class="fas fa-book"></i>&nbsp;266&nbsp;words
  
  
  
</span>


    </p>
    <div class="post-entry">
        
        <p class="lead">Cool research discussions at a nice location. The workshop was perfect fit to my research interests.</p>
<p>The <a href="http://www.am.sanken.osaka-u.ac.jp/ASVAI2013/">ASVAI workshop</a> gave a good overview about several research
efforts part of and related to the <a href="http://www.jst.go.jp/kisoken/crest/en/research_area/ongoing/areah21-1.html">JST CREST</a> and the JSPS Core-to-Core Sanken Program.</p>
<p><a href="http://www.am.sanken.osaka-u.ac.jp/~yagi/">Prof. Yasushi Yagi</a>
showed how to infer intention from gait analysis.
Interestingly, he showed research about the relationship
of gaze and gait.</p>
<p><a href="http://ai.stanford.edu/~alireza/">Dr. Alireza Fathi</a>
presented cool work about ego centric cameras. He showed
how to estimate gaze using ego centric cameras during
cooking tasks and psychological studies.</p>
<p><a href="http://www.uh.edu/class/psychology/about/people/hanako-yoshida/index.php">Prof. Hanako Yoshida</a> explores
social learning in infants (equipping children with mobile
eye trackers &hellip; awesome!), inferring developmental stages
giving more insights in the learning process.</p>
<p><a href="http://www.irc.atr.jp/~m-shiomi/">Prof. Masahiro Shiomi</a>
spoke about his research trying to adapt robot behavior
to fit into social public spaces ( videos about
people running away from a robot included ;) ).
Currently, they focus on service robots and model their
behavior according to successful human service personnel.</p>
<p><a href="http://www.hci.iis.u-tokyo.ac.jp/~ysato/">Prof. Yoichi Sato</a> presented work related to
detecting visual attention. They use visual saliency
on video to train an appearance-based eye tracking.
Really interesting work, I had a chance to talk a bit
more with <a href="http://www.hci.iis.u-tokyo.ac.jp/~sugano/">Yusuke Sugano</a>, cool research :)</p>
<p>Of course, Koichi also gave an overview about our work.
If you want to read more, checkout the <a href="/papers/pdf/kunze2013activity.pdf">IEEE Computer article</a>.</p>
<p>I&rsquo;m looking forward to the main conference.
Here&rsquo;s a tag cloud using the abstracts of ACPR and ASVAI papers:</p>
<p><img src="/imgs/acpr_wordcloud.png" alt="Tag cloud"></p>
<p>We present demonstrations and new results
of the eye tracking on commodity
tablets/smart phones and a sharing infrastructure for our document annotation for smart phones.</p>

        
    </div>

    

</article>
          
            <article class="post-preview">
    <a href="https://kaikunze.de/2013/09/20/a-week-with-glass/">
        <h2 class="post-title">A Week with Glass</h2>
        
        
        
    </a>

    <p class="post-meta">
        <span class="post-meta">
  
  
  <i class="fas fa-calendar"></i>&nbsp;
  
  
    &nbsp;|&nbsp;<i class="fas fa-clock"></i>&nbsp;8&nbsp;minutes
  
  
    &nbsp;|&nbsp;<i class="fas fa-book"></i>&nbsp;1642&nbsp;words
  
  
  
</span>


    </p>
    <div class="post-entry">
        
        <p class="lead"> How my grandparents interact with GLASS, showed me that Google seems to be onto something. </p>
As there were a lot of researchers visiting for the Ubicomp /ISWC conferences, I could grab a Google Glass from one of the participants for a week. Thanks to the anonymous
donor (to avoid speculations it was none of the people mentioned below).
In the following some unsorted thoughts ... Sorry for typos etc. It's more a scratch pad so I don't forget the impressions I had.
<p>##First Impressions##
The Glass device feels expensive and a little bit futuristic.
I&rsquo;m impressed by the build quality and design. It has also a &ldquo;google&rdquo; feel to it, e.g. &ldquo;funny&rdquo; jokes in the manual (&ldquo;don&rsquo;t use glass for scuba diving &hellip;&rdquo;).
The display works extremely well and although glass is made for micro-interactions (quickly checking an email/sms, google now updates, making pictures), I could watch videos and read longer emails/documents on it without trouble and any sight problems (I experienced no headaches as happend with other setups, see below). It would be perfect for boring meetings if other people could not see what you are doing &hellip;
I assume the Glass design team made the conscious decision, to let other people know if you interact with glass. People can see if the screen is on and even recognize what&rsquo;s on the screen if they get close enough.</p>
<p>##Grandparents and Mother with Google Glass##</p>
<p>I have a basic test for technology or research topics in general.
I try to explain it to my grandparents and mother to see if they understand it and find it interesting.
Various head-mounted displays, tablets and activity recognition algorithms were tested this way &hellip;
E.g. they were not so big fans of tablets/slates or smart phones until they played with an iPhone and iPad.</p>
<p><img src="/imgs/oma.jpg" alt="oma">
<img src="/imgs/opa_sh.jpg" alt="opa"></p>
<p>Surprisingly, my grandparents did not have the reservation they have towards other computing devices.
Usually, they have the feeling that they could destroy something and are extra careful/hesitant.
Yet, Google Glass looks like glasses, so it was easy for them to setup and use.
The system worked quite well (although so far only English is supported), speech recognition
and touch interface were simple to learn after a quick 5 min. introduction. I was surprised myself &hellip;</p>
<p>Sadly, the speech interface does a poor job with German names, e.g. googleing for &ldquo;Apfelkuchen Rezept&rdquo; (Apple cake recipe) did not work as intended.</p>
<p>Yet, both of them saw potential in Glass and could imagine wearing it during the day.
I was most astound by the application cases they came up with.</p>
<p><img src="/imgs/opa_pills.jpg" alt="opa pills"></p>
<p>My grandfather took a picture of his pills he needs to take after each meal.
He told me, he always wonders if he has taken them or not and sometimes checks 2-3 times after
a meal to be certain. Taking a picture and using the touch panel to browse recent pictures (with timestamp), he can easily figure out when he took them the last time.</p>
<p>My grandmother would love to use Glass for gardening. It happens sometimes, that she gets a
phone call during garden work and then she has to change shoes, take of gloves etc. and hurry
to the portable phone. Additionally, she likes to get the advice of my mum or friends about
where to put which flower seeds etc. so she asked me if it&rsquo;s possible to show the video stream from
Glass to other people over the Internet :)</p>
<p>We also did a practise test, My grandmother and mother wore Glass during shopping in Karlsruhe.
Both of them wear glasses, so not too many people noticed or looked at them. I think they assumed it&rsquo;s some kind of medical device or sight improvement etc.</p>
<p><img src="/imgs/oma2.jpg" alt="oma ka">
<img src="/imgs/mum.jpg" alt="mum ka"></p>
<p>My mother used the time line in glass to track when she made the pictures and traced back when she saw something nice to figure out at which store the item was. She tried taking pictures of price tags. Unfortunately, the resolution on the screen is not high enough
to read the price, yet this could be easily fixed with a zoom function for pictures.
Interestingly, she also carries a smart phone, yet she never got the idea to use it for shopping like Glass.</p>
<p>##Public Reactions##</p>
<p>As mentioned my mum and grandmother wore Glass nearly unnoticed.
This is quite different to my experience &hellip; If I wear it in public, most people in Karlsruhe
and Mannheim (the two cities I tried) eyed at me with wary faces (you can see the questions in their eyes : &ldquo;What is he wearing ?? Some medical device ?? NERD!! &ldquo;). This was particularly bad
when I spoke with a clerk or a person directly, as they kept staring at Glass instead of looking into my eyes ;)
Social reception was better when I was with my family. Strangely, people asked mostly my grandmother
what I was wearing. Very few approached me directly.
Reactions fell into 3 broad categories:</p>
<ol>
<li>&ldquo;WOW Cool &hellip; Glass! How is it? Can I try??&rdquo; &ndash; <em>Note</em> : Before it&rsquo;s released in public, I strongly recommend not wearing it on any campus with a larger IT faculty. I did not account for that and it was quite difficult to get over Karlsruhe University Campus :)</li>
<li>&ldquo;Stop violating my privacy!&rdquo; &ndash; During the week I had only one person directly approach me about privacy concerns. The person was quite angry at first. I believe it&rsquo;s mostly due to misinformation (something Google needs to take serious), as he believed Glass would stream automatically everything to Google and listen to all the conversations etc.. After I showed him the functionality of the device, how to use it and how to see if somebody is using it, he was calmer and actually liked it (could see the potential of a wearable display).</li>
<li>&ldquo;What&rsquo;s wrong with this guy?&rdquo; &ndash; Especially if I was traveling alone people stared at me. I asked 1 or 2 of the most obnoxious persons starring at me about it and they answered they thought I was wearing a medical device and they wondered &ldquo;what&rsquo;s wrong with me&rdquo; as I looked otherwise &ldquo;normal&rdquo;.</li>
</ol>
<p>##Some Issues##</p>
<p>The 3 biggest issues I had with it:</p>
<ol>
<li>Weight and placement - You need to get used to its weight. As I&rsquo;m not wearing prescription glasses, it feels strange to me wearing something on my nose. It&rsquo;s definitely heavier than glasses. After a couple of hours it is ok. Also it&rsquo;s always in your peripheral view, you need to get used to it.</li>
<li>Battery life - Ok, I played a lot with it, given I could use Glass only for a week. At the end (when me playing with it got fewer) I could get barely a day of usage. I expect that&rsquo;s something they can easily fix. Pst&hellip; you can also plug-in a portable USB battery to charge during usage :)</li>
<li>Social acceptance - This is the hardest one to crack. Having used Glass, I don&rsquo;t understand most of the privacy fears people raise. It&rsquo;s very obvious if a person is using the device/taking a picture etc. If I want to take covert pictures/videos of people, I believe it&rsquo;s easier to do with today&rsquo;s smart phones or spy cameras (available on Amazon for example) &hellip;</li>
</ol>
<p>##Some more Context##</p>
<p>When I unboxed Glass, I remembered how Paul, my phD. advisor, and Thad (Glass project manager)
chatted about how in future everybody would wear some kind of head-mounted
display and a computing device always connected to the
Internet, helping us with everyday tasks - augmentations
to our brain.</p>
<p>In the past, Paul was not a huge enthusiast about wearable displays and I agreed
with him. I attempted to use the <a href="https://en.wikipedia.org/wiki/Optical_head-mounted_display#MicroOptical_.2F_MyVu">MicroOptical</a> (the display used by Thad) several times and had always terrible headaches afterwards &hellip; Just not for me.</p>
<p><img src="/imgs/me.jpg" alt="Me"></p>
<p>Around 2004 - 2010, I played with various wearable setups to use during everyday life during my phD. each only for a week or couple of days. If you work on wearable computing you have to try at least. As seen in the picture above, the only setup working for me was a Prototype HMD from Zeiss with the
<a href="http://www.qbic.ethz.ch">Qbic</a>, an awesome belt-integrated linux pc by ETH (black belt buckle in the picture), and <a href="http://www.handykey.com">Twiddler 2</a>. Yet, I stopped using it as the glasses were quite heavy, maintaining/adjusting the software was a hassle (compared to the advantages) and -I have to admit- due to social pressure, imagine living as a cyborg in a small Bavarian town, mostly occupied by law and business students &hellip; I found my small, black, analog notebook more handy and less intimidating to other people. Today, I&rsquo;m an avid iPhone user (Things, Clear, Habit List, Textastic, Prompt and Lendromat  &hellip;).</p>
<p>##To sum up##
In total I was quite sceptical at first, the design reminded me too much on the Microoptical and the headaches I got using it. Completely unfounded! Even given the social acceptance issue, I cannot wait to get Glass for a longer test. However, I really need a good note taking app, running vim on glass would already be a selling point for me, replacing my black notebook (and maybe smart phone?). I undusted my Twiddler2 (took a long time to find it in the cellar) with hacked bluetooth connection, started practicing again and hope I can try it soon with Vim for Glass :D This is definitely not an application case for the mass market &hellip;  My grandparents told me that they believe there is a broader demand for such a device also by &ldquo;normal&rdquo; people (they actually want to use it!). So let&rsquo;s see.</p>
<p>Plus the researcher in my cannot wait to get easy accessible motion sensors onto the heads of a lot of people. Combined with the sensors in your pocket it&rsquo;s activity recognition heaven!</p>
<p>Let&rsquo;s <a href="https://news.ycombinator.com/item?id=6456220">discuss on Hacker News</a> if you want.</p>

        
    </div>

    
    <div class="blog-tags">
        
        <a href="https://kaikunze.de//tags/glass/">glass</a>&nbsp;
        
    </div>
    

</article>
          
            <article class="post-preview">
    <a href="https://kaikunze.de/2013/09/15/ubicom-iswc-impressions/">
        <h2 class="post-title">Ubicomp ISWC Impressions</h2>
        
        
        
    </a>

    <p class="post-meta">
        <span class="post-meta">
  
  
  <i class="fas fa-calendar"></i>&nbsp;
  
  
    &nbsp;|&nbsp;<i class="fas fa-clock"></i>&nbsp;2&nbsp;minutes
  
  
    &nbsp;|&nbsp;<i class="fas fa-book"></i>&nbsp;314&nbsp;words
  
  
  
</span>


    </p>
    <div class="post-entry">
        
        <p>Usually, I&rsquo;m not such a big fan of conference openings,
yet Friedemann Mattern provided a great intro giving
an overview about the origins of Pervasive and Ubicom
mentioning all important people and showing nice vintage
pictures from Hans Gellersen, Alois Ferscha,  Marc Langheinrich, Albrecht Schmidt, Kristof Van Laerhoven etc.</p>
<p>Deeply impressed by the organization, social and general
talk quality, I was a bit sceptical before the merger
of Pervasive / Ubicom and collocating ISWC, yet it was
completely unfounded.</p>
<p>We got some great feedback for Kazuma&rsquo;s and Shoya&rsquo;s demos.
They both did a great job introducing their work about:</p>
<ul>
<li><a href="/papers/kunze2013my.pdf">My Reading Life – Towards Utilizing Eyetracking on Unmodified Tablets and Phones</a></li>
<li><a href="/papers/kunze2013annotate.pdf">Annotate Me – Supporting Active Reading using Real-Time Document Image Retrieval On Mobile Devices</a></li>
</ul>
<p>We got also a lof of interest and feedback
to Andreas Bulling&rsquo;s and my work about recognizing
document types using only eye gaze.
By the way, below are the talk slides and the abstract of
the paper.
##ISWC Talk Slides##</p>
<script class="speakerdeck-embed" data-id="a0f70c60fdc20130468e062acf92b5fe" data-ratio="1.33333333333333" src="//speakerdeck.com/assets/embed.js">  </script>
<p>##Abstract##
Reading is a ubiquitous activity that many people even per- form in transit, such as while on the bus or while walking. Tracking reading enables us to gain more insights about ex- pertise level and potential knowledge of users – towards a reading log tracking and improve knowledge acquisition. As a first step towards this vision, in this work we investigate whether different document types can be automatically de- tected from visual behaviour recorded using a mobile eye tracker. We present an initial recognition approach that com- bines special purpose eye movement features as well as ma- chine learning for document type detection. We evaluate our approach in a user study with eight participants and five Japanese document types and achieve a recognition perfor- mance of 74% using user-independent training.</p>
<p>Full paper link:
<a href="/papers/2013Kunze-5.pdf">I know what you are reading – Recognition of Document Types Using Mobile Eye Tracking</a></p>

        
    </div>

    
    <div class="blog-tags">
        
        <a href="https://kaikunze.de//tags/conference/">conference</a>&nbsp;
        
        <a href="https://kaikunze.de//tags/research/">research</a>&nbsp;
        
        <a href="https://kaikunze.de//tags/ubicomp/">ubicomp</a>&nbsp;
        
        <a href="https://kaikunze.de//tags/iswc/">iswc</a>&nbsp;
        
    </div>
    

</article>
          
            <article class="post-preview">
    <a href="https://kaikunze.de/2013/09/06/excited-about-ubicomp-and-iswc/">
        <h2 class="post-title">Excited about Ubicomp and ISWC</h2>
        
        
        
    </a>

    <p class="post-meta">
        <span class="post-meta">
  
  
  <i class="fas fa-calendar"></i>&nbsp;
  
  
    &nbsp;|&nbsp;<i class="fas fa-clock"></i>&nbsp;2&nbsp;minutes
  
  
    &nbsp;|&nbsp;<i class="fas fa-book"></i>&nbsp;240&nbsp;words
  
  
  
</span>


    </p>
    <div class="post-entry">
        
        <p>This year I&rsquo;m really looking forward to <a href="http://ubicomp.org">Ubicomp</a> and <a href="http://iswc.net">ISWC</a>,
it&rsquo;s the first time that Ubicomp and Pervasive merged into one conference and it&rsquo;s the first time the venue sold out with 700 participants.</p>
<p>I cannot wait to chat with old friends and experts (most are both :)).</p>
<p>The field slowly matures. Especially, the wearable research is really pushing towards prime-time. Most prominently, Google Glass is getting a lot of focus also discussing its impacts on privacy. Yet, there is more and more talk about fitness bracelets/trackers and smart watches. I expect that we see more intelligent clothes and activity recognition work in commercial products in the coming years.</p>
<p>By the way, we have 3 poster papers and 2 demos at Ubicomp
and a short paper at ISWC.</p>
<p>###Ubicomp Demos and Posters###</p>
<ul>
<li><a href="/papers/kunze2013my.pdf">My Reading Life – Towards Utilizing Eyetracking on Unmodified Tablets and Phones</a></li>
<li><a href="/papers/kunze2013annotate.pdf">Annotate Me – Supporting Active Reading using Real-Time Document Image Retrieval On Mobile Devices</a></li>
<li><a href="/papers/cheng2013activity.pdf">Activity Recognition and Nutrition Monitoring in Every Day Situations with a Textile Capacitive Neckband</a></li>
</ul>
<p>###ISWC paper###</p>
<ul>
<li><a href="/papers/kunze2013know.pdf">I know what you are reading – Recognition of Document Types Using Mobile Eye Tracking</a></li>
</ul>
<p>Drop by at the demo,poster sessions and/or see me my talk on Thursday.</p>
<p>On a side note, Ubicomp really picks great locations. This year it&rsquo;s Zurich, next year Seattle and the year after it will be in Osaka. Seems I might be staying longer in Japan, than I originally planned ;)</p>

        
    </div>

    
    <div class="blog-tags">
        
        <a href="https://kaikunze.de//tags/conference/">conference</a>&nbsp;
        
        <a href="https://kaikunze.de//tags/research/">research</a>&nbsp;
        
        <a href="https://kaikunze.de//tags/ubicomp/">ubicomp</a>&nbsp;
        
        <a href="https://kaikunze.de//tags/iswc/">iswc</a>&nbsp;
        
    </div>
    

</article>
          
            <article class="post-preview">
    <a href="https://kaikunze.de/2013/08/26/icdar-2013-talk-slides-online/">
        <h2 class="post-title">ICDAR 2013 Talk Slides Online</h2>
        
        
        
    </a>

    <p class="post-meta">
        <span class="post-meta">
  
  
  <i class="fas fa-calendar"></i>&nbsp;
  
  
    &nbsp;|&nbsp;<i class="fas fa-clock"></i>&nbsp;1&nbsp;minutes
  
  
    &nbsp;|&nbsp;<i class="fas fa-book"></i>&nbsp;66&nbsp;words
  
  
  
</span>


    </p>
    <div class="post-entry">
        
        <p>The slides for my two talks today are online now.</p>
<p>##The Wordometer##</p>
<script class="speakerdeck-embed" data-id="05bef8c0f0aa01300a6d065f623bb37c" data-ratio="1.33333333333333" src="//speakerdeck.com/assets/embed.js">  </script>
<p>##Reading activity recognition using an off-the-shelf EEG##</p>
<script class="speakerdeck-embed" data-id="8a535520f0a901304b9d56bebfa6c61b" data-ratio="1.33333333333333" src="//speakerdeck.com/assets/embed.js"> </script>
<p>For more details, check out the papers:</p>
<ul>
<li><a href="/papers/pdf/kunze2013wordometer.pdf">The Wordometer – Estimating the Number of Words Read Using Document Image Retrieval and Mobile Eye Tracking</a></li>
<li><a href="/papers/pdf/kunze2013reading.pdf">Reading Activity Recognition using an off-the-shelf EEG — Detecting Reading Activities and Distinguishing Genres of Documents</a></li>
</ul>
<p>They are both published at <a href="http://icdar2013.org">ICDAR 2013</a>.</p>

        
    </div>

    

</article>
          
            <article class="post-preview">
    <a href="https://kaikunze.de/2013/08/23/recognizing-reading-activities/">
        <h2 class="post-title">Recognizing Reading Activities</h2>
        
        
        
    </a>

    <p class="post-meta">
        <span class="post-meta">
  
  
  <i class="fas fa-calendar"></i>&nbsp;
  
  
    &nbsp;|&nbsp;<i class="fas fa-clock"></i>&nbsp;2&nbsp;minutes
  
  
    &nbsp;|&nbsp;<i class="fas fa-book"></i>&nbsp;326&nbsp;words
  
  
  
</span>


    </p>
    <div class="post-entry">
        
        <script class="speakerdeck-embed" data-id="e9181bd0ee3501308d2a1ad4cad5346c" data-ratio="1.2994923857868" src="//speakerdeck.com/assets/embed.js"> </script>
<p>Just finished my keynote talk at CBDAR (Workshop of <a href="http://icdar2013.org/">ICDAR</a>),
got a lot of questions and have a lot of new research ideas :)</p>
<p>I&rsquo;m pretty ignorant about Document Analysis (and Computer Vision in general), so it&rsquo;s great to talk to some
experts in the field. Pervasive Computing and Document Analysis are very complementary
and as such interesting to combine.</p>
<p>Here are my talk slides, followed by the talk abstract.</p>
<p>##Real-life Activity Recognition - Talk Abstract##</p>
<p>Most applications in intelligent environments so far strongly rely on specific sensor combinations at predefined positions, orientations etc. While this might be acceptable for some application domains (e.g. industry), it hinders the wide adoption of pervasive computing. How can we extract high level information about human actions and complex real world situations from heterogeneous ensembles of simple, often unreliable sensors embedded in commodity devices?</p>
<p>This talk mostly focuses on how to use body-worn devices for activity recognition in general, and how to combine them with infrastructure sensing and computer vision approaches for a specific high level human activity, namely better understanding knowledge acquisition (e.g. recognizing reading activities).</p>
<p>We discuss how placement variations of electronic appliances carried by the user influence the possibility of using sensors integrated in those appliances for human activity recognition. I categorize possible variations into four classes: environmental placements, placement on different body parts (e.g. jacket pocket on the chest, vs. a hip holster vs. the trousers pocket), small displacement within a given coarse location (e.g. device shifting in a pocket), and different orientations.For each of these variations, I give an overview of our efforts to deal with them.</p>
<p>In the second part of the talk, we combine several pervasive sensing approaches (computer vision, motion-based activity recognition etc.) to tackle the problem of recognizing and classifying knowledge acquisition tasks with a special focus on reading. We discuss which sensing modalities can be used for digital and offline reading recognition, as well as how to combine them dynamically.</p>

        
    </div>

    
    <div class="blog-tags">
        
        <a href="https://kaikunze.de//tags/research/">research</a>&nbsp;
        
        <a href="https://kaikunze.de//tags/keynote/">keynote</a>&nbsp;
        
    </div>
    

</article>
          
            <article class="post-preview">
    <a href="https://kaikunze.de/2013/08/20/wordometer-and-document-analysis-using-pervasive-sensing/">
        <h2 class="post-title">Wordometer and Document Analysis using Pervasive Sensing</h2>
        
        
        
    </a>

    <p class="post-meta">
        <span class="post-meta">
  
  
  <i class="fas fa-calendar"></i>&nbsp;
  
  
    &nbsp;|&nbsp;<i class="fas fa-clock"></i>&nbsp;2&nbsp;minutes
  
  
    &nbsp;|&nbsp;<i class="fas fa-book"></i>&nbsp;424&nbsp;words
  
  
  
</span>


    </p>
    <div class="post-entry">
        
        <p><img src="/images/wordometer.png" alt="wordometer"></p>
<p>In the last couple of months, I got more and more interested in learning, especially reading.
Loving tech and sports, I got easily hooked on the Quantified Self movement (I own a Zeo Sleeping Coach and several step counters).
Seeing how measuring myself transformed me. I lost around 4 kg and feel healthier/fitter, since
I started tracking. I wonder why we don&rsquo;t have similar tools for our learning behavior.</p>
<p>So we created a simple Wordometer in our Lab, using the SMI mobile eyetracker and document image retrieval
(LLAH). We simply detect reading (very distinct horizontal or vertical movements) and afterwards count line breaks.
Assuming a fixed number of words per line, voilà here is your Wordometer. The document image retreival is used
to keep the accuracy at around 5-7 % (comparable to the pedometers measuring your steps each day).</p>
<p>Of course, there are a couple of limitations:</p>
<ol>
<li>Mobile Eyetrackers are DAMN expensive. Yet, the main reason being that there is no demand and they are manufactured in relatively low numbers. A glass frame, 2 cameras and 2 infra-red sources that&rsquo;s it (together with a bit of image processing magic).</li>
<li>Document Image Retrieval means you need to register all documents with a server before reading them. I won&rsquo;t go into details as this limitation is the easiest to get rid of. We are currently working on a method without it. At the beginning it was easier to include (and improve the accuracy rate).</li>
<li>Not everybody likes to wear glasses. With the recent mixed reception of Google Glass, it seems that wearing glasses is way more a fashion statement than wearing a big &ldquo;smart&rdquo; phone or similar. So this tech might not be for everybody.</li>
</ol>
<p>Overall, I&rsquo;m still very exited on what a cheap, public avaiable Wordometer will do to the reading habits of people and their &ldquo;knowledge life&rdquo;.
We&rsquo;ll continue working on it ;)</p>
<p>We are also using eyetracking, EEG and other sensors to get more information about what/how a user is reading.
Interestingly, it seems using the Emotiv EEG we can detect reading versus not reading and even some document types
(manga versus textbook).</p>
<blockquote>
<p>Disclaimer:
This work would not be possible without two very talented students: Hitoshi Kawaichi and Kazuyo Yoshimura.
Thanks for the hard work :D</p>
</blockquote>
<p>For more details, check out the papers:</p>
<ul>
<li><a href="/papers/pdf/kunze2013wordometer.pdf">The Wordometer – Estimating the Number of Words Read Using Document Image Retrieval and Mobile Eye Tracking</a></li>
<li><a href="/papers/pdf/kunze2013reading.pdf">Reading Activity Recognition using an off-the-shelf EEG — Detecting Reading Activities and Distinguishing Genres of Documents</a></li>
</ul>
<p>They are both published at <a href="http://icdar2013.org">ICDAR 2013</a>.</p>

        
    </div>

    

</article>
          
            <article class="post-preview">
    <a href="https://kaikunze.de/2013/04/29/kai-@-chi/">
        <h2 class="post-title">Kai @ CHI</h2>
        
        
        
    </a>

    <p class="post-meta">
        <span class="post-meta">
  
  
  <i class="fas fa-calendar"></i>&nbsp;
  
  
    &nbsp;|&nbsp;<i class="fas fa-clock"></i>&nbsp;1&nbsp;minutes
  
  
    &nbsp;|&nbsp;<i class="fas fa-book"></i>&nbsp;137&nbsp;words
  
  
  
</span>


    </p>
    <div class="post-entry">
        
        <p>So it&rsquo;s my first time at <a href="http://chi2013.acm.org">CHI</a>. Pretty amazing so far &hellip;
Will blog about more later.</p>
<p><a href="http://kaikunze.de/posters/chiposter2013.pdf"><img src="/posters/tb-chiposter2013.jpg" alt="Chi Poster"></a><img src="/imgs/kai@chi.jpg" alt="Kai@CHI"></p>
<p>I&rsquo;m in the first poster rotation, starting this afternoon:
&ldquo;Towards inferring language expertise using eye tracking&rdquo;
Drop by my poster if you&rsquo;re around (or try to spot me, I&rsquo;m wearing the white &ldquo;Kai@CHI&rdquo; Shirt today :)).</p>
<p>Here&rsquo;s the abstract of our work, as well as the <a href="http://kaikunze.de/papers/kunze2013towards.pdf">link to the paper</a>.</p>
<p>&ldquo;We present initial work towards recognizing reading activities. This paper describes our efforts detect the English skill level of a user and infer which words are difficult for them to understand. We present an initial study of 5 students and show our findings regarding the skill level assessment. We explain a method to spot difficult words. Eye tracking is a promising technology to examine and assess a user’s skill level.&rdquo;</p>

        
    </div>

    
    <div class="blog-tags">
        
        <a href="https://kaikunze.de//tags/chi/">chi</a>&nbsp;
        
        <a href="https://kaikunze.de//tags/research/">research</a>&nbsp;
        
        <a href="https://kaikunze.de//tags/poster/">poster</a>&nbsp;
        
    </div>
    

</article>
          
            <article class="post-preview">
    <a href="https://kaikunze.de/2013/04/24/ar-dagstuhl-report-online/">
        <h2 class="post-title">Activity Recognition Dagstuhl Report Online</h2>
        
        
        
    </a>

    <p class="post-meta">
        <span class="post-meta">
  
  
  <i class="fas fa-calendar"></i>&nbsp;
  
  
    &nbsp;|&nbsp;<i class="fas fa-clock"></i>&nbsp;1&nbsp;minutes
  
  
    &nbsp;|&nbsp;<i class="fas fa-book"></i>&nbsp;166&nbsp;words
  
  
  
</span>


    </p>
    <div class="post-entry">
        
        <p>If you wonder how we spent German tax money, the summary of the
Activity Recognition Dagstuhl seminar is now online.</p>
<p><a href="http://drops.dagstuhl.de/opus/volltexte/2013/3987/">Human Activity Recognition in Smart Environments (Dagstuhl Seminar 12492)</a></p>
<hr>
<p>Here&rsquo;s the abstract:</p>
<p>This report documents the program and the outcomes of Dagstuhl Seminar 12492 &ldquo;Human Activity Recognition in Smart Environments&rdquo;. We established the basis for a scientific community surrounding &ldquo;activity recognition&rdquo; by involving researchers from a broad range of related research fields. 30 academic and industry researchers from US, Europe and Asia participated from diverse fields including pervasive computing, over network analysis and computer vision to human computer interaction. The major results of this Seminar are the creation of a activity recognition repository to share information, code, publications and the start of an activity recognition book aimed to serve as a scientific introduction to the field. In the following, we go into more detail about the structure of the seminar, discuss the major outcomes and give an overview about discussions and talks given during the seminar.</p>

        
    </div>

    
    <div class="blog-tags">
        
        <a href="https://kaikunze.de//tags/dagstuhl/">dagstuhl</a>&nbsp;
        
        <a href="https://kaikunze.de//tags/research/">research</a>&nbsp;
        
    </div>
    

</article>
          
            <article class="post-preview">
    <a href="https://kaikunze.de/2013/02/06/some-of-my-favorites-from-the-29c3-recordings/">
        <h2 class="post-title">Some of my favorites from the 29c3 recordings</h2>
        
        
        
    </a>

    <p class="post-meta">
        <span class="post-meta">
  
  
  <i class="fas fa-calendar"></i>&nbsp;
  
  
    &nbsp;|&nbsp;<i class="fas fa-clock"></i>&nbsp;2&nbsp;minutes
  
  
    &nbsp;|&nbsp;<i class="fas fa-book"></i>&nbsp;381&nbsp;words
  
  
  
</span>


    </p>
    <div class="post-entry">
        
        <p>Over the last weeks, I finally got around to watch some of the <a href="https://events.ccc.de/congress/2012/wiki/Main_Page">29c3</a> recordings. Here are some of my favorites. I will update the list accordingly.</p>
<p>I link to the official recording available from the CCC domain. The talks however are also on youtube. Just search for the talk title.</p>
<p>In General, I found most talks focused on security, sadly not really my main interest. I missed some research and culture talks that were present the last years. Examples from the last years:<a href="http://www.youtube.com/watch?v=mS4k0hFhPeQ">Data Mining for Hackers</a> awesome talk!! or <a href="http://www.youtube.com/watch?v=Lxf60pRz1tM">one</a> of Bicyclemark episodes. Bicylcemark we miss you :)</p>
<h2 id="english">English</h2>
<p>Out of the hacking talks, for me by far the most entertaining was <a href="http://media.ccc.de/browse/congress/2012/29c3-5400-en-hacking_cisco_phones_h264.html">Hacking Cisco Phones</a>. Scary and so cool. Ang Cui and Michael Costello are also quite good presenters. The hand-drawn slides give the visuals also a nice touch. I won&rsquo;t spoil the contents. just watch it.</p>
<p>So far my most favorite talk is <a href="http://media.ccc.de/browse/congress/2012/29c3-5138-en-romantichackers_h264.html">Romantic Hackers</a> by Anne Marggraf-Turley and Prof. Richard Marggraf-Turley. About surveillance andy hackers in the Romantic period. I was not aware that the privacy problems and the ideas about pervasive surveillance had been discussed and encountered so early in human history. Very Insightful and fun.</p>
<p>The <a href="http://media.ccc.de/browse/congress/2012/29c3-5088-en-many_tamagotchis_were_harmed_in_the_making_of_this_presentation_h264.html">Tamagochi Talk</a> was fun. Although the speaker seemed to be a bit nervous (listening to her voice), she gave some great insides how Tamagochis work and how to hack them.</p>
<p>The keynote from Jacob Applebaum, <a href="http://media.ccc.de/browse/congress/2012/29c3-5385-en-not_my_department_h264.html">Not my department</a> is a call to action for the tech community discussing about the responsibilities we have regarding our research and how it might be used. Although Applebaum is a great public speaker and the topic is of utmost importance, for some people new to the discussion it might seem a bit out of context and difficult to understand.</p>
<h2 id="german">German</h2>
<p>If you can speak German or want to practice it, check them out &hellip;
Of course, the usual subjects <a href="http://media.ccc.de/browse/congress/2012/29c3-5198-en-de-fnord_jahresrueckblick2012_h264.html">Fnord News Show</a> and <a href="http://media.ccc.de/browse/congress/2012/29c3-5244-de-en-security_nightmares2012_h264.html">Security Nightmares</a> are always great candidates to watch.</p>
<p>I&rsquo;m also always looking forward to the yearly Martin Haase Talk. Unfortunately, the official release is not online yet. Interesting especially for language geeks.</p>
<p>The talk <a href="http://media.ccc.de/browse/congress/2012/29c3-5121-de-en-sind_faire_computer_moeglich_h264.html">Are fair computers possible?</a> explores what needs to change in manufacturing standards etc. to produce computers without child labor and fair employment conditions for all workers involved.</p>

        
    </div>

    
    <div class="blog-tags">
        
        <a href="https://kaikunze.de//tags/hacking/">hacking</a>&nbsp;
        
        <a href="https://kaikunze.de//tags/conference/">conference</a>&nbsp;
        
    </div>
    

</article>
          
        </div>
        
          <ul class="pager main-pager">
            
              <li class="previous">
                <a href="https://kaikunze.de/post/page/3/">&larr; Newer Posts</a>
              </li>
            
            
              <li class="next">
                <a href="https://kaikunze.de/post/page/5/">Older Posts &rarr;</a>
              </li>
            
          </ul>
        
      </div>
    </div>
  </div>

      
<footer>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <ul class="list-inline text-center footer-links">
          
          
          <li>
            <a href="/post/index.xml" title="RSS">
              <span class="fa-stack fa-lg">
                <i class="fas fa-circle fa-stack-2x"></i>
                <i class="fas fa-rss fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
        </ul>
        <p class="credits copyright text-muted">
          

          &nbsp;&bull;&nbsp;&copy;
          
            2026
          

          
            &nbsp;&bull;&nbsp;
            <a href="https://kaikunze.de/">Kai Kunze</a>
          
      </div>
    </div>
  </div>
</footer><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.js" integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/contrib/auto-render.min.js" integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe" crossorigin="anonymous"></script>
<script   src="https://code.jquery.com/jquery-3.7.0.min.js"  integrity="sha256-2Pmvv0kuTBOenSvLm6bvfBSSHrUJ+3A7x6P5Ebd07/g="   crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

<script src="https://kaikunze.de/js/main.js"></script>
<script src="https://kaikunze.de/js/highlight.min.js"></script>
<script> hljs.initHighlightingOnLoad(); </script>
<script> $(document).ready(function() {$("pre.chroma").css("padding","0");}); </script><script> renderMathInElement(document.body); </script><script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.js" integrity="sha384-QELNnmcmU8IR9ZAykt67vGr9/rZJdHbiWi64V88fCPaOohUlHCqUD/unNN0BXSqy" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe-ui-default.min.js" integrity="sha384-m67o7SkQ1ALzKZIFh4CiTA8tmadaujiTa9Vu+nqPSwDOqHrDmxLezTdFln8077+q" crossorigin="anonymous"></script><script src="https://kaikunze.de/js/load-photoswipe.js"></script>







    
  </body>
</html>

