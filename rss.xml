<?xml version="1.0"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>example.com</title>
    <link>http://example.com/</link>
    <atom:link href="http://example.com/rss.xml" rel="self" type="application/rss+xml" />
    <description>Your Website</description>
    <language>en-us</language>
    <pubDate>Mon, 29 Apr 2013 12:25:17 +0200</pubDate>
    <lastBuildDate>Mon, 29 Apr 2013 12:25:17 +0200</lastBuildDate>

    
    <item>
      <title>Kai  Chi</title>
      <link>http://example.com/2013/04/29/kai--chi</link>
      <pubDate>Mon, 29 Apr 2013 00:00:00 +0200</pubDate>
      <author>you@example.com (Your Name)</author>
      <guid>http://example.com/2013/04/29/kai--chi</guid>
      <description>&lt;p&gt;So it&amp;#8217;s my first time at &lt;a href='http://chi2013.acm.org'&gt;CHI&lt;/a&gt;. Pretty amazing so far &amp;#8230; Will blog about more later.&lt;/p&gt;

&lt;p&gt;&lt;a href='http://kaikunze.de/posters/chiposter2013.pdf'&gt;&lt;img alt='Chi Poster' src='/posters/tb-chiposter2013.jpg' /&gt;&lt;/a&gt;&lt;img alt='Kai@CHI' src='/imgs/kai@chi.jpg' /&gt;&lt;/p&gt;

&lt;p&gt;I&amp;#8217;m in the first poster rotation, starting this afternoon: &amp;#8220;Towards inferring language expertise using eye tracking&amp;#8221; Drop by my poster if you&amp;#8217;re around (or try to spot me, I&amp;#8217;m wearing the white &amp;#8220;Kai@CHI&amp;#8221; Shirt today :)).&lt;/p&gt;

&lt;p&gt;Here&amp;#8217;s the abstract of our work, as well as the &lt;a href='http://kaikunze.de/papers/2013kunze.pdf'&gt;link to the paper&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&amp;#8220;We present initial work towards recognizing reading activities. This paper describes our efforts detect the English skill level of a user and infer which words are difficult for them to understand. We present an initial study of 5 students and show our findings regarding the skill level assessment. We explain a method to spot difficult words. Eye tracking is a promising technology to examine and assess a user’s skill level.&amp;#8221;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Activity Recognition Dagstuhl Report Online</title>
      <link>http://example.com/research/2013/04/24/ar-dagstuhl-report-online</link>
      <pubDate>Wed, 24 Apr 2013 00:00:00 +0200</pubDate>
      <author>you@example.com (Your Name)</author>
      <guid>http://example.com/research/2013/04/24/ar-dagstuhl-report-online</guid>
      <description>&lt;p&gt;If you wonder how we spent German tax money, the summary of the Activity Recognition Dagstuhl seminar is now online.&lt;/p&gt;

&lt;p&gt;&lt;a href='http://drops.dagstuhl.de/opus/volltexte/2013/3987/'&gt;Human Activity Recognition in Smart Environments (Dagstuhl Seminar 12492)&lt;/a&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Here&amp;#8217;s the abstract:&lt;/p&gt;

&lt;p&gt;This report documents the program and the outcomes of Dagstuhl Seminar 12492 &amp;#8220;Human Activity Recognition in Smart Environments&amp;#8221;. We established the basis for a scientific community surrounding &amp;#8220;activity recognition&amp;#8221; by involving researchers from a broad range of related research fields. 30 academic and industry researchers from US, Europe and Asia participated from diverse fields including pervasive computing, over network analysis and computer vision to human computer interaction. The major results of this Seminar are the creation of a activity recognition repository to share information, code, publications and the start of an activity recognition book aimed to serve as a scientific introduction to the field. In the following, we go into more detail about the structure of the seminar, discuss the major outcomes and give an overview about discussions and talks given during the seminar.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Some of my favorites from the 29c3 recordings</title>
      <link>http://example.com/2013/02/06/some-of-my-favorites-from-the-29c3-recordings</link>
      <pubDate>Wed, 06 Feb 2013 00:00:00 +0100</pubDate>
      <author>you@example.com (Your Name)</author>
      <guid>http://example.com/2013/02/06/some-of-my-favorites-from-the-29c3-recordings</guid>
      <description>&lt;p&gt;Over the last weeks, I finally got around to watch some of the &lt;a href='https://events.ccc.de/congress/2012/wiki/Main_Page'&gt;29c3&lt;/a&gt; recordings. Here are some of my favorites. I will update the list accordingly.&lt;/p&gt;

&lt;p&gt;I link to the official recording available from the CCC domain. The talks however are also on youtube. Just search for the talk title.&lt;/p&gt;

&lt;p&gt;In General, I found most talks focused on security, sadly not really my main interest. I missed some research and culture talks that were present the last years. Examples from the last years:&lt;a href='http://www.youtube.com/watch?v=mS4k0hFhPeQ'&gt;Data Mining for Hackers&lt;/a&gt; awesome talk!! or &lt;a href='http://www.youtube.com/watch?v=Lxf60pRz1tM'&gt;one&lt;/a&gt; of Bicyclemark episodes. Bicylcemark we miss you :)&lt;/p&gt;

&lt;h2 id='english'&gt;English&lt;/h2&gt;

&lt;p&gt;Out of the hacking talks, for me by far the most entertaining was &lt;a href='http://media.ccc.de/browse/congress/2012/29c3-5400-en-hacking_cisco_phones_h264.html'&gt;Hacking Cisco Phones&lt;/a&gt;. Scary and so cool. Ang Cui and Michael Costello are also quite good presenters. The hand-drawn slides give the visuals also a nice touch. I won&amp;#8217;t spoil the contents. just watch it.&lt;/p&gt;

&lt;p&gt;So far my most favorite talk is &lt;a href='http://media.ccc.de/browse/congress/2012/29c3-5138-en-romantichackers_h264.html'&gt;Romantic Hackers&lt;/a&gt; by Anne Marggraf-Turley and Prof. Richard Marggraf-Turley. About surveillance andy hackers in the Romantic period. I was not aware that the privacy problems and the ideas about pervasive surveillance had been discussed and encountered so early in human history. Very Insightful and fun.&lt;/p&gt;

&lt;p&gt;The &lt;a href='http://media.ccc.de/browse/congress/2012/29c3-5088-en-many_tamagotchis_were_harmed_in_the_making_of_this_presentation_h264.html'&gt;Tamagochi Talk&lt;/a&gt; was fun. Although the speaker seemed to be a bit nervous (listening to her voice), she gave some great insides how Tamagochis work and how to hack them.&lt;/p&gt;

&lt;p&gt;The keynote from Jacob Applebaum, &lt;a href='http://media.ccc.de/browse/congress/2012/29c3-5385-en-not_my_department_h264.html'&gt;Not my department&lt;/a&gt; is a call to action for the tech community discussing about the responsibilities we have regarding our research and how it might be used. Although Applebaum is a great public speaker and the topic is of utmost importance, for some people new to the discussion it might seem a bit out of context and difficult to understand.&lt;/p&gt;

&lt;h2 id='german'&gt;German&lt;/h2&gt;

&lt;p&gt;If you can speak German or want to practice it, check them out &amp;#8230; Of course, the usual subjects &lt;a href='http://media.ccc.de/browse/congress/2012/29c3-5198-en-de-fnord_jahresrueckblick2012_h264.html'&gt;Fnord News Show&lt;/a&gt; and &lt;a href='http://media.ccc.de/browse/congress/2012/29c3-5244-de-en-security_nightmares2012_h264.html'&gt;Security Nightmares&lt;/a&gt; are always great candidates to watch.&lt;/p&gt;

&lt;p&gt;I&amp;#8217;m also always looking forward to the yearly Martin Haase Talk. Unfortunately, the official release is not online yet. Interesting especially for language geeks.&lt;/p&gt;

&lt;p&gt;The talk &lt;a href='http://media.ccc.de/browse/congress/2012/29c3-5121-de-en-sind_faire_computer_moeglich_h264.html'&gt;Are fair computers possible?&lt;/a&gt; explores what needs to change in manufacturing standards etc. to produce computers without child labor and fair employment conditions for all workers involved.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ACM Multimedia 2012 Main Conference Notes</title>
      <link>http://example.com/conferences/2012/10/31/acm-multimedia-2012-day-2-notes</link>
      <pubDate>Wed, 31 Oct 2012 00:00:00 +0100</pubDate>
      <author>you@example.com (Your Name)</author>
      <guid>http://example.com/conferences/2012/10/31/acm-multimedia-2012-day-2-notes</guid>
      <description>&lt;p&gt;This is a scratchpad &amp;#8230; will fill the rest when I have time.&lt;/p&gt;

&lt;h2 id='papers'&gt;Papers&lt;/h2&gt;

&lt;p&gt;I really enjoyed the work from Heng Liu, Tao Mei et. al. &amp;#8220;Finding Perfect Rendezvous On the Go: Accurate Mobile Visual Localization and Its Applications to Routing&amp;#8221;. They combine existing research in a very interesting mixture. They use a visual localization method based on &lt;a href='http://phototour.cs.washington.edu/bundler/'&gt;bundler&lt;/a&gt; to detect where in the city a mobile phone user is. The application scenario I liked best was their collaborative localization for rendezvous :)&lt;/p&gt;

&lt;p&gt;The best paper award went to Zhi Wang, Lifeng Sun, Xiangwen Chen, Wenwu Zhu, Jiangchuan Liu, Minghua Chen and Shiqiang Yang for &amp;#8220;Propagation-Based Social-Aware Replication for Social Video Contents&amp;#8221;. They use the contacts mined over social networking to replicate content for better streaming and content distribution. The presentation was great, the research solid, still it&amp;#8217;s not a topic I&amp;#8217;m very interested in. However, for content providers it seems very useful.&lt;/p&gt;

&lt;p&gt;Shih-Yao Lin et. al. presented a system to recognize the users motion using the kinect and imitate them via a marionette in &amp;#8220;Action Recognition for Human-Marionette Interaction&amp;#8221;. I hoped to get more information about the interactions between users and marionettes, still very stylish presentation and artsy topic.&lt;/p&gt;

&lt;p&gt;Hamdi Dibeklioglu et. al. showed how to infer the age of a person when they are simling in &amp;#8220;A Smile Can Reveal Your Age: Enabling Facial Dynamics in Age Estimation&amp;#8221;. I find fascinating to hear about small cues that can tell a lot about a person or a situation.&lt;/p&gt;

&lt;p&gt;Fascinating work by Victoria Yanulevskaya et. al. (&amp;#8220;In the Eye of the Beholder: Employing Statistical Analysis and Eye Tracking for Analyzing Abstract Paintings&amp;#8221;). They link the emotional impact of a painting to the eye movements of the observer. Very interesting and in line with my current focus. I wonder if also expertise etc. can be recognized using sensors.&lt;/p&gt;

&lt;p&gt;Another very art focused paper I enjoyed was &amp;#8220;Dinner of Luciérnaga-An interactive Play with iPhone App in Theater&amp;#8221; by Yu-Chuan Tseng. Theater visitors can interact with the play using their smart phone (getting also feedback on the device &amp;#8230;.).&lt;/p&gt;

&lt;h2 id='posters_demos_competitions'&gt;Posters, Demos, Competitions&lt;/h2&gt;

&lt;p&gt;The winner of the Multimedia Grand Challenge was very well deserved. &amp;#8220;Analysis of Dance Movements using Gaussian Processes&amp;#8221; by Antoine Liutkus et. al. decomposed dance moves using Gaussian processes in movements with slow periodicity, high periodicity and moves that happened just once. Fascinating and applicable to so many fields &amp;#8230; :)&lt;/p&gt;

&lt;p&gt;A very neat demo was presented by Wei Zhang et. al.: &amp;#8220;FashionAsk: Pushing Community Answers to Your Fingertips&amp;#8221;.&lt;/p&gt;

&lt;h2 id='other_notes'&gt;Other Notes&lt;/h2&gt;

&lt;p&gt;As expected from any conference in Japan :), the organisation was flawless. In case any if the organisers is reading this. Thanks again. Nara is a perfect place for a venue like this (deer, world heritage sites, good food &amp;#8230;).&lt;/p&gt;

&lt;p&gt;More curiously, although there was a lot of talk about social media and some lively discussions on twitter, I seemed to be the only participant on &lt;a href='https://alpha.app.net/'&gt;ADN&lt;/a&gt; at least posting with hashtag.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ACM Multimedia 2012 Tutorials and Workshops</title>
      <link>http://example.com/conferences/2012/10/29/acm-multimedia-2012-day-1-notes</link>
      <pubDate>Mon, 29 Oct 2012 00:00:00 +0100</pubDate>
      <author>you@example.com (Your Name)</author>
      <guid>http://example.com/conferences/2012/10/29/acm-multimedia-2012-day-1-notes</guid>
      <description>&lt;p&gt;I attended the Tutorials &amp;#8220;Interacting with Image Collections – Visualisation and Browsing of Image Repositories&amp;#8221; and &amp;#8220;Continuous Analysis of Emotions for Multimedia Applications&amp;#8221; on the first day.&lt;/p&gt;

&lt;p&gt;The last day I went to &amp;#8220;Workshop on Audio and Multimedia Methods for Large Scale Video Analysis&amp;#8221; and to the &amp;#8220;Workshop on Interactive Multimedia on Mobile and Portable Devices&amp;#8221;.&lt;/p&gt;

&lt;p&gt;This is meant as a scratchpad &amp;#8230; I&amp;#8217;ll add more later if I have time.&lt;/p&gt;

&lt;h3 id='interacting_with_image_collections__visualisation_and_browsing_of_image_repositories'&gt;Interacting with Image Collections – Visualisation and Browsing of Image Repositories&lt;/h3&gt;

&lt;p&gt;Schaefer gave a overview about how to browse large scale image repositories. Interesting, yet of not really related to my research interests. He showed 3 approaches for retrieval: mapping-based, clustering-based and graph-based. I would have loved if he could have gone a bit more in detail in the mobile section at the end.&lt;/p&gt;

&lt;h3 id='continuous_analysis_of_emotions_for_multimedia_applications'&gt;Continuous Analysis of Emotions for Multimedia Applications&lt;/h3&gt;

&lt;p&gt;Hatice Gunes and Bjoern Schuller introduced a state of the art in emotion analysis. Their problems seem very similar to what we have to cope with in activity recognition, especially in terms of segmentation and continuous recognition problems. Their inference pipeline is comparable to ours in context recognition.&lt;/p&gt;

&lt;p&gt;Where Affective Computing seems to have an edge is in the standardized data sets. There are already quite a lot (mainly focusing on video and audio). I guess it&amp;#8217;s also easier compared to the very multi-modal datasets we deal with in activity recogntion.&lt;/p&gt;

&lt;p&gt;Hatice Gunes showed two videos of two girls, one is faking a laugh the other one is authentic. Interestingly enough, the whole audience was wrong in picking the authentic laugh. The fake laughing girl was overdoing it and laughed constantly. However, authentic laughter has a time component (coming in waves: increasing, decreasing, increasing again etc.).&lt;/p&gt;

&lt;p&gt;The tools section contained the obvious candidates (opencv, kinect, weka &amp;#8230;). Sadly they did not mention the new set of tools I love to use. Check out &lt;a href='http://pandas.pydata.org'&gt;Pandas&lt;/a&gt; and &lt;a href='http://ipython.org'&gt;iPython&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Good overview about the state of the art. I would have loved to get more information about the subjective nature of emotion. For me it&amp;#8217;s not as obvious as activity (already there is a lot of room of ambiguity). Also, depending on personal experience and cultural background, the emotional response to specific stimuli can be diverse.&lt;/p&gt;

&lt;h4 id='interesting_links'&gt;interesting links&lt;/h4&gt;

&lt;p&gt;&lt;a href='http://semaine-db.eu'&gt;Semaine Corpus&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href='http://www.multimediaeval.org/'&gt;Media Eval&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href='http://www.informatik.uni-augsburg.de/de/lehrstuehle/hcm/projects/tools/emovoice/'&gt;EmoVoice Audio Emotion classifier&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href='http://www.affectiva.com/q-sensor/'&gt;qsensor&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href='http://cinimodstudio.com/project/london-eye-mood-conductor/'&gt;London eye mood&lt;/a&gt;&lt;/p&gt;

&lt;h3 id='workshop_on_audio_and_multimedia_methods_for_large_scale_video_analysis'&gt;Workshop on Audio and Multimedia Methods for Large Scale Video Analysis&lt;/h3&gt;

&lt;h3 id='workshop_on_interactive_multimedia_on_mobile_and_portable_devices'&gt;Workshop on Interactive Multimedia on Mobile and Portable Devices&lt;/h3&gt;</description>
    </item>
    
    <item>
      <title>Laughing Faces App in the AppStore</title>
      <link>http://example.com/hacking/2012/08/30/laughing-faces-app-in-the-appstore</link>
      <pubDate>Thu, 30 Aug 2012 00:00:00 +0200</pubDate>
      <author>you@example.com (Your Name)</author>
      <guid>http://example.com/hacking/2012/08/30/laughing-faces-app-in-the-appstore</guid>
      <description>&lt;p&gt;Over the last couple of weeks, I was getting settled in my new job. As I&amp;#8217;m working with &lt;a href='http://imlab.jp'&gt;computer vision researchers&lt;/a&gt; now, I started playing with the camera api for the iPhone.&lt;/p&gt;

&lt;p&gt;Again, I&amp;#8217;m very surprised by the accessibility and quality of Apples apis and their sample code.&lt;/p&gt;

&lt;p&gt;&lt;img alt='Laughing Face' src='/imgs/laughing.png' /&gt;&lt;/p&gt;

&lt;p&gt;As a start, this little app is a &amp;#8220;privacy enhanced&amp;#8221; camera app for entertainment purposes. It uses face detection and draws a little laughing face on top of each recognized head in real time. I hesitated putting it in the store, yet was asked by some friends to do so (had to exchange the laughing face due to copyright constraints).&lt;/p&gt;

&lt;p&gt;Grab it while it&amp;#8217;s hot &amp;#8230; it&amp;#8217;s quite popular in Japan (understandable given the background, see below), China and Saudi Arabia (of all places, &amp;#8230; if somebody can tell me why, please send me a mail): &lt;a href='http://itunes.apple.com/us/app/laughing-faces/id551656355?mt=8'&gt;Laughing Faces AppStore Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;By the way, I had over 250 downloads the first day :) Oh if you wonder, the inspiration came from &lt;a href='http://en.wikipedia.org/wiki/Laughing_Man_(Ghost_in_the_Shell'&gt;Ghost in the Shell Standalone Complex&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;If people bug me enough, I will make the png exchangable. Cannot tell you too much, yet expect an update when iOS6 hits.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>AAAI activity context workshop notes</title>
      <link>http://example.com/scratchpad/2012/07/26/aaai-activity-context-workshop-notes</link>
      <pubDate>Thu, 26 Jul 2012 00:00:00 +0200</pubDate>
      <author>you@example.com (Your Name)</author>
      <guid>http://example.com/scratchpad/2012/07/26/aaai-activity-context-workshop-notes</guid>
      <description>&lt;p&gt;I enjoyed the AAAI context activity workshop a lot.&lt;/p&gt;

&lt;p&gt;The keynote &lt;a href='http://www.aaai.org/ocs/index.php/WS/AAAIW12/paper/view/5292/5556'&gt;How to make Face Recognition work (pdf)&lt;/a&gt; by &lt;a href='http://research.microsoft.com/en-us/um/people/akapoor/'&gt;Ashis Kapoor&lt;/a&gt; showed how to increase face recognition introducing very simple &amp;#8220;context&amp;#8221; constrains (two people in the same image cannot be the same person etc.). Very interesting work, I wonder how much better you can get introducing some more dynamic context recognition to the face recognition task.&lt;/p&gt;

&lt;p&gt;&lt;a href='http://www.cs.ubc.ca/~murphy/'&gt;Gail Murphy&lt;/a&gt; gave the other keynote &lt;a href='http://www.aaai.org/ocs/index.php/WS/AAAIW12/paper/view/5302/5557'&gt;Task Context for Knowledge Workers (pdf)&lt;/a&gt;. She introduces context modelling for tasks in GTD scenarios. Also quite interesting, as completely complimentary to my work (no mobile clients, sensors etc.).&lt;/p&gt;

&lt;p&gt;A lot of people were aware of our efforts during the &lt;a href='http://www.opportunity-project.eu/'&gt;Opportunity Project&lt;/a&gt; and the standard datasets we want to put out.&lt;/p&gt;

&lt;p&gt;Rim Helaoui presented work about using &lt;a href='http://www.aaai.org/ocs/index.php/WS/AAAIW12/paper/view/5269/5552'&gt;Probabilistic Description Logics (pdf)&lt;/a&gt; for activity recognition, an interesting approach trying to combine data driven and rule-based activity inference. They used the opportunity dataset ;)&lt;/p&gt;

&lt;p&gt;Bostjan Kaluza shared the call for more standardized datasets in context recognition in his talk about &lt;a href='http://www.aaai.org/ocs/index.php/WS/AAAIW12/paper/view/5305/5555'&gt;The Activity Recognition Repository (pdf)&lt;/a&gt;. A very important endeavor, I already also discussed several times. I think a broad effort in the field is necessary.&lt;/p&gt;

&lt;p&gt;All the final papers are up on the &lt;a href='http://activitycontext.org/final-papers/'&gt;workshop website&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Towards Dynamically Configurable Context Recognition Systems</title>
      <link>http://example.com/publication/2012/07/09/draft-version-of-aaai-workshop-paper-online</link>
      <pubDate>Mon, 09 Jul 2012 00:00:00 +0200</pubDate>
      <author>you@example.com (Your Name)</author>
      <guid>http://example.com/publication/2012/07/09/draft-version-of-aaai-workshop-paper-online</guid>
      <description>&lt;p&gt;Here&amp;#8217;s a &lt;a href='http://kaikunze.de/papers/2012Kunze.pdf'&gt;draft version of my publication&lt;/a&gt; for the &lt;a href='http://activitycontext.org/'&gt;Activity Context Workshop&lt;/a&gt; in Toronto. Bellow the abstract.&lt;/p&gt;

&lt;p&gt;Also download &lt;a href='http://kaikunze.de/slides/2012aaai-slides.pdf'&gt;the slides&lt;/a&gt; of my talk.&lt;/p&gt;

&lt;p&gt;Here&amp;#8217;s the link to the &lt;a href='https://github.com/kkai/snsrlog'&gt;source code for snsrlog for iPhone&lt;/a&gt; (which I mentioned during my talk).&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Abstract&lt;/p&gt;

&lt;p&gt;General representation, abstraction and exchange definitions are crucial for dynamically configurable context recognition. However, to evaluate potential definitions, suitable standard datasets are needed. This paper presents our effort to create and maintain large scale, multimodal standard datasets for context recognition research. We ourselves used these datasets in previous research to deal with placement effects and presented low-level sensor abstractions in motion based on-body sensing. Researchers, conducting novel data collections, can rely on the toolchain and the the low-level sensor abstractions summarized in this paper. Additionally, they can draw from our experiences developing and conducting context recognition experiments. Our toolchain is already a valuable rapid prototyping tool. Still, we plan to extend it to crowd-based sensing, enabling the general public to gather context data, learn more about their lives and contribute to context recognition research. Applying higher level context reasoning on the gathered context data is a obvious extension to our work.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Some of my publications are online</title>
      <link>http://example.com/publication/2012/07/08/some-of-my-publications-are-online</link>
      <pubDate>Sun, 08 Jul 2012 00:00:00 +0200</pubDate>
      <author>you@example.com (Your Name)</author>
      <guid>http://example.com/publication/2012/07/08/some-of-my-publications-are-online</guid>
      <description>&lt;p&gt;I&amp;#8217;m slowly uploading a couple of references and the pdf draft versions of them. Please find some of my &lt;a href='http://kaikunze.de/publications.html'&gt;publications&lt;/a&gt; in the corresponding section of this website.&lt;/p&gt;

&lt;p&gt;Stay tuned for the bibtex description and some more papers.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Compensating for On-body Placement Effects in Activity Recognition</title>
      <link>http://example.com/publication/2012/07/07/phd-thesis-sources-on-github</link>
      <pubDate>Sat, 07 Jul 2012 00:00:00 +0200</pubDate>
      <author>you@example.com (Your Name)</author>
      <guid>http://example.com/publication/2012/07/07/phd-thesis-sources-on-github</guid>
      <description>&lt;p&gt;Finished my phD. last year in Passau. The thesis is already published over &lt;a href='http://www.opus-bayern.de/uni-passau/volltexte/2012/2611/'&gt;Opus Bayern&lt;/a&gt;. The pdf is open access, so feel free to read it (careful 19 MB pdf): &lt;a href='http://www.opus-bayern.de/uni-passau/volltexte/2012/2611/pdf/kunze_kai.pdf'&gt;Compensating for On-Body Placement Effects in Activity Recognition as pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;However, the sources were not available. Finally, I got around to push the &lt;a href='http://github.com/kkai/phdthesis'&gt;latex sources of my dissertation&lt;/a&gt; up to github.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Please feel free to use it as a thesis template, attribution would be apprecitated ;)&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Please share if you make improvements, there are a lot of hacks and quick fixes in the sources.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;I&amp;#8217;ll try to share most of the algorithms discussed in my dissertation.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Here&amp;#8217;s a quick summary about the content:&lt;/p&gt;

&lt;p&gt;This thesis investigates, how placement variations of electronic devices influence the possibility of using sensors integrated in those devices for context recognition. The vast majority of context recognition research assumes well defined, fixed sen- sor locations. Although this might be acceptable for some application domains (e.g. in an industrial setting), users, in general, will have a hard time coping with these limitations. If one needs to remember to carry dedicated sensors and to adjust their orientation from time to time, the activity recognition system is more distracting than helpful. How can we deal with device location and orientation changes to make context sensing mainstream? This thesis presents a systematic evaluation of device placement effects in context recognition.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Using device motion in html/javascript</title>
      <link>http://example.com/hacking/2012/06/18/using-device-motion-from-a-mobile-device-in-htmljavascript</link>
      <pubDate>Mon, 18 Jun 2012 00:00:00 +0200</pubDate>
      <author>you@example.com (Your Name)</author>
      <guid>http://example.com/hacking/2012/06/18/using-device-motion-from-a-mobile-device-in-htmljavascript</guid>
      <description>&lt;p&gt;A while ago, I built a simple demonstration on how to stream accelerometer data from a mobile device over websockets to a server just using html and javascript. It consists of a nodejs web server and a processing.org visualization. As soon as a mobile browser connects to the server a new red cube is shown on the screen (placed between randomly generated cubes). The transparent area around the cube changes depending on how strong one shakes the phone.&lt;/p&gt;
&lt;iframe frameborder='0' height='281' src='http://player.vimeo.com/video/45626605' width='500'&gt;
&lt;/iframe&gt;&lt;p&gt;&lt;a href='http://vimeo.com/45626605'&gt;Visualization based on mobile phone data&lt;/a&gt; from &lt;a href='http://vimeo.com/user8093378'&gt;Kai Kunze&lt;/a&gt; on &lt;a href='http://vimeo.com'&gt;Vimeo&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can get the code from my &lt;a href='https://github.com/kkai/devicemotion-demo'&gt;github page&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It&amp;#8217;s based on these tutorials and sample code:&lt;/p&gt;

&lt;p&gt;&lt;a href='http://martinsikora.com/nodejs-and-websocket-simple-chat-tutorial'&gt;a simple chat server node.js tutorial&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href='http://www.paulrhayes.com/2009-07/animated-css3-cube-interface-using-3d-transforms/'&gt;3d css cube&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href='http://openprocessing.org/sketch/19216'&gt;3d cube world&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    

  </channel> 
</rss>
