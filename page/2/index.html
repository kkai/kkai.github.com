<!DOCTYPE html>
<html lang="en" itemscope itemtype="http://schema.org/WebPage">
  <head>
    

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">

  <title>Kai Kunze - Kai Kunze</title>
  <meta name="author" content="Kai Kunze"/><script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "WebSite",
    "name": "Kai Kunze",
    
    "url": "https:\/\/kaikunze.de\/"
}
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Organization",
  "name": "",
  "url": "https:\/\/kaikunze.de\/"
  
  
  
  
}
</script>

<meta property="og:title" content="Kai Kunze" />
<meta property="og:image" content="https://kaikunze.de/img/avatar-icon-1.png" />
<meta property="og:url" content="https://kaikunze.de/" />
<meta property="og:type" content="website" />
<meta property="og:site_name" content="Kai Kunze" />

  <meta name="twitter:title" content="Kai Kunze" />
  <meta name="twitter:image" content="https://kaikunze.de/img/avatar-icon-1.png" />
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:site" content="@_kai_ku" />
  <meta name="twitter:creator" content="@_kai_ku" />
  <link href='https://kaikunze.de/img/favicon.ico' rel='icon' type='image/x-icon'/>
  <meta name="generator" content="Hugo 0.81.0" />
  <link rel="alternate" href="https://kaikunze.de/index.xml" type="application/rss+xml" title="Kai Kunze"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css" integrity="sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.5.0/css/all.css" integrity="sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU" crossorigin="anonymous">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous"><link rel="stylesheet" href="https://kaikunze.de/css/main.css" /><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" />
  <link rel="stylesheet" href="https://kaikunze.de/css/highlight.min.css" /><link rel="stylesheet" href="https://kaikunze.de/css/codeblock.css" /><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.css" integrity="sha384-h/L2W9KefUClHWaty3SLE5F/qvc4djlyR4qY3NUV5HGQBBW7stbcfff1+I/vmsHh" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/default-skin/default-skin.min.css" integrity="sha384-iD0dNku6PYSIQLyfTOpB06F2KCZJAKLOThS5HRe8b3ibhdEQ6eKsFf/EeFxdOt5R" crossorigin="anonymous">



  </head>
  <body>
    <nav class="navbar navbar-default navbar-fixed-top navbar-custom">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#main-navbar">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="https://kaikunze.de/">Kai Kunze</a>
    </div>

    <div class="collapse navbar-collapse" id="main-navbar">
      <ul class="nav navbar-nav navbar-right">
        
          
            <li>
              <a title="writing" href="/">writing</a>
            </li>
          
        
          
            <li>
              <a title="projects" href="/projects/">projects</a>
            </li>
          
        
          
            <li>
              <a title="papers" href="/publications/">papers</a>
            </li>
          
        
          
            <li>
              <a title="about" href="/about/">about</a>
            </li>
          
        

        

        
      </ul>
    </div>

    
      <div class="avatar-container">
        <div class="avatar-img-border">
          <a title="Kai Kunze" href="https://kaikunze.de/">
            <img class="avatar-img" src="https://kaikunze.de/img/avatar-icon-1.png" alt="Kai Kunze" />
          </a>
        </div>
      </div>
    

  </div>
</nav>




    


<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>


  
  
  






<div id = "boids"> </div>
<div id="splash" >
  <iframe id="splash_iframe" src="-splash/boids/boids.html" scrolling="no"></iframe>
    <div id="splash_title"> <h1> Kai Kunze </h1> </div>
    <div id="splash_subtitle"> is Augmenting Humans and loves Science, Hacking, Playing with Technology. </div>
    <div id="splash_role"> Professor at Keio University, Yokohama, Japan.</div>
</div>



    
  <div role="main" class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        

        <div class="posts-list">
          
          
            <article class="post-preview">
    <a href="https://kaikunze.de/2015/08/18/-affective-wear/">
        <h2 class="post-title">Affective Wear- Recognizing facial expressions</h2>
        
        
        
    </a>

    <p class="post-meta">
        <span class="post-meta">
  
  
  <i class="fas fa-calendar"></i>&nbsp;
  
  
    &nbsp;|&nbsp;<i class="fas fa-clock"></i>&nbsp;2&nbsp;minutes
  
  
    &nbsp;|&nbsp;<i class="fas fa-book"></i>&nbsp;423&nbsp;words
  
  
  
</span>


    </p>
    <div class="post-entry">
        
        <p>Katsutoshi Masai, one of my Master students had the idea to track facial expressions using low cost sensors in glasses. Quite nice work ;)</p>
        <a href="https://kaikunze.de/2015/08/18/-affective-wear/" class="post-read-more">[Read More]</a>
        
    </div>

    

</article>
          
            <article class="post-preview">
    <a href="https://kaikunze.de/2015/02/18/my-first-blind-soccer-training/">
        <h2 class="post-title">Super Human Sports: Augmenting Blind Soccer</h2>
        
        
        
    </a>

    <p class="post-meta">
        <span class="post-meta">
  
  
  <i class="fas fa-calendar"></i>&nbsp;
  
  
    &nbsp;|&nbsp;<i class="fas fa-clock"></i>&nbsp;2&nbsp;minutes
  
  
    &nbsp;|&nbsp;<i class="fas fa-book"></i>&nbsp;369&nbsp;words
  
  
  
</span>


    </p>
    <div class="post-entry">
        
        <p>I&rsquo;m getting more and more fascinated by augmenting Blind Soccer.
After 3 blind soccer trainings, we had now a couple
of meetings to discuss how to extend and enhance the play experience.</p>
<p><img src="/imgs/stretch.jpg" alt="stretch">
<img src="/imgs/kick.jpg" alt="kick"></p>
<p>For me there are 3 interesting points about blind
soccer:</p>
<ol>
<li>It&rsquo;s very hard to learn. Can we make it easier for blind people to learn it?
If you can play it, it&rsquo;s very fast and empowering. We train with a soccer player from the Japanese national team.  He plays better than me without being blind folded (ok &hellip; that&rsquo;s maybe not really an achievement, I&rsquo;m terrible at soccer).</li>
<li>Can we make it easier for seeing people learn blind soccer and in turn understand more about the blind and improve their hearing skills?</li>
<li>Can we level the playing field making it possible for blind and seeing to play together using tech?</li>
</ol>
<p>However, the most interesting point, I think blind soccer can teach us that &ldquo;disability&rdquo; is a question of definition and the environment.</p>
<p>If you can play it, it&rsquo;s very fast and empowering. We train with a soccer player from the Japanese national team.  He plays better than me without being blind folded (ok &hellip; that&rsquo;s maybe not really an achievement, I&rsquo;m terrible at soccer).</p>
<p>The biggest take-away for me, I rely too much on vision to make sense of my environment. The training made me more aware of sounds. I find myself to listen more and more. Sometimes in a train on the street etc., I now close my eyes and explore the environment just by sound. It&rsquo;s fascinating how much we can hear. This opened a new world for me. Looking into it more, I believe sound is an underestimated modality for augmented and virtual realities, which is worth exploring more.
I stumbled over a couple of papers about sonic interface design. Looking forward to applying some of the findings we get out of the blind soccer use case to our lives ;)</p>
<p>If I have some more time, I&rsquo;ll write a bit more about the training and the ideathlons we did so far. In the mean time, I recommend you try it sometime (if you are near Tokyo, you can maybe also join our sessions).</p>

        
    </div>

    

</article>
          
            <article class="post-preview">
    <a href="https://kaikunze.de/2015/02/09/31c3-talk-slides-eye-wear-computing/">
        <h2 class="post-title">31C3 Talk Slides: Eye Wear Computing</h2>
        
        
        
    </a>

    <p class="post-meta">
        <span class="post-meta">
  
  
  <i class="fas fa-calendar"></i>&nbsp;
  
  
    &nbsp;|&nbsp;<i class="fas fa-clock"></i>&nbsp;1&nbsp;minutes
  
  
    &nbsp;|&nbsp;<i class="fas fa-book"></i>&nbsp;127&nbsp;words
  
  
  
</span>


    </p>
    <div class="post-entry">
        
        <!-- raw HTML omitted -->
<p>I got tremendous, positive feedback. Thanks a lot!
Even Heise had a news post about it.
<a href="http://www.heise.de/newsticker/meldung/31C3-Mit-smarten-Brillen-das-Gehirn-ausforschen-2507482.html">http://www.heise.de/newsticker/meldung/31C3-Mit-smarten-Brillen-das-Gehirn-ausforschen-2507482.html</a>
(Although I cannot and don&rsquo;t want to read your thoughts, as the article implies ;-) ).</p>
<p>Video on Youtube:</p>
<!-- raw HTML omitted -->
<p>Slides on Speakerdeck:</p>
<!-- raw HTML omitted -->
<p>I got mixed some feedback on twitter. Some people
mentioned that I&rsquo;m working on spy wear helping
the surveillance state&hellip; I believe that
the research I do is necessary to be out in the
open for the society to discuss the merits
and problems. I want to make sure that we can maximise the benefit for the individual for the tech I develop and minimize abuse from military, companies and governments. Please contact me
if you want to discuss privacy issues or found concrete problems with my work.</p>

        
    </div>

    

</article>
          
            <article class="post-preview">
    <a href="https://kaikunze.de/2015/01/17/31c3-aftermath/">
        <h2 class="post-title">31C3 Aftermath</h2>
        
        
        
    </a>

    <p class="post-meta">
        <span class="post-meta">
  
  
  <i class="fas fa-calendar"></i>&nbsp;
  
  
    &nbsp;|&nbsp;<i class="fas fa-clock"></i>&nbsp;1&nbsp;minutes
  
  
    &nbsp;|&nbsp;<i class="fas fa-book"></i>&nbsp;164&nbsp;words
  
  
  
</span>


    </p>
    <div class="post-entry">
        
        <!-- raw HTML omitted -->
<p>Here&rsquo;s my talk selection in random order (I definitely forgot some as I haven&rsquo;t watched all):</p>
<p><!-- raw HTML omitted --> The Machine to be Another<!-- raw HTML omitted -->
great research and talk. Mesmerizing, I&rsquo;m thinking about how to use this effect for my work ;)</p>
<p><!-- raw HTML omitted -->From Computation to Consciousness<!-- raw HTML omitted -->.
Nice &ldquo;Philosopy&rdquo; talk by Joscha.</p>
<p><!-- raw HTML omitted -->Rocket Science<!-- raw HTML omitted -->
David Madlener gives a nice, entertaining
intro in why it&rsquo;s important to go to space and
how to build rockets.</p>
<p><!-- raw HTML omitted -->Why are computers so @#!*, and what can we do about it?<!-- raw HTML omitted -->.</p>
<p><!-- raw HTML omitted --> Traue keinem Scan, den du nicht selbst gefälscht hast
<!-- raw HTML omitted --> In German, yet I think there&rsquo;s a translation.
Extremely funny, hope the translation captures it.</p>
<p>Don&rsquo;t watch the keynote. I wonder who picked
the speaker &hellip; terrible.</p>
<p>This year, I spent a substantial amount
with at the <!-- raw HTML omitted --> Food Hacking Base<!-- raw HTML omitted -->. Interesting talks.
I&rsquo;m thinking more and more to do some research
in this direction. Especially after I listened
to the new Resonator Podcast about
the <!-- raw HTML omitted -->connection between our gut and our brain<!-- raw HTML omitted -->
(recording is unfortunately in German).</p>

        
    </div>

    

</article>
          
            <article class="post-preview">
    <a href="https://kaikunze.de/2014/11/30/eyewear-computing/">
        <h2 class="post-title">Eye-Wear Computing</h2>
        
        
        
    </a>

    <p class="post-meta">
        <span class="post-meta">
  
  
  <i class="fas fa-calendar"></i>&nbsp;
  
  
    &nbsp;|&nbsp;<i class="fas fa-clock"></i>&nbsp;2&nbsp;minutes
  
  
    &nbsp;|&nbsp;<i class="fas fa-book"></i>&nbsp;255&nbsp;words
  
  
  
</span>


    </p>
    <div class="post-entry">
        
        <!-- raw HTML omitted -->
<p>You might have seen the <!-- raw HTML omitted --> J!NS academic videos <!-- raw HTML omitted --> by now,
I added embedded versions to the end of the post.</p>
<p>Bellow is the full video of the sneak peek of our work in the J!NS promotion. Special thanks to <!-- raw HTML omitted --> Shoya Ishimaru <!-- raw HTML omitted --> and <!-- raw HTML omitted -->Katsuma Tanaka <!-- raw HTML omitted -->, two talented students
Koichi Kise Sensei (Osaka Prefecture University)
and I are co-supervising. Check out their other (private) work if you are into programming for smart phones, Mac, iOS and Google Glass ;).
The video is a summary  of research work mostly done by Shoya.</p>
<!-- raw HTML omitted -->
<p>In the video we show applications using an early prototype of J!NS MEME, smart glasses with integrated electrodes to detect eye movements (Electrooculography, EOG) and motion sensors (accelerometer and gyroscope) to monitor head motions. We show several demonstrations: a simple eye movement visualization, detecting left/right eye motion and blink. Additionally, users can play a game, &ldquo;Blinky Bird&rdquo;. They need to help a bird avoid obstacles using eye movements. Using a combination of blink, eye movement and head motion
we can also detect reading and talking behavior. We can give people a long term view of their reading, talking, and also walking activity over the day.</p>
<p>Publications still pending, so I cannot
talk about the features, algorithms used etc.
In the mean time, here is a demo we gave at UbiComp this year.</p>
<!-- raw HTML omitted -->
<p>J!NS Academic Video:</p>
<!-- raw HTML omitted -->
<p>Oh and if you haven&rsquo;t had enough:
Here&rsquo;s an extended Interview with Inami Sensei and me.
Me wearing the optical camouflage for the first time at 0:04 :D (very short).</p>
<!-- raw HTML omitted -->

        
    </div>

    

</article>
          
            <article class="post-preview">
    <a href="https://kaikunze.de/2014/08/24/google-glass-for-eldery/">
        <h2 class="post-title">Google Glass for Older Adults</h2>
        
        
        
    </a>

    <p class="post-meta">
        <span class="post-meta">
  
  
  <i class="fas fa-calendar"></i>&nbsp;
  
  
    &nbsp;|&nbsp;<i class="fas fa-clock"></i>&nbsp;6&nbsp;minutes
  
  
    &nbsp;|&nbsp;<i class="fas fa-book"></i>&nbsp;1276&nbsp;words
  
  
  
</span>


    </p>
    <div class="post-entry">
        
        <!-- raw HTML omitted -->
<p>My grandparents conducted a longer Google Glass usability study for me.
I&rsquo;m happy they agreed that I can use their images and
insights to share here.</p>
<p><img src="/imgs/work.jpg" alt="oma">
<img src="/imgs/shopping_g.jpeg" alt="opa"></p>
<p>##Evaluation of the Current Google Glass</p>
<p>My grandparents mentioned that the current functionality of the device is quite limited. This might be due to the English only menu and bad to no Internet connectivity during usage. The experimental setup seems unobtrusive, as both of them are used to wear glasses, they got easily accustomed to carry Glass. All confirmed that the head-mounted display was not hindering them performing everyday tasks. Only
my grandmother mentioned discomfort as the device got unusual hot after a longer usage session of recording video and displaying directions.
<!-- raw HTML omitted -->
During the simple reading test, they could read text of font sizes of 40px and higher on the 640 x 360 screen if a white font on a black background was used (best contrast). However, other font colors were problematic and needed larger sizes to be legible. Especially the light grey font color
sed by some standard Glass applications  was hard to read. For example, they could not read the night temperature for the weather card.
Participants were able to use most speech commands and the &ldquo;head wake&rdquo; feature (tilting the head back to activate Glass) without problems. Only the &ldquo;google search&rdquo; speech command was not recognized, probably due to German accent ;)</p>
<p><img src="/imgs/wrong_g.png" alt="oma">
<img src="/imgs/dessert_g.png" alt="opa"></p>
<p>While both were able to activate the photo functions easily, they had difficulties to take the picture they intended. Glass seems to be optimized for taking photos in a slight distance (e.g. taking tourist pictures). However, my grandparents mostly wanted to take pictures to remember what they were doing (e.g. what things they had in the hand). It took a while for them to realize that the camera won’t make a photo of what they see (depicted in the both pictures above).</p>
<p>It was difficult for the two to use the touch panel for navigating Glass. The activation tab and scrolling usually works, yet the cancel gesture (”swiping down”) is more problematic and often not recognized. We assume this is due to fingers getting dry when getting older.</p>
<p>The active card display of Glass – swiping right to see cards relevant right now (e.g. weather, appointments), swiping left to see older events (e.g. taken pictures) – was intuitive. Yet, they had difficulties to use some of the hierarchical menu structures (e.g. for settings and doing a video call). They mentioned that it is hard to realize in which context they are currently operating, as Glass gives no indication of the hierarchy level.</p>
<p>###Usage Patterns
Already during the two days of system use and the shopping tour a number of usage patterns emerged. They used the camera feature the most. A common use case was memory augmentation. Making pictures of things they don&rsquo;t want to forget. For example, taking a picture of medication, so they can remember that it was already taken or taking pictures of interesting items while shopping.</p>
<p>Both preferred the &ldquo;hands free&rdquo; operations using the speech interface (although it was in English) compared to the touch interface during house work. Yet, being in town, they switched to the touch interface.
During cooking and house work, my grandmother appreciated the timer provided by Glass. However, it was difficult for them to set the timer using the current interface, as this involves a hierarchical menu. It was not clear if they currently change the hours, minutes or seconds while in the respective sub menu. Wearing the timer on the body is highly appreciated, as it&rsquo;s not possible to miss the timer going off.</p>
<p>For gardening and cooking, they would like to do video chats, for example to ask friends about tips and get their advice. Unfortunately, the limited internet reception at participants’ place did not allow video chats during the test phase.</p>
<p>###Requirements</p>
<p>Through the initial assessment of the system&rsquo;s functionalities and through the observation we found a number of requirements.
The focus on readability is even more crucial for older adults. Although Glass was already designed with this in mind, it seems font size is not the only thing that matters. Contrast seems equally important, as participants found it very difficult to read the light grey fonts used in some of the screens provided by Glass.</p>
<p>My grandparents request intuitive, hands-free interactions. They appreciated the speech interface if they are not in public or the &ldquo;blink to take a picture&rdquo; feature as they don&rsquo;t need to interact with the touch panel. They also did not want to make the device dirty, especially during cooking and gardening.
The touch interface was sometimes difficult to use for them. A potential reason is that they are not used to capacitive touch devices such as current smartphones. Scrolling worked, yet the cancel gesture (swiping down) was difficult to perform. It needed 2 or 3 tries every time they wanted to use it.</p>
<p>##Application Ideas
<img src="/imgs/garden_g.jpeg" alt="work"></p>
<p><strong>Short Term Memory Augmentation</strong> – As we described above, participants frequently took pictures to use them as reminder (e.g. taking medication). Using the time card interface of Glass, it was already easy to check if they performed the ac- tion or task in question by browsing through the taken pic- tures. Each picture has also a timestamp with it (see Figure 2). However, this only works for the last couple of days, as other- wise the user needs to scroll too far back. The most requested feature was Zoom for images. While shopping, users took pictures to remember interesting items. Prices from items can be recorded, yet as the device does not support zoom for pic- tures, it’s impossible to read the price on the head mounted display (see Figure 5).</p>
<p><strong>Long Term Capture and Access</strong>– The participants saw po- tential in having a long term capture and access interface. Checking how and what they worked on/did a couple months or even years back. Search on specific activities (e.g. bak- ing a apple cake) should be possible for access. Participants thought other types of indexing (e.g. location or time) would be not so useful.</p>
<p><strong>Timer and Reminders</strong> – Although the interface was not op- timal for them the users already found the timer application useful. The raised the need for several simultaneous timers and reminders. Right now the installed timer application just supports a single task.</p>
<p><strong>Instructions</strong> – For the gardening, cooking and workshop scenario, my grandparents would like to get instructions (e.g. ingredient lists, work steps) for more complex tasks they do not perform often. They prefer the Glass display to paper or instruction manuals, as they don’t need a context switch, clean their hands, stop what they do. Yet, they emphasized that the instructions need to be easily browsable. They would prefer the ”right” information at the ”right” time. Participant 2: ”Can’t Glass infer that I’m backing a cake right now and show me the ingredients I need for it?”</p>
<p>##Summary</p>
<p>These application scenarios are very similar to applications discussed for maintenance and, in general, industrial domains. Yet, as seen from the requirements usability and interface need to be adjusted significantly before wearable computing can be used by older adults without help.</p>
<p>##Finally, see you @ UbiComp</p>
<p>This is a serious of articles about our UbiComp Submissions
if you want to read more, check out the Poster paper:
<a href="/papers/kunze2014wearable.pdf"><em>Wearable Computing for Older Adults -Initial Insights into Head-Mounted Display Usage</em></a>. Kunze, Kai and Henze, Niels and Kise, Koichi. Proceedings of UbiComp'14 Adjunct. 2014. <a href="papers/bib/kunze2014wearable.bib">Bibtex</a>.</p>
<p>If you are attending UbiComp/ISWC this year please drop by at
our poster. Oh and if you found the write-up useful, please cite us
or share the article :)</p>

        
    </div>

    

</article>
          
            <article class="post-preview">
    <a href="https://kaikunze.de/2014/08/10/looking-forward-to-iswcubicomp-2014/">
        <h2 class="post-title">Looking forward to ISWC/Ubicomp 2014</h2>
        
        
        
    </a>

    <p class="post-meta">
        <span class="post-meta">
  
  
  <i class="fas fa-calendar"></i>&nbsp;
  
  
    &nbsp;|&nbsp;<i class="fas fa-clock"></i>&nbsp;1&nbsp;minutes
  
  
    &nbsp;|&nbsp;<i class="fas fa-book"></i>&nbsp;200&nbsp;words
  
  
  
</span>


    </p>
    <div class="post-entry">
        
        <!-- raw HTML omitted -->
<p>I hope to see you in Seattle. This year we have
again a couple papers outline work from our students.
Attached is a list with draft versions of the papers.
I will write a bit about each topic in the next coming
weeks.</p>
<p>Oh and if you attend please think about stopping
by our <a href="http://recall-fet.eu/wahm2014/">Workshop on Ubiquitous Technologies for Augmenting the Human Mind</a>.</p>
<p><a href="/papers/okoso2014implicit.pdf"><em>Implicit Gaze based Annotations to Support Second Language Learning</em></a>. Okoso, Ayano and Kunze, Kai and Kise, Koichi. Proceedings of UbiComp'14 Adjunct. 2014. <a href="papers/bib/okoso2014implicit.bib">Bibtex</a>.</p>
<p><a href="/papers/kunze2014wearable.pdf"><em>Wearable Computing for Older Adults -Initial Insights into Head-Mounted Display Usage</em></a>. Kunze, Kai and Henze, Niels and Kise, Koichi. Proceedings of UbiComp'14 Adjunct. 2014. <a href="papers/bib/kunze2014wearable.bib">Bibtex</a>.</p>
<p><a href="/papers/tanaka2014memory.pdf"><em>Memory Specs-An Annotation System on Google Glass using Document Image Retrieval</em></a>. Tanaka, Katsuma and Kunze, Kai and Iwata, Motoi and Kise, Koichi. Proceedings of UbiComp'14 Adjunct. 2014. <a href="papers/bib/tanaka2014memory.bib">Bibtex</a>.</p>
<p><a href="/papers/ishimaru2014smarter.pdf"><em>Smarter Eyewear- Using Commercial EOG Glasses for Activity Recognition</em></a>. Ishimaru, Shoya and Kunze, Kai and Tanaka, Katsuma and Uema, Uji and Kise, Koichi and Inami, Masahiko. Proceedings of UbiComp'14 Adjunct. 2014. <a href="papers/bib/ishimaru2014smarter.bib">Bibtex</a>.</p>
<hr>
<p><a href="/papers/ishimaru2014brain.pdf"><em>Position Paper: Brain Teasers - Toward Wearable Computing that Engages Our Mind</em></a>. Ishimaru, Shoya and Kunze, Kai and Kise, Koichi and Inami, Masahiko. Proceedings of UbiComp'14 Adjunct. 2014. <a href="papers/bib/ishimaru2014brain.bib">Bibtex</a>.</p>

        
    </div>

    
    <div class="blog-tags">
        
        <a href="https://kaikunze.de//tags/ubicomp/">ubicomp</a>&nbsp;
        
        <a href="https://kaikunze.de//tags/iswc/">iswc</a>&nbsp;
        
    </div>
    

</article>
          
            <article class="post-preview">
    <a href="https://kaikunze.de/2014/03/19/augmented-human-2014/">
        <h2 class="post-title">Augmented Human 2014</h2>
        
        
        
    </a>

    <p class="post-meta">
        <span class="post-meta">
  
  
  <i class="fas fa-calendar"></i>&nbsp;
  
  
    &nbsp;|&nbsp;<i class="fas fa-clock"></i>&nbsp;5&nbsp;minutes
  
  
    &nbsp;|&nbsp;<i class="fas fa-book"></i>&nbsp;931&nbsp;words
  
  
  
</span>


    </p>
    <div class="post-entry">
        
        <!-- raw HTML omitted -->
<p>Ok, I&rsquo;m &ldquo;a bit&rdquo; biased as I&rsquo;m one of the conference co-chairs. Still I enjoyed this years Augmented Human.
Below is the tag cloud from all abstracts, to give you a brief overview about the topics.</p>
<p><img src="/imgs/ah-cloud.png" alt="Tag Cloud"></p>
<p>Considering the small size of the conference, the quality of the work is exceptional. It&rsquo;s not one of the conferences that gets the rejected papers from CHI, Ubicomp, PerComp etc. The steering committee really set up a venue for far-out, novel ideas. Also it&rsquo;s a good opportunity to meet great researchers up close; last year for example Thad Starner, Albrecht Schmidt etc. this year, Jun Rekimoto, Masahiko Inami, Paul Lukowicz and especially <a href="http://www.cyberdyne.jp/english/">Yoshiyuki Sankai</a> &hellip;
pretty impressive if you ask me. They might be around at other bigger events, yet try to catch them and talk to them, impossible. At AH, it&rsquo;s very easy. I recommend any young researcher interested in the research topics to attend next year&rsquo;s AH. Surely, I will try to get some papers accepted ;)</p>
<p>I believe we will see a lot of the work presented at AH2014 at CHI or Ubicomp next year. Yet, decide for yourself.</p>
<p>In the following, I&rsquo;ll show you just a couple of highlights. I&rsquo;m sorry, I cannot mention all of the cool work (I realized by writing that the blog post got bigger and bigger and decided to stop at some point so I can finally publish it &hellip;).</p>
<p>#Sports#
As already the tag cloud suggested, augmenting sports was a hot topic at the conference.</p>
<p>So just in case you want to play a round of Quidditch or Shaolin Soccer in the real world, we might have the tech for it. Rekimoto research about &ldquo;Hover Ball&rdquo;.
This topic has also been picked up by the <a href="http://www.newscientist.com/article/mg22129614.900-dronepowered-hoverball-could-spice-up-games.html">New Scientist</a>.
I also recommend to check out some work by Takuya Nojima Sensei (TAMA and PhotoelasticBall).</p>
<p>The best paper award also went to a sports themed paper: &ldquo;Around Me: A System for Providing Sports Player&rsquo;s Self-images with an Escort Robot&rdquo;. Nice!</p>
<!-- raw HTML omitted -->
<p>#Around the Eye#
As you might know, I have a personal interest in <a href="http://kaikunze.de/posts/30c3-toward-a-cognitive-quantified-self/">Eyetracking and related research</a>, as I think it&rsquo;s a very promising direction (especially inferring types of information that you otherwise cannot easily get hold off). So I was very curious about related work presented at AH about the topic and was not disappointed.</p>
<p>I&rsquo;m wondering if I feel comfortable sharing my sad emotions, as suggested by Tearsense (Marina Mitani, Yasuaki Kakehi). Maybe in a dark cinema this is alright. As a part of life logging it might be also interesting. We had a couple of interesting discussions also during the social about the technology.</p>
<!-- raw HTML omitted -->
<p>Asako Hosobori and Yasuaki Kakehi want to support face to face interaction with Eyefeel &amp; EyeChime.
Although the setup still seems a bit unnatural, I love the direction of the research using technology to enrich our social life and make us focus more on things that are important (away from looking at smartphone screens). Yet, judge yourself.</p>
<!-- raw HTML omitted -->
<p>#Haptics#</p>
<p>The most far out work regarding output devices was
definitely &ldquo;A Haptic Foot Interface for Language Communication&rdquo; by Erik Hill et. al. They use vibration
motors to convey text messages on your foot. Made me wonder why we don&rsquo;t use our feet more for HCI, regarding how sensitive our feet are and that a large part of our brain is also dedicated to sensing on the foot.</p>
<p>Max Peiffer et. al. showed how to make free-hand interactions (e.g. with a kinect or similar body tracking system) more realistic using haptic feedback. Nice work!</p>
<!-- raw HTML omitted -->
<p>The half implant device on a fingernail by Emi Tamaki and Ken Iwasaki was also nice; especially considering that it&rsquo;s already (or soon) a commercial product.</p>
<!-- raw HTML omitted -->
<p>As always, I particularly liked Inami-Sensei&rsquo;s work.
Suzanne Low presented &ldquo;Pressure Detection on Mobile Phone By Camera and Flash&rdquo;. Very innovative use of the camera and nice demonstrations.</p>
<!-- raw HTML omitted -->
<p>#Our work#
We had 3 papers and 1 poster at the conference.</p>
<hr>
<p><a href="/papers/cheng2014tip.pdf"><em>On the Tip of my Tongue - A Non-Invasive Pressure-Based Tongue Interface</em></a>. Cheng, Jingyuan and Okoso, Ayano and Kunze, Kai and Henze, Niels and Schmidt, Albrecht and Lukowicz, Paul and Kise. Proceedings of the 5th Augmented Human International Conference. 2014. <a href="papers/bib/cheng2014tip.bib">Bibtex</a>.</p>
<hr>
<p><a href="/papers/ishimaru2014blink.pdf"><em>In the Blink of an Eye - Combining Head Motion and Eye Blink Frequency for Activity Recognition with Google Glass</em></a>. Ishimaru, Shoya and Kunze, Kai and Kise, Koichi and Weppner, Jens and Dengel, Andreas and Lukowicz, Paul and Bulling, Andreas. Proceedings of the 5th Augmented Human International Conference. 2014. <a href="papers/bib/ishimaru2014blink.bib">Bibtex</a>.</p>
<hr>
<p><a href="/papers/shirazi2014what.pdf"><em>What&rsquo;s on your mind? Mental Task Awareness Using Single Electrode Brain Computer Interfaces</em></a>. Shirazi, Alireza Sahami and Hassib, Mariam and Henze, Niels and Schmidt, Albrecht and Kunze, Kai. Proceedings of the 5th Augmented Human International Conference. 2014. <a href="papers/bib/shirazi2014what.bib">Bibtex</a>.</p>
<hr>
<p><a href="/papers/iwamura2014havent.pdf"><em>Haven&rsquo;t we met before? - A Realistic Memory Assistance System to Remind You of The Person in Front of You</em></a>. Iwamura, Masakazu and Kunze, Kai and Kato, Yuya and Utsumi, Yuzuko and Kise, Koichi. Proceedings of the 5th Augmented Human International Conference. 2014. <a href="papers/bib/iwamura2014havent.bib">Bibtex</a>.</p>
<p>I&rsquo;m particularly proud of Okoso&rsquo;s and Shoya&rsquo;s work.
They are both still Bachelor students and their
research is already published in an international conference.</p>
<p>As Shoya was still visiting DFKI in Germany, he sadly
could not attend.
Okoso gave the Tongue Interface presentation and I was impressed by her. It&rsquo;s her first talk at a conference and she&rsquo;s a 3rd year bachelor. The English was perfect, the talk easy to understand and entertaining. Well done!</p>
<p>#Concluding#</p>
<p>The full program can be found at the <a href="http://cse.eedept.kobe-u.ac.jp/ah2014/program/">AH website</a> in
case you&rsquo;re looking for the references.
See you next year at <a href="http://www.augmented-human.com/augmented-human-international-conference-2015">AH in Singapore</a>.</p>

        
    </div>

    

</article>
          
            <article class="post-preview">
    <a href="https://kaikunze.de/2014/03/05/Attending-the-symposium-on-service-systems-science/">
        <h2 class="post-title">Beyond FuturICT</h2>
        
        
        
    </a>

    <p class="post-meta">
        <span class="post-meta">
  
  
  <i class="fas fa-calendar"></i>&nbsp;
  
  
    &nbsp;|&nbsp;<i class="fas fa-clock"></i>&nbsp;3&nbsp;minutes
  
  
    &nbsp;|&nbsp;<i class="fas fa-book"></i>&nbsp;588&nbsp;words
  
  
  
</span>


    </p>
    <div class="post-entry">
        
        <!-- raw HTML omitted -->
<p>Although the <a href="http://www.futurict.eu">FuturICT</a> project did not get funding from
the EU so far (I still believe this was a grave mistake),
I can see that the spirit and our ideas live on. The Japanese COI-T Program focuses on the same issues and problems as FuturICT.</p>
<p><img src="/imgs/pres.jpg" alt="FuturICT"></p>
<p><a href="http://www.soms.ethz.ch/people/dhelbing">Dirk Helbing</a>&rsquo;s presentation gave an overview of the FuturICT effort and the refined research agenda. I really enjoyed seeing how the material has matured.
Also the talks from Shunri Oda and Maso Fukuma addressed similar problems and presented similar conclusions.</p>
<p>In the afternoon, Cornelius Herstatt gave some interesting observations about the future potential of the Japanese Market. Especially, I liked his conclusions about the use of robots in society: Seeing robots not as replacements to workers but as complementary, allowing more independence and privacy for older adults.</p>
<p>#Take Home Message#
Not sure, if I got it right as social and system sciences are quite new to me, but what I took home from the
symposium:
System behavior is determined by connectivity patterns, coupling strength, interactions etc. and small changes in any of them can lead to dramatic changes in the overall system.
Technology today has fundamentally changed connectivity and coupling in human interactions. Yet, so far we develop technology as a standalone in an uncontrolled way. So far, we have little idea how it influences society.</p>
<p>The Internet made any piece of digital, archival knowledge instantly, globally available.
We are now at the verge of any real life event becoming, instantly globally connected to the digital domain.
With this &ldquo;Internet of Things&rdquo; we might have a perfect substrate and basis to explore these effects. A key word seems &ldquo;participatory social sensing&rdquo;.
<img src="/imgs/Tokyo.jpg" alt="FuturICT"></p>
<p>#Discussions and Plenary Summary#
The discussions centered around the big
picture of society and how to induce beneficial change (as well as to prevent negative effects).
Yet, most strikingly, the panel members also mentioned
some concrete ideas about change and addressed especially
the research community. There was a long discussion about how to accelerate research, in which Dirk Helbing again emphasized his concept of an &ldquo;Idea Github for researchers&rdquo;: a platform to share ideas /implementations and research results more freely with easy reproducibility and attribution to the corresponding inventors. The main premise is it should not take us 2-3 years to finally publish our findings.</p>
<p>I ran into this problem, also in the wearable computing field. It is hard for &ldquo;outsiders&rdquo; (researchers not in the community) to enter the field. If they just read papers and work on the published research, they can never work on bleeding edge research. You have to meet with people of the different labs and know what they are working on to find interesting topics (and more important they have to trust you &hellip;).</p>
<p>#Concluding#
I&rsquo;m very happy that the FuturICT lives on.
In general, I find the Japanese research community
is very open towards social computing, especially the idea of participatory social sensing. Maybe it&rsquo;s more fruitful to continue the project ideas first here in Japan. It&rsquo;s a bit sad that Europe has given away the chance of being a innovation leader in this field.
Yet meeting Dirk again and seeing how his ideas and research towards social computing matured and developed was also great. Europe might have missed a chance, yet there&rsquo;s still hope ;)</p>
<p>If you got interested in research about society and social science, I can recommed <a href="http://www.springer.com/physics/complexity/book/978-3-642-28999-6">&ldquo;Society is a Complex Matter&rdquo; by Philip Ball</a>. It&rsquo;s an engaging read for novices to the topic. I could get a copy of at the event.</p>

        
    </div>

    

</article>
          
            <article class="post-preview">
    <a href="https://kaikunze.de/2014/02/13/glass-talk-at-hacker-news-kansai/">
        <h2 class="post-title">Glass Talk at Hacker News Kansai</h2>
        
        
        
    </a>

    <p class="post-meta">
        <span class="post-meta">
  
  
  <i class="fas fa-calendar"></i>&nbsp;
  
  
    &nbsp;|&nbsp;<i class="fas fa-clock"></i>&nbsp;1&nbsp;minutes
  
  
    &nbsp;|&nbsp;<i class="fas fa-book"></i>&nbsp;116&nbsp;words
  
  
  
</span>


    </p>
    <div class="post-entry">
        
        <!-- raw HTML omitted -->
<p><img src="/imgs/glass3.jpg" alt="Glass 1">
<img src="/imgs/glass4.jpg" alt="Glass 2"></p>
<p>I&rsquo;m sorry it took so long to put online.
Too much to do, especially due to Augmented Human 2014 (really looking forward to the beginning of march) :)</p>
<p>As part of our monthly <a href="http://hnkansai.org">Hacker News meetup</a>in Kansai,
I gave a small introduction on how to use (and hack for) Google Glass.</p>
<!-- raw HTML omitted -->
<p>The slides are also available as <a href="../../slides/kansai_15.pdf">pdf download</a> (7 MB).</p>
<p>The &ldquo;Hacks&rdquo; are fairly simple. I more or less show how to develop for Glass using the Native Development Kit and some demonstrations how to read out sensors or other demo code I scraped from github and adjusted (see the slides for links to the sources). The examples include CameraZoom, Face Detection and Blink Recognition.</p>
<p><img src="/imgs/glass0.jpg" alt="Glass 3">
<img src="/imgs/glass2.jpg" alt="Glass 4"></p>

        
    </div>

    

</article>
          
        </div>

        
          <ul class="pager main-pager">
            
              <li class="previous">
                <a href="https://kaikunze.de/page/1/">&larr; Newer Posts</a>
              </li>
            
            
              <li class="next">
                <a href="https://kaikunze.de/page/3/">Older Posts &rarr;</a>
              </li>
            
          </ul>
        
      </div>
    </div>
  </div>

      
    <div class="page-meta">
  
  
  
</div>


  
<footer>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <ul class="list-inline text-center footer-links">
          
              <li>
                <a href="mailto:kai.kunze@pm.me" title="Email me">
                  <span class="fa-stack fa-lg">
                    <i class="fas fa-circle fa-stack-2x"></i>
                    <i class="fas fa-envelope fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li>
                <a href="https://www.facebook.com/kai.kunze" title="Facebook">
                  <span class="fa-stack fa-lg">
                    <i class="fas fa-circle fa-stack-2x"></i>
                    <i class="fab fa-facebook fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li>
                <a href="https://github.com/kkai" title="GitHub">
                  <span class="fa-stack fa-lg">
                    <i class="fas fa-circle fa-stack-2x"></i>
                    <i class="fab fa-github fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li>
                <a href="https://twitter.com/_kai_ku" title="Twitter">
                  <span class="fa-stack fa-lg">
                    <i class="fas fa-circle fa-stack-2x"></i>
                    <i class="fab fa-twitter fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li>
                <a href="https://linkedin.com/in/kaikunze" title="LinkedIn">
                  <span class="fa-stack fa-lg">
                    <i class="fas fa-circle fa-stack-2x"></i>
                    <i class="fab fa-linkedin fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li>
                <a href="https://www.instagram.com/kaikunze" title="Instagram">
                  <span class="fa-stack fa-lg">
                    <i class="fas fa-circle fa-stack-2x"></i>
                    <i class="fab fa-instagram fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
          
          <li>
            <a href="/index.xml" title="RSS">
              <span class="fa-stack fa-lg">
                <i class="fas fa-circle fa-stack-2x"></i>
                <i class="fas fa-rss fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
        </ul>
        <p class="credits copyright text-muted">
          
            
              <a href="kaikunze.de">Kai Kunze</a>
            
          

          &nbsp;&bull;&nbsp;&copy;
          
            2020
          

          
            &nbsp;&bull;&nbsp;
            <a href="https://kaikunze.de/">Kai Kunze</a>
          
      </div>
    </div>
  </div>
</footer><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.js" integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/contrib/auto-render.min.js" integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe" crossorigin="anonymous"></script>
<script src="https://code.jquery.com/jquery-1.12.4.min.js" integrity="sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ=" crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

<script src="https://kaikunze.de/js/main.js"></script>
<script src="https://kaikunze.de/js/highlight.min.js"></script>
<script> hljs.initHighlightingOnLoad(); </script>
<script> $(document).ready(function() {$("pre.chroma").css("padding","0");}); </script><script> renderMathInElement(document.body); </script><script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.js" integrity="sha384-QELNnmcmU8IR9ZAykt67vGr9/rZJdHbiWi64V88fCPaOohUlHCqUD/unNN0BXSqy" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe-ui-default.min.js" integrity="sha384-m67o7SkQ1ALzKZIFh4CiTA8tmadaujiTa9Vu+nqPSwDOqHrDmxLezTdFln8077+q" crossorigin="anonymous"></script><script src="https://kaikunze.de/js/load-photoswipe.js"></script>









    
  </body>
</html>

