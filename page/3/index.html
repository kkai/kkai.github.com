<!DOCTYPE html>
<html lang="en" itemscope itemtype="http://schema.org/WebPage">
  <head>
    

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">

  <title>Kai Kunze - Kai Kunze</title>
  <meta name="author" content="Kai Kunze"/><script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "WebSite",
    "name": "Kai Kunze",
    
    "url": "https:\/\/kaikunze.de\/"
}
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Organization",
  "name": "",
  "url": "https:\/\/kaikunze.de\/"
  
  
  
  
}
</script>

<meta property="og:title" content="Kai Kunze" />
<meta property="og:image" content="https://kaikunze.de/img/avatar-icon-1.png" />
<meta property="og:url" content="https://kaikunze.de/" />
<meta property="og:type" content="website" />
<meta property="og:site_name" content="Kai Kunze" />

  <meta name="twitter:title" content="Kai Kunze" />
  <meta name="twitter:image" content="https://kaikunze.de/img/avatar-icon-1.png" />
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:site" content="@_kai_ku" />
  <meta name="twitter:creator" content="@_kai_ku" />
  <link href='https://kaikunze.de/img/favicon.ico' rel='icon' type='image/x-icon'/>
  <meta name="generator" content="Hugo 0.113.0">
  <link rel="alternate" href="https://kaikunze.de/index.xml" type="application/rss+xml" title="Kai Kunze"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css" integrity="sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.5.0/css/all.css" integrity="sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU" crossorigin="anonymous">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous"><link rel="stylesheet" href="https://kaikunze.de/css/main.css" /><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" />
  <link rel="stylesheet" href="https://kaikunze.de/css/highlight.min.css" /><link rel="stylesheet" href="https://kaikunze.de/css/codeblock.css" /><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.css" integrity="sha384-h/L2W9KefUClHWaty3SLE5F/qvc4djlyR4qY3NUV5HGQBBW7stbcfff1+I/vmsHh" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/default-skin/default-skin.min.css" integrity="sha384-iD0dNku6PYSIQLyfTOpB06F2KCZJAKLOThS5HRe8b3ibhdEQ6eKsFf/EeFxdOt5R" crossorigin="anonymous">



  </head>
  <body>
    <nav class="navbar navbar-default navbar-fixed-top navbar-custom">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#main-navbar">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="https://kaikunze.de/">Kai Kunze</a>
    </div>

    <div class="collapse navbar-collapse" id="main-navbar">
      <ul class="nav navbar-nav navbar-right">
        
          
            <li>
              <a title="writing" href="/">writing</a>
            </li>
          
        
          
            <li>
              <a title="projects" href="/projects/">projects</a>
            </li>
          
        
          
            <li>
              <a title="papers" href="/publications/">papers</a>
            </li>
          
        
          
            <li>
              <a title="about" href="/about/">about</a>
            </li>
          
        

        

        
      </ul>
    </div>

    
      <div class="avatar-container">
        <div class="avatar-img-border">
          <a title="Kai Kunze" href="https://kaikunze.de/">
            <img class="avatar-img" src="https://kaikunze.de/img/avatar-icon-1.png" alt="Kai Kunze" />
          </a>
        </div>
      </div>
    

  </div>
</nav>




    


<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>


  
  
  






<div id = "boids"> </div>
<div id="splash" >
  <iframe id="splash_iframe" src="-splash/boids/boids.html" scrolling="no"></iframe>
    <div id="splash_title"> <h1> Kai Kunze </h1> </div>
    <div id="splash_subtitle"> is Augmenting Humans and loves Science, Hacking, Playing with Technology. </div>
    <div id="splash_role"> Professor at Keio University, Yokohama, Japan.</div>
</div>



    
  <div role="main" class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        

        <div class="posts-list">
          
          
            <article class="post-preview">
    <a href="https://kaikunze.de/2014/03/19/augmented-human-2014/">
        <h2 class="post-title">Augmented Human 2014</h2>
        
        
        
    </a>

    <p class="post-meta">
        <span class="post-meta">
  
  
  <i class="fas fa-calendar"></i>&nbsp;
  
  
    &nbsp;|&nbsp;<i class="fas fa-clock"></i>&nbsp;5&nbsp;minutes
  
  
    &nbsp;|&nbsp;<i class="fas fa-book"></i>&nbsp;931&nbsp;words
  
  
  
</span>


    </p>
    <div class="post-entry">
        
        <!-- raw HTML omitted -->
<p>Ok, I&rsquo;m &ldquo;a bit&rdquo; biased as I&rsquo;m one of the conference co-chairs. Still I enjoyed this years Augmented Human.
Below is the tag cloud from all abstracts, to give you a brief overview about the topics.</p>
<p><img src="/imgs/ah-cloud.png" alt="Tag Cloud"></p>
<p>Considering the small size of the conference, the quality of the work is exceptional. It&rsquo;s not one of the conferences that gets the rejected papers from CHI, Ubicomp, PerComp etc. The steering committee really set up a venue for far-out, novel ideas. Also it&rsquo;s a good opportunity to meet great researchers up close; last year for example Thad Starner, Albrecht Schmidt etc. this year, Jun Rekimoto, Masahiko Inami, Paul Lukowicz and especially <a href="http://www.cyberdyne.jp/english/">Yoshiyuki Sankai</a> &hellip;
pretty impressive if you ask me. They might be around at other bigger events, yet try to catch them and talk to them, impossible. At AH, it&rsquo;s very easy. I recommend any young researcher interested in the research topics to attend next year&rsquo;s AH. Surely, I will try to get some papers accepted ;)</p>
<p>I believe we will see a lot of the work presented at AH2014 at CHI or Ubicomp next year. Yet, decide for yourself.</p>
<p>In the following, I&rsquo;ll show you just a couple of highlights. I&rsquo;m sorry, I cannot mention all of the cool work (I realized by writing that the blog post got bigger and bigger and decided to stop at some point so I can finally publish it &hellip;).</p>
<p>#Sports#
As already the tag cloud suggested, augmenting sports was a hot topic at the conference.</p>
<p>So just in case you want to play a round of Quidditch or Shaolin Soccer in the real world, we might have the tech for it. Rekimoto research about &ldquo;Hover Ball&rdquo;.
This topic has also been picked up by the <a href="http://www.newscientist.com/article/mg22129614.900-dronepowered-hoverball-could-spice-up-games.html">New Scientist</a>.
I also recommend to check out some work by Takuya Nojima Sensei (TAMA and PhotoelasticBall).</p>
<p>The best paper award also went to a sports themed paper: &ldquo;Around Me: A System for Providing Sports Player&rsquo;s Self-images with an Escort Robot&rdquo;. Nice!</p>
<!-- raw HTML omitted -->
<p>#Around the Eye#
As you might know, I have a personal interest in <a href="http://kaikunze.de/posts/30c3-toward-a-cognitive-quantified-self/">Eyetracking and related research</a>, as I think it&rsquo;s a very promising direction (especially inferring types of information that you otherwise cannot easily get hold off). So I was very curious about related work presented at AH about the topic and was not disappointed.</p>
<p>I&rsquo;m wondering if I feel comfortable sharing my sad emotions, as suggested by Tearsense (Marina Mitani, Yasuaki Kakehi). Maybe in a dark cinema this is alright. As a part of life logging it might be also interesting. We had a couple of interesting discussions also during the social about the technology.</p>
<!-- raw HTML omitted -->
<p>Asako Hosobori and Yasuaki Kakehi want to support face to face interaction with Eyefeel &amp; EyeChime.
Although the setup still seems a bit unnatural, I love the direction of the research using technology to enrich our social life and make us focus more on things that are important (away from looking at smartphone screens). Yet, judge yourself.</p>
<!-- raw HTML omitted -->
<p>#Haptics#</p>
<p>The most far out work regarding output devices was
definitely &ldquo;A Haptic Foot Interface for Language Communication&rdquo; by Erik Hill et. al. They use vibration
motors to convey text messages on your foot. Made me wonder why we don&rsquo;t use our feet more for HCI, regarding how sensitive our feet are and that a large part of our brain is also dedicated to sensing on the foot.</p>
<p>Max Peiffer et. al. showed how to make free-hand interactions (e.g. with a kinect or similar body tracking system) more realistic using haptic feedback. Nice work!</p>
<!-- raw HTML omitted -->
<p>The half implant device on a fingernail by Emi Tamaki and Ken Iwasaki was also nice; especially considering that it&rsquo;s already (or soon) a commercial product.</p>
<!-- raw HTML omitted -->
<p>As always, I particularly liked Inami-Sensei&rsquo;s work.
Suzanne Low presented &ldquo;Pressure Detection on Mobile Phone By Camera and Flash&rdquo;. Very innovative use of the camera and nice demonstrations.</p>
<!-- raw HTML omitted -->
<p>#Our work#
We had 3 papers and 1 poster at the conference.</p>
<hr>
<p><a href="/papers/cheng2014tip.pdf"><em>On the Tip of my Tongue - A Non-Invasive Pressure-Based Tongue Interface</em></a>. Cheng, Jingyuan and Okoso, Ayano and Kunze, Kai and Henze, Niels and Schmidt, Albrecht and Lukowicz, Paul and Kise. Proceedings of the 5th Augmented Human International Conference. 2014. <a href="papers/bib/cheng2014tip.bib">Bibtex</a>.</p>
<hr>
<p><a href="/papers/ishimaru2014blink.pdf"><em>In the Blink of an Eye - Combining Head Motion and Eye Blink Frequency for Activity Recognition with Google Glass</em></a>. Ishimaru, Shoya and Kunze, Kai and Kise, Koichi and Weppner, Jens and Dengel, Andreas and Lukowicz, Paul and Bulling, Andreas. Proceedings of the 5th Augmented Human International Conference. 2014. <a href="papers/bib/ishimaru2014blink.bib">Bibtex</a>.</p>
<hr>
<p><a href="/papers/shirazi2014what.pdf"><em>What&rsquo;s on your mind? Mental Task Awareness Using Single Electrode Brain Computer Interfaces</em></a>. Shirazi, Alireza Sahami and Hassib, Mariam and Henze, Niels and Schmidt, Albrecht and Kunze, Kai. Proceedings of the 5th Augmented Human International Conference. 2014. <a href="papers/bib/shirazi2014what.bib">Bibtex</a>.</p>
<hr>
<p><a href="/papers/iwamura2014havent.pdf"><em>Haven&rsquo;t we met before? - A Realistic Memory Assistance System to Remind You of The Person in Front of You</em></a>. Iwamura, Masakazu and Kunze, Kai and Kato, Yuya and Utsumi, Yuzuko and Kise, Koichi. Proceedings of the 5th Augmented Human International Conference. 2014. <a href="papers/bib/iwamura2014havent.bib">Bibtex</a>.</p>
<p>I&rsquo;m particularly proud of Okoso&rsquo;s and Shoya&rsquo;s work.
They are both still Bachelor students and their
research is already published in an international conference.</p>
<p>As Shoya was still visiting DFKI in Germany, he sadly
could not attend.
Okoso gave the Tongue Interface presentation and I was impressed by her. It&rsquo;s her first talk at a conference and she&rsquo;s a 3rd year bachelor. The English was perfect, the talk easy to understand and entertaining. Well done!</p>
<p>#Concluding#</p>
<p>The full program can be found at the <a href="http://cse.eedept.kobe-u.ac.jp/ah2014/program/">AH website</a> in
case you&rsquo;re looking for the references.
See you next year at <a href="http://www.augmented-human.com/augmented-human-international-conference-2015">AH in Singapore</a>.</p>

        
    </div>

    

</article>
          
            <article class="post-preview">
    <a href="https://kaikunze.de/2014/03/05/Attending-the-symposium-on-service-systems-science/">
        <h2 class="post-title">Beyond FuturICT</h2>
        
        
        
    </a>

    <p class="post-meta">
        <span class="post-meta">
  
  
  <i class="fas fa-calendar"></i>&nbsp;
  
  
    &nbsp;|&nbsp;<i class="fas fa-clock"></i>&nbsp;3&nbsp;minutes
  
  
    &nbsp;|&nbsp;<i class="fas fa-book"></i>&nbsp;588&nbsp;words
  
  
  
</span>


    </p>
    <div class="post-entry">
        
        <!-- raw HTML omitted -->
<p>Although the <a href="http://www.futurict.eu">FuturICT</a> project did not get funding from
the EU so far (I still believe this was a grave mistake),
I can see that the spirit and our ideas live on. The Japanese COI-T Program focuses on the same issues and problems as FuturICT.</p>
<p><img src="/imgs/pres.jpg" alt="FuturICT"></p>
<p><a href="http://www.soms.ethz.ch/people/dhelbing">Dirk Helbing</a>&rsquo;s presentation gave an overview of the FuturICT effort and the refined research agenda. I really enjoyed seeing how the material has matured.
Also the talks from Shunri Oda and Maso Fukuma addressed similar problems and presented similar conclusions.</p>
<p>In the afternoon, Cornelius Herstatt gave some interesting observations about the future potential of the Japanese Market. Especially, I liked his conclusions about the use of robots in society: Seeing robots not as replacements to workers but as complementary, allowing more independence and privacy for older adults.</p>
<p>#Take Home Message#
Not sure, if I got it right as social and system sciences are quite new to me, but what I took home from the
symposium:
System behavior is determined by connectivity patterns, coupling strength, interactions etc. and small changes in any of them can lead to dramatic changes in the overall system.
Technology today has fundamentally changed connectivity and coupling in human interactions. Yet, so far we develop technology as a standalone in an uncontrolled way. So far, we have little idea how it influences society.</p>
<p>The Internet made any piece of digital, archival knowledge instantly, globally available.
We are now at the verge of any real life event becoming, instantly globally connected to the digital domain.
With this &ldquo;Internet of Things&rdquo; we might have a perfect substrate and basis to explore these effects. A key word seems &ldquo;participatory social sensing&rdquo;.
<img src="/imgs/Tokyo.jpg" alt="FuturICT"></p>
<p>#Discussions and Plenary Summary#
The discussions centered around the big
picture of society and how to induce beneficial change (as well as to prevent negative effects).
Yet, most strikingly, the panel members also mentioned
some concrete ideas about change and addressed especially
the research community. There was a long discussion about how to accelerate research, in which Dirk Helbing again emphasized his concept of an &ldquo;Idea Github for researchers&rdquo;: a platform to share ideas /implementations and research results more freely with easy reproducibility and attribution to the corresponding inventors. The main premise is it should not take us 2-3 years to finally publish our findings.</p>
<p>I ran into this problem, also in the wearable computing field. It is hard for &ldquo;outsiders&rdquo; (researchers not in the community) to enter the field. If they just read papers and work on the published research, they can never work on bleeding edge research. You have to meet with people of the different labs and know what they are working on to find interesting topics (and more important they have to trust you &hellip;).</p>
<p>#Concluding#
I&rsquo;m very happy that the FuturICT lives on.
In general, I find the Japanese research community
is very open towards social computing, especially the idea of participatory social sensing. Maybe it&rsquo;s more fruitful to continue the project ideas first here in Japan. It&rsquo;s a bit sad that Europe has given away the chance of being a innovation leader in this field.
Yet meeting Dirk again and seeing how his ideas and research towards social computing matured and developed was also great. Europe might have missed a chance, yet there&rsquo;s still hope ;)</p>
<p>If you got interested in research about society and social science, I can recommed <a href="http://www.springer.com/physics/complexity/book/978-3-642-28999-6">&ldquo;Society is a Complex Matter&rdquo; by Philip Ball</a>. It&rsquo;s an engaging read for novices to the topic. I could get a copy of at the event.</p>

        
    </div>

    

</article>
          
            <article class="post-preview">
    <a href="https://kaikunze.de/2014/02/13/glass-talk-at-hacker-news-kansai/">
        <h2 class="post-title">Glass Talk at Hacker News Kansai</h2>
        
        
        
    </a>

    <p class="post-meta">
        <span class="post-meta">
  
  
  <i class="fas fa-calendar"></i>&nbsp;
  
  
    &nbsp;|&nbsp;<i class="fas fa-clock"></i>&nbsp;1&nbsp;minutes
  
  
    &nbsp;|&nbsp;<i class="fas fa-book"></i>&nbsp;116&nbsp;words
  
  
  
</span>


    </p>
    <div class="post-entry">
        
        <!-- raw HTML omitted -->
<p><img src="/imgs/glass3.jpg" alt="Glass 1">
<img src="/imgs/glass4.jpg" alt="Glass 2"></p>
<p>I&rsquo;m sorry it took so long to put online.
Too much to do, especially due to Augmented Human 2014 (really looking forward to the beginning of march) :)</p>
<p>As part of our monthly <a href="http://hnkansai.org">Hacker News meetup</a>in Kansai,
I gave a small introduction on how to use (and hack for) Google Glass.</p>
<!-- raw HTML omitted -->
<p>The slides are also available as <a href="../../slides/kansai_15.pdf">pdf download</a> (7 MB).</p>
<p>The &ldquo;Hacks&rdquo; are fairly simple. I more or less show how to develop for Glass using the Native Development Kit and some demonstrations how to read out sensors or other demo code I scraped from github and adjusted (see the slides for links to the sources). The examples include CameraZoom, Face Detection and Blink Recognition.</p>
<p><img src="/imgs/glass0.jpg" alt="Glass 3">
<img src="/imgs/glass2.jpg" alt="Glass 4"></p>

        
    </div>

    

</article>
          
            <article class="post-preview">
    <a href="https://kaikunze.de/2014/01/28/looking-back-at-30c3/">
        <h2 class="post-title">Looking back at 30C3</h2>
        
        
        
    </a>

    <p class="post-meta">
        <span class="post-meta">
  
  
  <i class="fas fa-calendar"></i>&nbsp;
  
  
    &nbsp;|&nbsp;<i class="fas fa-clock"></i>&nbsp;2&nbsp;minutes
  
  
    &nbsp;|&nbsp;<i class="fas fa-book"></i>&nbsp;352&nbsp;words
  
  
  
</span>


    </p>
    <div class="post-entry">
        
        <!-- raw HTML omitted -->
<p>Honestly I was impressed by the professionalism of the 30th Chaos Communication Congress. It changed a lot from the last time I visited ( 25c3), grew bigger without loosing its atmosphere. With the assemblies and workshops the event starts to get more and more interactive.</p>
<p>##Talk Recommendations##</p>
<p>I link to the youtube streams of the recordings
yet you can get them also over at <a href="http://media.ccc.de/browse/congress/2013/">media.ccc.de</a></p>
<p>Due to recent events, there were a lot of Snowden/NSA themed talks. I skipped most of them (as some were just plain to catch attention, others were simply to depressing).</p>
<p><a href="http://www.youtube.com/watch?v=HTVgPw7TR_k">Seeing The Secret State: Six Landscapes</a> is a must watch on the topic. I don&rsquo;t want to spoil to much, just watch it.</p>
<p><a href="http://www.youtube.com/watch?v=52NbMghN4ws">Machines that make</a> DIY is always fun. Although talking with Nadya Peek after the presentation, I think she could have given a better talk (the chat was more interesting).</p>
<p><a href="http://www.youtube.com/watch?v=2o2xBOQeB7Q">How to Build a Mind</a> Relatively high level talk about artificial
intelligence and related topics, quite entertaining.</p>
<p><a href="http://www.youtube.com/watch?v=Er9luiBa32k">FPGA 101</a>
Karsten Becker gives a good overview about why and when you want/don&rsquo;t want to use FPGAs. I have not much experience, yet usually programming FPGAs sucked the few days I tried it.
The toolchain he introduces sounds cool.
As a friend of mine said the right level
of nerdiness enough to be interesting and
not too much to lose the audience.</p>
<p><a href="http://www.youtube.com/watch?v=CPEzLNh5YIo">SD-Card Exploits</a>
So SD-cards have micro processors and guess what you can program them yourself. Quite scary :)</p>
<p>##My Talk Feedback##
I was a bit surprised how positive it was.
As the talk is kind of scary. If you imagine
we are really able to infer how much somebody
understands about a material using eye gaze,
who should be allowed to access this information?
Is it alright, to trust Google, Apple, Samsung etc.
with this type of information? It might no longer be
&ldquo;just&rdquo; our personal communication that can be
recorded and analyzed, but also our reading habits and
comprehension level.</p>
<p>If you haven&rsquo;t seen it yet, feel free <a href="http://www.youtube.com/embed/LrARH4KJRro">to watch it</a>.
You can also <a href="https://frab.cccv.de/en/30C3/public/events/5387/feedback/new">give feedback</a> on the talk if you want.
Highly appreciated.</p>

        
    </div>

    
    <div class="blog-tags">
        
        <a href="https://kaikunze.de//tags/30c3/">30c3</a>&nbsp;
        
    </div>
    

</article>
          
            <article class="post-preview">
    <a href="https://kaikunze.de/2013/12/28/30c3-toward-a-cognitive-quantified-self/">
        <h2 class="post-title">30C3 Toward a Cognitive Quantified Self</h2>
        
        
        
    </a>

    <p class="post-meta">
        <span class="post-meta">
  
  
  <i class="fas fa-calendar"></i>&nbsp;
  
  
    &nbsp;|&nbsp;<i class="fas fa-clock"></i>&nbsp;1&nbsp;minutes
  
  
    &nbsp;|&nbsp;<i class="fas fa-book"></i>&nbsp;118&nbsp;words
  
  
  
</span>


    </p>
    <div class="post-entry">
        
        <!-- raw HTML omitted -->
<p>Here are my <a href="/slides/30c3.pdf">talk slides</a> for my 30c3 talk &hellip; Sorry it took so long.
Thanks for the great feedback, will write a bit
more if I have sometime ;)</p>
<p>The video is also online. This happened very quick
after the event. I&rsquo;m impressed by the 30c3 content team.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Thanks a lot for the great reviews and suggestions.
They are highly appreciated. You can still <a href="https://frab.cccv.de/en/30C3/public/events/5387/feedback/new">review
the talk</a>.</p>
<p>##Talk Abstract##</p>
<p>The talk gives an overview about our work of quantifying knowledge acquisition tasks in real-life environments, focusing on reading. We combine several pervasive sensing approaches (computer vision, motion-based activity recognition etc.) to tackle the problem of recognizing and classifying knowledge acquisition tasks with a special focus on reading.</p>

        
    </div>

    

</article>
          
            <article class="post-preview">
    <a href="https://kaikunze.de/2013/12/22/hacking-glass/">
        <h2 class="post-title">Hacking Glass</h2>
        
        
        
    </a>

    <p class="post-meta">
        <span class="post-meta">
  
  
  <i class="fas fa-calendar"></i>&nbsp;
  
  
    &nbsp;|&nbsp;<i class="fas fa-clock"></i>&nbsp;2&nbsp;minutes
  
  
    &nbsp;|&nbsp;<i class="fas fa-book"></i>&nbsp;413&nbsp;words
  
  
  
</span>


    </p>
    <div class="post-entry">
        
        <!-- raw HTML omitted -->
<p>Disclaimer: Rooting and flashing your device voids your warranty and can brick your Glass. Also you won&rsquo;t receive OTA updates afterwards.
This is not an instruction manual. I just use it as
a scratch pad to give a record what I did and
what worked for me. The commands below will erase all data on your device. Proceed at your own risk.</p>
<p>##Rooting and Flashing Images##
To get root follow the instructions from Google.
Unfortunately, the fastboot under Mac OS does not work.
I could use a virtual machine on my Mac with ubuntu
to get root and flash images.</p>
<pre><code>adb devices
adb reboot-bootloader
fastboot devices

fastboot oem unlock
</code></pre>
<p>I needed to execute the last command twice. The first time
it just asked me if I was sure if I want to void my
warranty etc.</p>
<p>Next I flashed the boot image from the <a href="">Glass developer page</a>.</p>
<pre><code>fastboot flash boot boot.img
fastboot reboot
adb root
adb shell
</code></pre>
<p>If you want to update to a new OTA (in my case XE12) and
rooted your device,
you can download the zip with all necessary images from
Google. It&rsquo;s cool that they support rooting and
flashing (even if it voids your warranty).</p>
<pre><code>fastboot flash boot boot.img
fastboot flash system system.img
fastboot flash recovery recovery.img
fastboot flash userdata userdata.img
fastboot erase cache
</code></pre>
<p>##Reading out the Proximity Sensor##
I&rsquo;m most interested in accessing the proximity
sensor facing the eye. So thanks to Philip Scholl&rsquo;s and
Shoya&rsquo;s help, I was able to do it.
The device is under</p>
<pre><code>adb root
adb shell
&gt; cat /sys/bus/i2c/devices/4-0035/proxraw
</code></pre>
<p>Gives back one raw proximity value from
the sensor. Unfortunately without timestamp.
If you want to read out the proximity
data from Android Apps etc. you need to change
the access rights.</p>
<pre><code>&gt;chmod 664 /sys/bus/i2c/devices/4-0035/proxraw
</code></pre>
<p>##Privacy Enhancement##</p>
<p>As I will be visiting the <a href="">Chaos Communication Congress</a>
next weekend, I wanted to &ldquo;privacy enhance&rdquo; GLASS
for the event. I want to wear Glass but don&rsquo;t
really need the camera functionality.</p>
<p>So I used my <a href="http://www.the3doodler.com">3Doodler</a> to make a simple attachment to block the camera of.</p>
<p><img src="/imgs/doodler.jpg" alt="Doodler"></p>
<p>The tricky part is that the light sensor for adjusting
screen brightness is directly under the camera.
If it&rsquo;s blocked the screen will be very dark.
Here&rsquo;s the &ldquo;privacy enhanced&rdquo; Google Glass version.</p>
<p><img src="/imgs/glass_p.jpg" alt="Glass Enhanced"></p>
<p>and a picture taken by it. It&rsquo;s not completely black
due to the issue with the light sensor, yet I
think it&rsquo;s a start ;)</p>
<p><img src="/imgs/glass_p2.jpg" alt="Glass Enhanced"></p>
<p>Here is the very <a href="/imgs/glass.stencil.pdf">basic stencil</a> I used to build the attachment.</p>

        
    </div>

    
    <div class="blog-tags">
        
        <a href="https://kaikunze.de//tags/glass/">glass</a>&nbsp;
        
    </div>
    

</article>
          
            <article class="post-preview">
    <a href="https://kaikunze.de/2013/11/29/bits-and-bytes-instead-of-a-book-shelf/">
        <h2 class="post-title">Bits and Bytes instead of a Bookshelf</h2>
        
        
        
    </a>

    <p class="post-meta">
        <span class="post-meta">
  
  
  <i class="fas fa-calendar"></i>&nbsp;
  
  
    &nbsp;|&nbsp;<i class="fas fa-clock"></i>&nbsp;2&nbsp;minutes
  
  
    &nbsp;|&nbsp;<i class="fas fa-book"></i>&nbsp;357&nbsp;words
  
  
  
</span>


    </p>
    <div class="post-entry">
        
        <!-- raw HTML omitted -->
<p>Recently, I gave an interview for the German online issue
of the Scientific American (<a href="http://spectrum.de/">Spectrum der Wissenschaft</a>) for a <a href="http://www.spektrum.de/alias/lesen/bits-und-bytes-statt-buchregal/1211519">special about reading habits (in German, paywall)</a>.</p>
<p>As I&rsquo;m interested in the topic, I often hear
that &ldquo;endless scrolling&rdquo; is bad as it destroys
the mental map we make of books and pages or
that reading from backlit screens is eye-straining
inducing headaches. Personally, I cannot really
understand these complaints. I&rsquo;m doing most
of my reading on tablet devices or computer screens
though I never experienced these problems directly.</p>
<p>However, active reading -the process of working with the text through highlight, notes, marks- is still better
on paper. In Human Computer interaction terms, people talk about <a href="http://en.wikipedia.org/wiki/Affordance">affordances</a>. The affordance of paper is very high for active reading. So I find myself still printing out drafts
or review papers, especially if it&rsquo;s a close call and
I need to concentrate on the contents. In later case, I even like to change the reading environment, moving away
from my laptop/desktop to a meeting table or a bank outside. I believe this helps me concentrate, deliberately shutting out any distractions.</p>
<p>We just started to modify the reading experience using electronic devices.
So far most of the applications and reading devices
directly mimic the book. We have &ldquo;ebooks&rdquo;, &ldquo;e-reading&rdquo; software
use pages and page turns etc. I believe there is a lot of room for improvement related to reading on screens.</p>
<p>Given the possibility to assess the user&rsquo;s mental state using
<a href="/papers/kunze2013activity.pdf">Cognitive Activity Recognition</a>, we
can change content, structure and style of reading materials dynamically.
Most straight forward, if an application detects that a reader looses
interest, it could prompt her with an interactive challenge/video or
similar. Changing fonts, colors and lettering according to mood and
context could be also interesting. There is a fairly new
playground opening up for anybody curious and interested in
defining new forms of reading.</p>
<p>Interesting further reading:</p>
<p><a href="http://www.fxpal.com/publications/FXPAL-PR-98-053.pdf">Shilit et. al. Beyond Paper: Supporting Active Reading with Free Form Digital Ink Annotations</a></p>
<p><a href="http://courses.cs.vt.edu/~cs5714/fall2003/Affordances,%20as%20appeared.pdf">Hartson. Cognitive, physical, sensory, and functional affordances in interaction design</a></p>
<p><a href="http://hci.ucsd.edu/hollan/Pubs/piperCHI2009.pdf">Piper et. al. Tabletop Displays for Small Group Study: Affordances of Paper and Digital Materials</a></p>

        
    </div>

    

</article>
          
            <article class="post-preview">
    <a href="https://kaikunze.de/2013/11/05/amazing-okinawa-attending-the-asvai-workshop/">
        <h2 class="post-title">Amazing Okinawa - Attending the ASVAI Workshop</h2>
        
        
        
    </a>

    <p class="post-meta">
        <span class="post-meta">
  
  
  <i class="fas fa-calendar"></i>&nbsp;
  
  
    &nbsp;|&nbsp;<i class="fas fa-clock"></i>&nbsp;2&nbsp;minutes
  
  
    &nbsp;|&nbsp;<i class="fas fa-book"></i>&nbsp;250&nbsp;words
  
  
  
</span>


    </p>
    <div class="post-entry">
        
        <!-- raw HTML omitted -->
<p>The <a href="http://www.am.sanken.osaka-u.ac.jp/ASVAI2013/">ASVAI workshop</a> gave a good overview about several research
efforts part of and related to the <a href="http://www.jst.go.jp/kisoken/crest/en/research_area/ongoing/areah21-1.html">JST CREST</a> and the JSPS Core-to-Core Sanken Program.</p>
<p><a href="http://www.am.sanken.osaka-u.ac.jp/~yagi/">Prof. Yasushi Yagi</a>
showed how to infer intention from gait analysis.
Interestingly, he showed research about the relationship
of gaze and gait.</p>
<p><a href="http://ai.stanford.edu/~alireza/">Dr. Alireza Fathi</a>
presented cool work about ego centric cameras. He showed
how to estimate gaze using ego centric cameras during
cooking tasks and psychological studies.</p>
<p><a href="http://www.uh.edu/class/psychology/about/people/hanako-yoshida/index.php">Prof. Hanako Yoshida</a> explores
social learning in infants (equipping children with mobile
eye trackers &hellip; awesome!), inferring developmental stages
giving more insights in the learning process.</p>
<p><a href="http://www.irc.atr.jp/~m-shiomi/">Prof. Masahiro Shiomi</a>
spoke about his research trying to adapt robot behavior
to fit into social public spaces ( videos about
people running away from a robot included ;) ).
Currently, they focus on service robots and model their
behavior according to successful human service personnel.</p>
<p><a href="http://www.hci.iis.u-tokyo.ac.jp/~ysato/">Prof. Yoichi Sato</a> presented work related to
detecting visual attention. They use visual saliency
on video to train an appearance-based eye tracking.
Really interesting work, I had a chance to talk a bit
more with <a href="http://www.hci.iis.u-tokyo.ac.jp/~sugano/">Yusuke Sugano</a>, cool research :)</p>
<p>Of course, Koichi also gave an overview about our work.
If you want to read more, checkout the <a href="/papers/kunze2013activity.pdf">IEEE Computer article</a>.</p>
<p>I&rsquo;m looking forward to the main conference.
Here&rsquo;s a tag cloud using the abstracts of ACPR and ASVAI papers:</p>
<p><img src="/imgs/acpr_wordcloud.png" alt="Tag cloud"></p>
<p>We present demonstrations and new results
of the eye tracking on commodity
tablets/smart phones and a sharing infrastructure for our document annotation for smart phones.</p>

        
    </div>

    

</article>
          
            <article class="post-preview">
    <a href="https://kaikunze.de/2013/09/20/a-week-with-glass/">
        <h2 class="post-title">A Week with Glass</h2>
        
        
        
    </a>

    <p class="post-meta">
        <span class="post-meta">
  
  
  <i class="fas fa-calendar"></i>&nbsp;
  
  
    &nbsp;|&nbsp;<i class="fas fa-clock"></i>&nbsp;8&nbsp;minutes
  
  
    &nbsp;|&nbsp;<i class="fas fa-book"></i>&nbsp;1560&nbsp;words
  
  
  
</span>


    </p>
    <div class="post-entry">
        
        <!-- raw HTML omitted -->
<p>##First Impressions##
The Glass device feels expensive and a little bit futuristic.
I&rsquo;m impressed by the build quality and design. It has also a &ldquo;google&rdquo; feel to it, e.g. &ldquo;funny&rdquo; jokes in the manual (&ldquo;don&rsquo;t use glass for scuba diving &hellip;&rdquo;).
The display works extremely well and although glass is made for micro-interactions (quickly checking an email/sms, google now updates, making pictures), I could watch videos and read longer emails/documents on it without trouble and any sight problems (I experienced no headaches as happend with other setups, see below). It would be perfect for boring meetings if other people could not see what you are doing &hellip;
I assume the Glass design team made the conscious decision, to let other people know if you interact with glass. People can see if the screen is on and even recognize what&rsquo;s on the screen if they get close enough.</p>
<p>##Grandparents and Mother with Google Glass##</p>
<p>I have a basic test for technology or research topics in general.
I try to explain it to my grandparents and mother to see if they understand it and find it interesting.
Various head-mounted displays, tablets and activity recognition algorithms were tested this way &hellip;
E.g. they were not so big fans of tablets/slates or smart phones until they played with an iPhone and iPad.</p>
<p><img src="/imgs/oma.jpg" alt="oma">
<img src="/imgs/opa_sh.jpg" alt="opa"></p>
<p>Surprisingly, my grandparents did not have the reservation they have towards other computing devices.
Usually, they have the feeling that they could destroy something and are extra careful/hesitant.
Yet, Google Glass looks like glasses, so it was easy for them to setup and use.
The system worked quite well (although so far only English is supported), speech recognition
and touch interface were simple to learn after a quick 5 min. introduction. I was surprised myself &hellip;</p>
<p>Sadly, the speech interface does a poor job with German names, e.g. googleing for &ldquo;Apfelkuchen Rezept&rdquo; (Apple cake recipe) did not work as intended.</p>
<p>Yet, both of them saw potential in Glass and could imagine wearing it during the day.
I was most astound by the application cases they came up with.</p>
<p><img src="/imgs/opa_pills.jpg" alt="opa pills"></p>
<p>My grandfather took a picture of his pills he needs to take after each meal.
He told me, he always wonders if he has taken them or not and sometimes checks 2-3 times after
a meal to be certain. Taking a picture and using the touch panel to browse recent pictures (with timestamp), he can easily figure out when he took them the last time.</p>
<p>My grandmother would love to use Glass for gardening. It happens sometimes, that she gets a
phone call during garden work and then she has to change shoes, take of gloves etc. and hurry
to the portable phone. Additionally, she likes to get the advice of my mum or friends about
where to put which flower seeds etc. so she asked me if it&rsquo;s possible to show the video stream from
Glass to other people over the Internet :)</p>
<p>We also did a practise test, My grandmother and mother wore Glass during shopping in Karlsruhe.
Both of them wear glasses, so not too many people noticed or looked at them. I think they assumed it&rsquo;s some kind of medical device or sight improvement etc.</p>
<p><img src="/imgs/oma2.jpg" alt="oma ka">
<img src="/imgs/mum.jpg" alt="mum ka"></p>
<p>My mother used the time line in glass to track when she made the pictures and traced back when she saw something nice to figure out at which store the item was. She tried taking pictures of price tags. Unfortunately, the resolution on the screen is not high enough
to read the price, yet this could be easily fixed with a zoom function for pictures.
Interestingly, she also carries a smart phone, yet she never got the idea to use it for shopping like Glass.</p>
<p>##Public Reactions##</p>
<p>As mentioned my mum and grandmother wore Glass nearly unnoticed.
This is quite different to my experience &hellip; If I wear it in public, most people in Karlsruhe
and Mannheim (the two cities I tried) eyed at me with wary faces (you can see the questions in their eyes : &ldquo;What is he wearing ?? Some medical device ?? NERD!! &ldquo;). This was particularly bad
when I spoke with a clerk or a person directly, as they kept staring at Glass instead of looking into my eyes ;)
Social reception was better when I was with my family. Strangely, people asked mostly my grandmother
what I was wearing. Very few approached me directly.
Reactions fell into 3 broad categories:</p>
<ol>
<li>&ldquo;WOW Cool &hellip; Glass! How is it? Can I try??&rdquo; &ndash; <em>Note</em> : Before it&rsquo;s released in public, I strongly recommend not wearing it on any campus with a larger IT faculty. I did not account for that and it was quite difficult to get over Karlsruhe University Campus :)</li>
<li>&ldquo;Stop violating my privacy!&rdquo; &ndash; During the week I had only one person directly approach me about privacy concerns. The person was quite angry at first. I believe it&rsquo;s mostly due to misinformation (something Google needs to take serious), as he believed Glass would stream automatically everything to Google and listen to all the conversations etc.. After I showed him the functionality of the device, how to use it and how to see if somebody is using it, he was calmer and actually liked it (could see the potential of a wearable display).</li>
<li>&ldquo;What&rsquo;s wrong with this guy?&rdquo; &ndash; Especially if I was traveling alone people stared at me. I asked 1 or 2 of the most obnoxious persons starring at me about it and they answered they thought I was wearing a medical device and they wondered &ldquo;what&rsquo;s wrong with me&rdquo; as I looked otherwise &ldquo;normal&rdquo;.</li>
</ol>
<p>##Some Issues##</p>
<p>The 3 biggest issues I had with it:</p>
<ol>
<li>Weight and placement - You need to get used to its weight. As I&rsquo;m not wearing prescription glasses, it feels strange to me wearing something on my nose. It&rsquo;s definitely heavier than glasses. After a couple of hours it is ok. Also it&rsquo;s always in your peripheral view, you need to get used to it.</li>
<li>Battery life - Ok, I played a lot with it, given I could use Glass only for a week. At the end (when me playing with it got fewer) I could get barely a day of usage. I expect that&rsquo;s something they can easily fix. Pst&hellip; you can also plug-in a portable USB battery to charge during usage :)</li>
<li>Social acceptance - This is the hardest one to crack. Having used Glass, I don&rsquo;t understand most of the privacy fears people raise. It&rsquo;s very obvious if a person is using the device/taking a picture etc. If I want to take covert pictures/videos of people, I believe it&rsquo;s easier to do with today&rsquo;s smart phones or spy cameras (available on Amazon for example) &hellip;</li>
</ol>
<p>##Some more Context##</p>
<p>When I unboxed Glass, I remembered how Paul, my phD. advisor, and Thad (Glass project manager)
chatted about how in future everybody would wear some kind of head-mounted
display and a computing device always connected to the
Internet, helping us with everyday tasks - augmentations
to our brain.</p>
<p>In the past, Paul was not a huge enthusiast about wearable displays and I agreed
with him. I attempted to use the <a href="https://en.wikipedia.org/wiki/Optical_head-mounted_display#MicroOptical_.2F_MyVu">MicroOptical</a> (the display used by Thad) several times and had always terrible headaches afterwards &hellip; Just not for me.</p>
<p><img src="/imgs/me.jpg" alt="Me"></p>
<p>Around 2004 - 2010, I played with various wearable setups to use during everyday life during my phD. each only for a week or couple of days. If you work on wearable computing you have to try at least. As seen in the picture above, the only setup working for me was a Prototype HMD from Zeiss with the
<a href="http://www.qbic.ethz.ch">Qbic</a>, an awesome belt-integrated linux pc by ETH (black belt buckle in the picture), and <a href="http://www.handykey.com">Twiddler 2</a>. Yet, I stopped using it as the glasses were quite heavy, maintaining/adjusting the software was a hassle (compared to the advantages) and -I have to admit- due to social pressure, imagine living as a cyborg in a small Bavarian town, mostly occupied by law and business students &hellip; I found my small, black, analog notebook more handy and less intimidating to other people. Today, I&rsquo;m an avid iPhone user (Things, Clear, Habit List, Textastic, Prompt and Lendromat  &hellip;).</p>
<p>##To sum up##
In total I was quite sceptical at first, the design reminded me too much on the Microoptical and the headaches I got using it. Completely unfounded! Even given the social acceptance issue, I cannot wait to get Glass for a longer test. However, I really need a good note taking app, running vim on glass would already be a selling point for me, replacing my black notebook (and maybe smart phone?). I undusted my Twiddler2 (took a long time to find it in the cellar) with hacked bluetooth connection, started practicing again and hope I can try it soon with Vim for Glass :D This is definitely not an application case for the mass market &hellip;  My grandparents told me that they believe there is a broader demand for such a device also by &ldquo;normal&rdquo; people (they actually want to use it!). So let&rsquo;s see.</p>
<p>Plus the researcher in my cannot wait to get easy accessible motion sensors onto the heads of a lot of people. Combined with the sensors in your pocket it&rsquo;s activity recognition heaven!</p>
<p>Let&rsquo;s <a href="https://news.ycombinator.com/item?id=6456220">discuss on Hacker News</a> if you want.</p>

        
    </div>

    
    <div class="blog-tags">
        
        <a href="https://kaikunze.de//tags/glass/">glass</a>&nbsp;
        
    </div>
    

</article>
          
            <article class="post-preview">
    <a href="https://kaikunze.de/2013/09/15/ubicom-iswc-impressions/">
        <h2 class="post-title">Ubicomp ISWC Impressions</h2>
        
        
        
    </a>

    <p class="post-meta">
        <span class="post-meta">
  
  
  <i class="fas fa-calendar"></i>&nbsp;
  
  
    &nbsp;|&nbsp;<i class="fas fa-clock"></i>&nbsp;2&nbsp;minutes
  
  
    &nbsp;|&nbsp;<i class="fas fa-book"></i>&nbsp;314&nbsp;words
  
  
  
</span>


    </p>
    <div class="post-entry">
        
        <p>Usually, I&rsquo;m not such a big fan of conference openings,
yet Friedemann Mattern provided a great intro giving
an overview about the origins of Pervasive and Ubicom
mentioning all important people and showing nice vintage
pictures from Hans Gellersen, Alois Ferscha,  Marc Langheinrich, Albrecht Schmidt, Kristof Van Laerhoven etc.</p>
<p>Deeply impressed by the organization, social and general
talk quality, I was a bit sceptical before the merger
of Pervasive / Ubicom and collocating ISWC, yet it was
completely unfounded.</p>
<p>We got some great feedback for Kazuma&rsquo;s and Shoya&rsquo;s demos.
They both did a great job introducing their work about:</p>
<ul>
<li><a href="/papers/kunze2013my.pdf">My Reading Life – Towards Utilizing Eyetracking on Unmodified Tablets and Phones</a></li>
<li><a href="/papers/kunze2013annotate.pdf">Annotate Me – Supporting Active Reading using Real-Time Document Image Retrieval On Mobile Devices</a></li>
</ul>
<p>We got also a lof of interest and feedback
to Andreas Bulling&rsquo;s and my work about recognizing
document types using only eye gaze.
By the way, below are the talk slides and the abstract of
the paper.
##ISWC Talk Slides##</p>
<!-- raw HTML omitted -->
<p>##Abstract##
Reading is a ubiquitous activity that many people even per- form in transit, such as while on the bus or while walking. Tracking reading enables us to gain more insights about ex- pertise level and potential knowledge of users – towards a reading log tracking and improve knowledge acquisition. As a first step towards this vision, in this work we investigate whether different document types can be automatically de- tected from visual behaviour recorded using a mobile eye tracker. We present an initial recognition approach that com- bines special purpose eye movement features as well as ma- chine learning for document type detection. We evaluate our approach in a user study with eight participants and five Japanese document types and achieve a recognition perfor- mance of 74% using user-independent training.</p>
<p>Full paper link:
<a href="/papers/2013Kunze-5.pdf">I know what you are reading – Recognition of Document Types Using Mobile Eye Tracking</a></p>

        
    </div>

    
    <div class="blog-tags">
        
        <a href="https://kaikunze.de//tags/conference/">conference</a>&nbsp;
        
        <a href="https://kaikunze.de//tags/research/">research</a>&nbsp;
        
        <a href="https://kaikunze.de//tags/ubicomp/">ubicomp</a>&nbsp;
        
        <a href="https://kaikunze.de//tags/iswc/">iswc</a>&nbsp;
        
    </div>
    

</article>
          
        </div>

        
          <ul class="pager main-pager">
            
              <li class="previous">
                <a href="https://kaikunze.de/page/2/">&larr; Newer Posts</a>
              </li>
            
            
              <li class="next">
                <a href="https://kaikunze.de/page/4/">Older Posts &rarr;</a>
              </li>
            
          </ul>
        
      </div>
    </div>
  </div>

      
    <div class="page-meta">
  
  
  
</div>


  
<footer>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <ul class="list-inline text-center footer-links">
          
              <li>
                <a href="mailto:kai.kunze@pm.me" title="Email me">
                  <span class="fa-stack fa-lg">
                    <i class="fas fa-circle fa-stack-2x"></i>
                    <i class="fas fa-envelope fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li>
                <a href="https://www.facebook.com/kai.kunze" title="Facebook">
                  <span class="fa-stack fa-lg">
                    <i class="fas fa-circle fa-stack-2x"></i>
                    <i class="fab fa-facebook fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li>
                <a href="https://github.com/kkai" title="GitHub">
                  <span class="fa-stack fa-lg">
                    <i class="fas fa-circle fa-stack-2x"></i>
                    <i class="fab fa-github fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li>
                <a href="https://twitter.com/_kai_ku" title="Twitter">
                  <span class="fa-stack fa-lg">
                    <i class="fas fa-circle fa-stack-2x"></i>
                    <i class="fab fa-twitter fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li>
                <a href="https://linkedin.com/in/kaikunze" title="LinkedIn">
                  <span class="fa-stack fa-lg">
                    <i class="fas fa-circle fa-stack-2x"></i>
                    <i class="fab fa-linkedin fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li>
                <a href="https://www.instagram.com/kaikunze" title="Instagram">
                  <span class="fa-stack fa-lg">
                    <i class="fas fa-circle fa-stack-2x"></i>
                    <i class="fab fa-instagram fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
          
          <li>
            <a href="/index.xml" title="RSS">
              <span class="fa-stack fa-lg">
                <i class="fas fa-circle fa-stack-2x"></i>
                <i class="fas fa-rss fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
        </ul>
        <p class="credits copyright text-muted">
          
            
              <a href="kaikunze.de">Kai Kunze</a>
            
          

          &nbsp;&bull;&nbsp;&copy;
          
            2023
          

          
            &nbsp;&bull;&nbsp;
            <a href="https://kaikunze.de/">Kai Kunze</a>
          
      </div>
    </div>
  </div>
</footer><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.js" integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/contrib/auto-render.min.js" integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe" crossorigin="anonymous"></script>
<script   src="https://code.jquery.com/jquery-3.7.0.min.js"  integrity="sha256-2Pmvv0kuTBOenSvLm6bvfBSSHrUJ+3A7x6P5Ebd07/g="   crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

<script src="https://kaikunze.de/js/main.js"></script>
<script src="https://kaikunze.de/js/highlight.min.js"></script>
<script> hljs.initHighlightingOnLoad(); </script>
<script> $(document).ready(function() {$("pre.chroma").css("padding","0");}); </script><script> renderMathInElement(document.body); </script><script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.js" integrity="sha384-QELNnmcmU8IR9ZAykt67vGr9/rZJdHbiWi64V88fCPaOohUlHCqUD/unNN0BXSqy" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe-ui-default.min.js" integrity="sha384-m67o7SkQ1ALzKZIFh4CiTA8tmadaujiTa9Vu+nqPSwDOqHrDmxLezTdFln8077+q" crossorigin="anonymous"></script><script src="https://kaikunze.de/js/load-photoswipe.js"></script>









    
  </body>
</html>

